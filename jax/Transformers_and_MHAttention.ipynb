{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 6 (JAX): Transformers and Multi-Head Attention\n",
    "\n",
    "![Status](https://img.shields.io/static/v1.svg?label=Status&message=Finished&color=green)\n",
    "\n",
    "\n",
    "**Filled notebook:** \n",
    "[![View on Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=View%20On%20Github&color=lightgrey)](https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/JAX/tutorial6/Transformers_and_MHAttention.ipynb)\n",
    "[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/JAX/tutorial6/Transformers_and_MHAttention.ipynb)  \n",
    "**Pre-trained models:** \n",
    "[![View files on Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=View%20On%20Github&color=lightgrey)](https://github.com/phlippe/saved_models/tree/main/JAX/tutorial6)   \n",
    "**PyTorch version:**\n",
    "[![View on RTD](https://img.shields.io/static/v1.svg?logo=readthedocs&label=RTD&message=View%20On%20RTD&color=8CA1AF)](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html)   \n",
    "**Author:** Phillip Lippe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Note:** This notebook is written in JAX+Flax. It is a 1-to-1 translation of the original notebook written in PyTorch+PyTorch Lightning with almost identical results. For an introduction to JAX, check out our [Tutorial 2 (JAX): Introduction to JAX+Flax](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/JAX/tutorial2/Introduction_to_JAX.html). Further, throughout the notebook, we comment on major differences to the PyTorch version and provide explanations for the major parts of the JAX code.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Speed comparison**: We note the training times for all models in the PyTorch and the JAX implementation below (PyTorch v1.11, JAX v0.3.13). The models were trained on the same hardware (NVIDIA RTX3090, 24 core CPU) and we slightly adjusted the tutorials to use the exact same training settings (same data loading parameters, evaluation schedule, etc.). Overall, the JAX implementation is almost *4x faster* than PyTorch! However, this is mostly due to the small model and input sizes, and the code has not been explicitly designed for benchmarking. With larger models, larger batch sizes, or smaller GPUs, the speed up is expected to become considerably smaller (see e.g. [Tutorial 15](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/JAX/tutorial15/Vision_Transformer.html)).\n",
    "    \n",
    "| Models            |   PyTorch   |     JAX    |\n",
    "|-------------------|:-----------:|:----------:|\n",
    "| Reverse Sequence  | 0min 26sec  | 0min 7sec  |\n",
    "| Anomaly Detection | 16min 34sec | 3min 45sec |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will discuss one of the most impactful architectures of the last 2 years: the Transformer model. Since the paper [Attention Is All You Need](https://arxiv.org/abs/1706.03762) by Vaswani et al. had been published in 2017, the Transformer architecture has continued to beat benchmarks in many domains, most importantly in Natural Language Processing. Transformers with an incredible amount of parameters can generate long, convincing [essays](https://www.theguardian.com/commentisfree/2020/sep/08/robot-wrote-this-article-gpt-3), and opened up new application fields of AI. As the hype of the Transformer architecture seems not to come to an end in the next years, it is important to understand how it works, and have implemented it yourself, which we will do in this notebook.\n",
    "\n",
    "Despite the huge success of Transformers in NLP, we will _not_ include the NLP domain in our notebook here. Why? Firstly, the Master AI at UvA offers many great NLP courses that will take a closer look at the application of the Transformer architecture in NLP ([NLP2](https://studiegids.uva.nl/xmlpages/page/2020-2021/zoek-vak/vak/79628), [Advanced Topics in Computational Semantics](https://studiegids.uva.nl/xmlpages/page/2020-2021/zoek-vak/vak/80162)). Secondly, assignment 2 takes already a closer look at language generation on character level, on which you could easily apply our transformer architecture. Finally, and most importantly, there is so much more to the Transformer architecture. NLP is the domain the Transformer architecture has been originally proposed for and had the greatest impact on, but it also accelerated research in other domains, recently even [Computer Vision](https://arxiv.org/abs/2010.11929). Thus, we focus here on what makes the Transformer and self-attention so powerful in general. In [Tutorial 15](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/JAX/tutorial15/Vision_Transformer.html), we will discuss the application of Transformers in Computer Vision.\n",
    "\n",
    "Below, we import our standard libraries. We use [JAX](https://jax.readthedocs.io/en/latest/) as acceleration backend, [Flax](https://flax.readthedocs.io/en/latest/index.html) for implementing neural networks, and [Optax](https://optax.readthedocs.io/en/latest/index.html) to optimize the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_61249/3875325789.py:13: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  set_matplotlib_formats('svg', 'pdf') # For export\n",
      "2022-10-17 15:54:08.207567: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-10-17 15:54:08.598036: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/sander/Downloads/tensorrt/lib:/home/sander/Downloads/cudnn/cuda/lib64/\n",
      "2022-10-17 15:54:08.598093: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/sander/Downloads/tensorrt/lib:/home/sander/Downloads/cudnn/cuda/lib64/\n",
      "2022-10-17 15:54:08.598097: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "WARNING:absl:GlobalAsyncCheckpointManager is not imported correctly. Checkpointing of GlobalDeviceArrays will not be available.To use the feature, install tensorstore.\n",
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: TFRT_CPU_0\n"
     ]
    }
   ],
   "source": [
    "## Standard libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import json\n",
    "from functools import partial\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "plt.set_cmap('cividis')\n",
    "%matplotlib inline \n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg', 'pdf') # For export\n",
    "from matplotlib.colors import to_rgb\n",
    "import matplotlib\n",
    "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
    "import seaborn as sns\n",
    "sns.reset_orig()\n",
    "\n",
    "## tqdm for loading bars\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "## To run JAX on TPU in Google Colab, uncomment the two lines below\n",
    "# import jax.tools.colab_tpu\n",
    "# jax.tools.colab_tpu.setup_tpu()\n",
    "\n",
    "## JAX\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "\n",
    "## Flax (NN in JAX)\n",
    "try:\n",
    "    import flax\n",
    "except ModuleNotFoundError: # Install flax if missing\n",
    "    !pip install --quiet flax\n",
    "    import flax\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state, checkpoints\n",
    "\n",
    "## Optax (Optimizers in JAX)\n",
    "try:\n",
    "    import optax\n",
    "except ModuleNotFoundError: # Install optax if missing\n",
    "    !pip install --quiet optax\n",
    "    import optax\n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR100\n",
    "\n",
    "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
    "DATASET_PATH = \"./data\"\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = \"./saved_models/tutorial6_jax\"\n",
    "\n",
    "# Seeding for random operations\n",
    "main_rng = random.PRNGKey(42)\n",
    "\n",
    "print(\"Device:\", jax.devices()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two pre-trained models are downloaded below. Make sure to have adjusted your `CHECKPOINT_PATH` before running this code if not already done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/phlippe/saved_models/main/JAX/tutorial6/ReverseTask.ckpt...\n",
      "Downloading https://raw.githubusercontent.com/phlippe/saved_models/main/JAX/tutorial6/SetAnomalyTask.ckpt...\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "from urllib.error import HTTPError\n",
    "# Github URL where saved models are stored for this tutorial\n",
    "base_url = \"https://raw.githubusercontent.com/phlippe/saved_models/main/JAX/tutorial6/\"\n",
    "# Files to download\n",
    "pretrained_files = [\"ReverseTask.ckpt\", \"SetAnomalyTask.ckpt\"]\n",
    "\n",
    "# Create checkpoint path if it doesn't exist yet\n",
    "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
    "\n",
    "# For each file, check whether it already exists. If not, try downloading it.\n",
    "for file_name in pretrained_files:\n",
    "    file_path = os.path.join(CHECKPOINT_PATH, file_name)\n",
    "    if \"/\" in file_name:\n",
    "        os.makedirs(file_path.rsplit(\"/\",1)[0], exist_ok=True)\n",
    "    if not os.path.isfile(file_path):\n",
    "        file_url = base_url + file_name\n",
    "        print(f\"Downloading {file_url}...\")\n",
    "        try:\n",
    "            urllib.request.urlretrieve(file_url, file_path)\n",
    "        except HTTPError as e:\n",
    "            print(\"Something went wrong. Please try to download the file from the GDrive folder, or contact the author with the full output including the following error:\\n\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Transformer architecture\n",
    "\n",
    "In the first part of this notebook, we will implement the Transformer architecture by hand. As the architecture is so popular, its main components are already integrated into Flax ([SelfAttention](https://flax.readthedocs.io/en/latest/_autosummary/flax.linen.SelfAttention.html#flax.linen.SelfAttention), [MultiHeadAttention](https://flax.readthedocs.io/en/latest/_autosummary/flax.linen.MultiHeadDotProductAttention.html#flax.linen.MultiHeadDotProductAttention)) and there exist several implementations (e.g. in [Trax](https://trax-ml.readthedocs.io/en/latest/trax.models.html#trax.models.transformer.Transformer)) and pre-trained models (e.g. on [Hugging Face](https://huggingface.co/docs/transformers/index)). However, we will implement it here ourselves, to get through to the smallest details.\n",
    "\n",
    "There are of course many more tutorials out there about attention and Transformers. Below, we list a few that are worth exploring if you are interested in the topic and might want yet another perspective on the topic after this one:\n",
    "\n",
    "* [Transformer: A Novel Neural Network Architecture for Language Understanding (Jakob Uszkoreit, 2017)](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html) - The original Google blog post about the Transformer paper, focusing on the application in machine translation.\n",
    "* [The Illustrated Transformer (Jay Alammar, 2018)](http://jalammar.github.io/illustrated-transformer/) - A very popular and great blog post intuitively explaining the Transformer architecture with many nice visualizations. The focus is on NLP.\n",
    "* [Attention? Attention! (Lilian Weng, 2018)](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html) - A nice blog post summarizing attention mechanisms in many domains including vision.\n",
    "* [Illustrated: Self-Attention (Raimi Karim, 2019)](https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a) - A nice visualization of the steps of self-attention. Recommended going through if the explanation below is too abstract for you.\n",
    "* [The Transformer family (Lilian Weng, 2020)](https://lilianweng.github.io/lil-log/2020/04/07/the-transformer-family.html) - A very detailed blog post reviewing more variants of Transformers besides the original one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Attention?\n",
    "\n",
    "The attention mechanism describes a recent new group of layers in neural networks that has attracted a lot of interest in the past few years, especially in sequence tasks. There are a lot of different possible definitions of \"attention\" in the literature, but the one we will use here is the following: _the attention mechanism describes a weighted average of (sequence) elements with the weights dynamically computed based on an input query and elements' keys_. So what does this exactly mean? The goal is to take an average over the features of multiple elements. However, instead of weighting each element equally, we want to weight them depending on their actual values. In other words, we want to dynamically decide on which inputs we want to \"attend\" more than others. In particular, an attention mechanism has usually four parts we need to specify:\n",
    "\n",
    "* **Query**: The query is a feature vector that describes what we are looking for in the sequence, i.e. what would we maybe want to pay attention to.\n",
    "* **Keys**: For each input element, we have a key which is again a feature vector. This feature vector roughly describes what the element is \"offering\", or when it might be important. The keys should be designed such that we can identify the elements we want to pay attention to based on the query.\n",
    "* **Values**: For each input element, we also have a value vector. This feature vector is the one we want to average over.\n",
    "* **Score function**: To rate which elements we want to pay attention to, we need to specify a score function $f_{attn}$. The score function takes the query and a key as input, and output the score/attention weight of the query-key pair. It is usually implemented by simple similarity metrics like a dot product, or a small MLP.\n",
    "\n",
    "\n",
    "The weights of the average are calculated by a softmax over all score function outputs. Hence, we assign those value vectors a higher weight whose corresponding key is most similar to the query. If we try to describe it with pseudo-math, we can write: \n",
    "\n",
    "$$\n",
    "\\alpha_i = \\frac{\\exp\\left(f_{attn}\\left(\\text{key}_i, \\text{query}\\right)\\right)}{\\sum_j \\exp\\left(f_{attn}\\left(\\text{key}_j, \\text{query}\\right)\\right)}, \\hspace{5mm} \\text{out} = \\sum_i \\alpha_i \\cdot \\text{value}_i\n",
    "$$\n",
    "\n",
    "Visually, we can show the attention over a sequence of words as follows:\n",
    "\n",
    "<center width=\"100%\" style=\"padding:25px\"><img src=\"../../tutorial6/attention_example.svg\" width=\"750px\"></center>\n",
    "\n",
    "For every word, we have one key and one value vector. The query is compared to all keys with a score function (in this case the dot product) to determine the weights. The softmax is not visualized for simplicity. Finally, the value vectors of all words are averaged using the attention weights.\n",
    "\n",
    "Most attention mechanisms differ in terms of what queries they use, how the key and value vectors are defined, and what score function is used. The attention applied inside the Transformer architecture is called **self-attention**. In self-attention, each sequence element provides a key, value, and query. For each element, we perform an attention layer where based on its query, we check the similarity of the all sequence elements' keys, and returned a different, averaged value vector for each element. We will now go into a bit more detail by first looking at the specific implementation of the attention mechanism which is in the Transformer case the scaled dot product attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled Dot Product Attention\n",
    "\n",
    "The core concept behind self-attention is the scaled dot product attention. Our goal is to have an attention mechanism with which any element in a sequence can attend to any other while still being efficient to compute. The dot product attention takes as input a set of queries $Q\\in\\mathbb{R}^{T\\times d_k}$, keys $K\\in\\mathbb{R}^{T\\times d_k}$ and values $V\\in\\mathbb{R}^{T\\times d_v}$ where $T$ is the sequence length, and $d_k$ and $d_v$ are the hidden dimensionality for queries/keys and values respectively. For simplicity, we neglect the batch dimension for now. The attention value from element $i$ to $j$ is based on its similarity of the query $Q_i$ and key $K_j$, using the dot product as the similarity metric. In math, we calculate the dot product attention as follows:\n",
    "\n",
    "$$\\text{Attention}(Q,K,V)=\\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "The matrix multiplication $QK^T$ performs the dot product for every possible pair of queries and keys, resulting in a matrix of the shape $T\\times T$. Each row represents the attention logits for a specific element $i$ to all other elements in the sequence. On these, we apply a softmax and multiply with the value vector to obtain a weighted mean (the weights being determined by the attention). Another perspective on this attention mechanism offers the computation graph which is visualized below (figure credit - [Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)).\n",
    "\n",
    "<center width=\"100%\"><img src=\"../../tutorial6/scaled_dot_product_attn.svg\" width=\"210px\"></center>\n",
    "\n",
    "One aspect we haven't discussed yet is the scaling factor of $1/\\sqrt{d_k}$. This scaling factor is crucial to maintain an appropriate variance of attention values after initialization. Remember that we intialize our layers with the intention of having equal variance throughout the model, and hence, $Q$ and $K$ might also have a variance close to $1$. However, performing a dot product over two vectors with a variance $\\sigma^2$ results in a scalar having $d_k$-times higher variance: \n",
    "\n",
    "$$q_i \\sim \\mathcal{N}(0,\\sigma^2), k_i \\sim \\mathcal{N}(0,\\sigma^2) \\to \\text{Var}\\left(\\sum_{i=1}^{d_k} q_i\\cdot k_i\\right) = \\sigma^4\\cdot d_k$$\n",
    "\n",
    "\n",
    "If we do not scale down the variance back to $\\sim\\sigma^2$, the softmax over the logits will already saturate to $1$ for one random element and $0$ for all others. The gradients through the softmax will be close to zero so that we can't learn the parameters appropriately. Note that the extra factor of $\\sigma^2$, i.e., having $\\sigma^4$ instead of $\\sigma^2$, is usually not an issue, since we keep the original variance $\\sigma^2$ close to $1$ anyways.\n",
    "\n",
    "The block `Mask (opt.)` in the diagram above represents the optional masking of specific entries in the attention matrix. This is for instance used if we stack multiple sequences with different lengths into a batch. To still benefit from parallelization in PyTorch, we pad the sentences to the same length and mask out the padding tokens during the calculation of the attention values. This is usually done by setting the respective attention logits to a very low value. \n",
    "\n",
    "After we have discussed the details of the scaled dot product attention block, we can write a function below which computes the output features given the triple of queries, keys, and values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(q, k, v, mask=None):\n",
    "    d_k = q.shape[-1]\n",
    "    attn_logits = jnp.matmul(q, jnp.swapaxes(k, -2, -1))\n",
    "    attn_logits = attn_logits / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        attn_logits = jnp.where(mask == 0, -9e15, attn_logits)\n",
    "    attention = nn.softmax(attn_logits, axis=-1)\n",
    "    values = jnp.matmul(attention, v)\n",
    "    return values, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our code above supports any additional dimensionality in front of the sequence length so that we can also use it for batches. However, for a better understanding, let's generate a few random queries, keys, and value vectors, and calculate the attention outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      " [[-0.6613315   0.70056266]\n",
      " [ 0.08239268 -1.7793142 ]\n",
      " [-0.04378588  1.0965251 ]]\n",
      "K\n",
      " [[ 1.7257481   0.35568172]\n",
      " [ 1.3034704   1.2873708 ]\n",
      " [ 1.6871481  -0.5714404 ]]\n",
      "V\n",
      " [[ 1.5129997   1.1050899 ]\n",
      " [ 0.27949408 -0.46224892]\n",
      " [-1.1003422  -1.1437944 ]]\n",
      "Values\n",
      " [[ 0.376226   -0.1465618 ]\n",
      " [-0.42778558 -0.5989566 ]\n",
      " [ 0.43624768 -0.11678301]]\n",
      "Attention\n",
      " [[0.27963293 0.54049295 0.17987415]\n",
      " [0.22194658 0.06706189 0.7109916 ]\n",
      " [0.27977085 0.5837308  0.13649832]]\n"
     ]
    }
   ],
   "source": [
    "seq_len, d_k = 3, 2\n",
    "main_rng, rand1 = random.split(main_rng)\n",
    "qkv = random.normal(rand1, (3, seq_len, d_k))\n",
    "q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "values, attention = scaled_dot_product(q, k, v)\n",
    "print(\"Q\\n\", q)\n",
    "print(\"K\\n\", k)\n",
    "print(\"V\\n\", v)\n",
    "print(\"Values\\n\", values)\n",
    "print(\"Attention\\n\", attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before continuing, make sure you can follow the calculation of the specific values here, and also check it by hand. It is important to fully understand how the scaled dot product attention is calculated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Attention\n",
    "\n",
    "The scaled dot product attention allows a network to attend over a sequence. However, often there are multiple different aspects a sequence element wants to attend to, and a single weighted average is not a good option for it. This is why we extend the attention mechanisms to multiple heads, i.e. multiple different query-key-value triplets on the same features. Specifically, given a query, key, and value matrix, we transform those into $h$ sub-queries, sub-keys, and sub-values, which we pass through the scaled dot product attention independently. Afterward, we concatenate the heads and combine them with a final weight matrix. Mathematically, we can express this operation as:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "    \\text{Multihead}(Q,K,V) & = \\text{Concat}(\\text{head}_1,...,\\text{head}_h)W^{O}\\\\\n",
    "    \\text{where } \\text{head}_i & = \\text{Attention}(QW_i^Q,KW_i^K, VW_i^V)\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "We refer to this as Multi-Head Attention layer with the learnable parameters $W_{1...h}^{Q}\\in\\mathbb{R}^{D\\times d_k}$, $W_{1...h}^{K}\\in\\mathbb{R}^{D\\times d_k}$, $W_{1...h}^{V}\\in\\mathbb{R}^{D\\times d_v}$, and $W^{O}\\in\\mathbb{R}^{h\\cdot d_k\\times d_{out}}$ ($D$ being the input dimensionality). Expressed in a computational graph, we can visualize it as below (figure credit - [Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)).\n",
    "\n",
    "<center width=\"100%\"><img src=\"../../tutorial6/multihead_attention.svg\" width=\"230px\"></center>\n",
    "\n",
    "How are we applying a Multi-Head Attention layer in a neural network, where we don't have an arbitrary query, key, and value vector as input? Looking at the computation graph above, a simple but effective implementation is to set the current feature map in a NN, $X\\in\\mathbb{R}^{B\\times T\\times d_{\\text{model}}}$, as $Q$, $K$ and $V$ ($B$ being the batch size, $T$ the sequence length, $d_{\\text{model}}$ the hidden dimensionality of $X$). The consecutive weight matrices $W^{Q}$, $W^{K}$, and $W^{V}$ can transform $X$ to the corresponding feature vectors that represent the queries, keys, and values of the input. Using this approach, we can implement the Multi-Head Attention module below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    embed_dim : int  # Output dimension\n",
    "    num_heads : int  # Number of parallel heads (h)\n",
    "    \n",
    "    def setup(self):\n",
    "        # Stack all weight matrices 1...h and W^Q, W^K, W^V together for efficiency\n",
    "        # Note that in many implementations you see \"bias=False\" which is optional\n",
    "        self.qkv_proj = nn.Dense(3*self.embed_dim,\n",
    "                                 kernel_init=nn.initializers.xavier_uniform(),  # Weights with Xavier uniform init\n",
    "                                 bias_init=nn.initializers.zeros  # Bias init with zeros\n",
    "                                )\n",
    "        self.o_proj = nn.Dense(self.embed_dim,\n",
    "                               kernel_init=nn.initializers.xavier_uniform(),\n",
    "                               bias_init=nn.initializers.zeros)\n",
    "\n",
    "    def __call__(self, x, mask=None):\n",
    "        batch_size, seq_length, embed_dim = x.shape\n",
    "        qkv = self.qkv_proj(x)\n",
    "        \n",
    "        # Separate Q, K, V from linear output\n",
    "        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, -1)\n",
    "        qkv = qkv.transpose(0, 2, 1, 3) # [Batch, Head, SeqLen, Dims]\n",
    "        q, k, v = jnp.array_split(qkv, 3, axis=-1)\n",
    "        \n",
    "        # Determine value outputs\n",
    "        values, attention = scaled_dot_product(q, k, v, mask=mask)\n",
    "        values = values.transpose(0, 2, 1, 3) # [Batch, SeqLen, Head, Dims]\n",
    "        values = values.reshape(batch_size, seq_length, embed_dim)\n",
    "        o = self.o_proj(values)\n",
    "        \n",
    "        return o, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out (3, 16, 128) Attention (3, 4, 16, 16)\n"
     ]
    }
   ],
   "source": [
    "## Test MultiheadAttention implementation\n",
    "# Example features as input\n",
    "main_rng, x_rng = random.split(main_rng)\n",
    "x = random.normal(x_rng, (3, 16, 128))\n",
    "# Create attention\n",
    "mh_attn = MultiheadAttention(embed_dim=128, num_heads=4)\n",
    "# Initialize parameters of attention with random key and inputs\n",
    "main_rng, init_rng = random.split(main_rng)\n",
    "params = mh_attn.init(init_rng, x)['params']\n",
    "# Apply attention with parameters on the inputs\n",
    "out, attn = mh_attn.apply({'params': params}, x)\n",
    "print('Out', out.shape, 'Attention', attn.shape)\n",
    "\n",
    "del mh_attn, params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One crucial characteristic of the multi-head attention is that it is permutation-equivariant with respect to its inputs. This means that if we switch two input elements in the sequence, e.g. $X_1\\leftrightarrow X_2$ (neglecting the batch dimension for now), the output is exactly the same besides the elements 1 and 2 switched. Hence, the multi-head attention is actually looking at the input not as a sequence, but as a set of elements. This property makes the multi-head attention block and the Transformer architecture so powerful and widely applicable! But what if the order of the input is actually important for solving the task, like language modeling? The answer is to encode the position in the input features, which we will take a closer look at later (topic _Positional encodings_ below).\n",
    "\n",
    "Before moving on to creating the Transformer architecture, we can compare the self-attention operation with our other common layer competitors for sequence data: convolutions and recurrent neural networks. Below you can find a table by [Vaswani et al. (2017)](https://arxiv.org/abs/1706.03762) on the complexity per layer, the number of sequential operations, and maximum path length. The complexity is measured by the upper bound of the number of operations to perform, while the maximum path length represents the maximum number of steps a forward or backward signal has to traverse to reach any other position. The lower this length, the better gradient signals can backpropagate for long-range dependencies. Let's take a look at the table below:\n",
    "\n",
    "\n",
    "<center width=\"100%\"><img src=\"../../tutorial6/comparison_conv_rnn.svg\" width=\"600px\"></center>\n",
    "\n",
    "$n$ is the sequence length, $d$ is the representation dimension and $k$ is the kernel size of convolutions. In contrast to recurrent networks, the self-attention layer can parallelize all its operations making it much faster to execute for smaller sequence lengths. However, when the sequence length exceeds the hidden dimensionality, self-attention becomes more expensive than RNNs. One way of reducing the computational cost for long sequences is by restricting the self-attention to a neighborhood of inputs to attend over, denoted by $r$. Nevertheless, there has been recently a lot of work on more efficient Transformer architectures that still allow long dependencies, of which you can find an overview in the paper by [Tay et al. (2020)](https://arxiv.org/abs/2009.06732) if interested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Encoder\n",
    "\n",
    "Next, we will look at how to apply the multi-head attention block inside the Transformer architecture. Originally, the Transformer model was designed for machine translation. Hence, it got an encoder-decoder structure where the encoder takes as input the sentence in the original language and generates an attention-based representation. On the other hand, the decoder attends over the encoded information and generates the translated sentence in an autoregressive manner, as in a standard RNN. While this structure is extremely useful for Sequence-to-Sequence tasks with the necessity of autoregressive decoding, we will focus here on the encoder part. Many advances in NLP have been made using pure encoder-based Transformer models (if interested, models include the [BERT](https://arxiv.org/abs/1810.04805)-family, the [Vision Transformer](https://arxiv.org/abs/2010.11929), and more), and in our tutorial, we will also mainly focus on the encoder part. If you have understood the encoder architecture, the decoder is a very small step to implement as well. The full Transformer architecture looks as follows (figure credit - [Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)).:\n",
    "\n",
    "<center width=\"100%\"><img src=\"../../tutorial6/transformer_architecture.svg\" width=\"400px\"></center>\n",
    "\n",
    "The encoder consists of $N$ identical blocks that are applied in sequence. Taking as input $x$, it is first passed through a Multi-Head Attention block as we have implemented above. The output is added to the original input using a residual connection, and we apply a consecutive Layer Normalization on the sum. Overall, it calculates $\\text{LayerNorm}(x+\\text{Multihead}(x,x,x))$ ($x$ being $Q$, $K$ and $V$ input to the attention layer). The residual connection is crucial in the Transformer architecture for two reasons: \n",
    "\n",
    "1. Similar to ResNets, Transformers are designed to be very deep. Some models contain more than 24 blocks in the encoder. Hence, the residual connections are crucial for enabling a smooth gradient flow through the model.\n",
    "2. Without the residual connection, the information about the original sequence is lost. Remember that the Multi-Head Attention layer ignores the position of elements in a sequence, and can only learn it based on the input features. Removing the residual connections would mean that this information is lost after the first attention layer (after initialization), and with a randomly initialized query and key vector, the output vectors for position $i$ has no relation to its original input. All outputs of the attention are likely to represent similar/same information, and there is no chance for the model to distinguish which information came from which input element. An alternative option to residual connection would be to fix at least one head to focus on its original input, but this is very inefficient and does not have the benefit of the improved gradient flow.\n",
    "\n",
    "The Layer Normalization also plays an important role in the Transformer architecture as it enables faster training and provides small regularization. Additionally, it ensures that the features are in a similar magnitude among the elements in the sequence. We are not using Batch Normalization because it depends on the batch size which is often small with Transformers (they require a lot of GPU memory), and BatchNorm has shown to perform particularly bad in language as the features of words tend to have a much higher variance (there are many, very rare words which need to be considered for a good distribution estimate).\n",
    "\n",
    "Additionally to the Multi-Head Attention, a small fully connected feed-forward network is added to the model, which is applied to each position separately and identically. Specifically, the model uses a Linear$\\to$ReLU$\\to$Linear MLP. The full transformation including the residual connection can be expressed as:  \n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "    \\text{FFN}(x) & = \\max(0, xW_1+b_1)W_2 + b_2\\\\\n",
    "    x & = \\text{LayerNorm}(x + \\text{FFN}(x))\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "This MLP adds extra complexity to the model and allows transformations on each sequence element separately. You can imagine as this allows the model to \"post-process\" the new information added by the previous Multi-Head Attention, and prepare it for the next attention block. Usually, the inner dimensionality of the MLP is 2-8$\\times$ larger than $d_{\\text{model}}$, i.e. the dimensionality of the original input $x$. The general advantage of a wider layer instead of a narrow, multi-layer MLP is the faster, parallelizable execution.\n",
    "\n",
    "Finally, after looking at all parts of the encoder architecture, we can start implementing it below. We first start by implementing a single encoder block. Additionally to the layers described above, we will add dropout layers in the MLP and on the output of the MLP and Multi-Head Attention for regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    input_dim : int  # Input dimension is needed here since it is equal to the output dimension (residual connection)\n",
    "    num_heads : int\n",
    "    dim_feedforward : int\n",
    "    dropout_prob : float\n",
    "    \n",
    "    def setup(self):\n",
    "        # Attention layer\n",
    "        self.self_attn = MultiheadAttention(embed_dim=self.input_dim, \n",
    "                                            num_heads=self.num_heads)\n",
    "        # Two-layer MLP\n",
    "        self.linear = [\n",
    "            nn.Dense(self.dim_feedforward),\n",
    "            nn.Dropout(self.dropout_prob),\n",
    "            nn.relu,\n",
    "            nn.Dense(self.input_dim)\n",
    "        ]\n",
    "        # Layers to apply in between the main layers\n",
    "        self.norm1 = nn.LayerNorm()\n",
    "        self.norm2 = nn.LayerNorm()\n",
    "        self.dropout = nn.Dropout(self.dropout_prob)\n",
    "\n",
    "    def __call__(self, x, mask=None, train=True):\n",
    "        # Attention part\n",
    "        attn_out, _ = self.self_attn(x, mask=mask)\n",
    "        x = x + self.dropout(attn_out, deterministic=not train)\n",
    "        x = self.norm1(x)\n",
    "        \n",
    "        # MLP part\n",
    "        linear_out = x\n",
    "        for l in self.linear:\n",
    "            linear_out = l(linear_out) if not isinstance(l, nn.Dropout) else l(linear_out, deterministic=not train)\n",
    "        x = x + self.dropout(linear_out, deterministic=not train)\n",
    "        x = self.norm2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out (3, 16, 128)\n"
     ]
    }
   ],
   "source": [
    "## Test EncoderBlock implementation\n",
    "# Example features as input\n",
    "main_rng, x_rng = random.split(main_rng)\n",
    "x = random.normal(x_rng, (3, 16, 128))\n",
    "# Create encoder block\n",
    "encblock = EncoderBlock(input_dim=128, num_heads=4, dim_feedforward=512, dropout_prob=0.1)\n",
    "# Initialize parameters of encoder block with random key and inputs\n",
    "main_rng, init_rng, dropout_init_rng = random.split(main_rng, 3)\n",
    "params = encblock.init({'params': init_rng, 'dropout': dropout_init_rng}, x, True)['params']\n",
    "# Apply encoder block with parameters on the inputs\n",
    "# Since dropout is stochastic, we need to pass a rng to the forward\n",
    "main_rng, dropout_apply_rng = random.split(main_rng)\n",
    "out = encblock.apply({'params': params}, x, train=True, rngs={'dropout': dropout_apply_rng})\n",
    "print('Out', out.shape)\n",
    "\n",
    "del encblock, params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this block, we can implement a module for the full Transformer encoder. Additionally to a forward function that iterates through the sequence of encoder blocks, we also provide a function called `get_attention_maps`. The idea of this function is to return the attention probabilities for all Multi-Head Attention blocks in the encoder. This helps us in understanding, and in a sense, explaining the model. However, the attention probabilities should be interpreted with a grain of salt as it does not necessarily reflect the true interpretation of the model (there is a series of papers about this, including [Attention is not Explanation](https://arxiv.org/abs/1902.10186) and [Attention is not not Explanation](https://arxiv.org/abs/1908.04626))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    num_layers : int\n",
    "    input_dim : int\n",
    "    num_heads : int\n",
    "    dim_feedforward : int\n",
    "    dropout_prob : float\n",
    "    \n",
    "    def setup(self):\n",
    "        self.layers = [EncoderBlock(self.input_dim, self.num_heads, self.dim_feedforward, self.dropout_prob) for _ in range(self.num_layers)]\n",
    "\n",
    "    def __call__(self, x, mask=None, train=True):\n",
    "        for l in self.layers:\n",
    "            x = l(x, mask=mask, train=train)\n",
    "        return x\n",
    "\n",
    "    def get_attention_maps(self, x, mask=None, train=True):\n",
    "        # A function to return the attention maps within the model for a single application\n",
    "        # Used for visualization purpose later\n",
    "        attention_maps = []\n",
    "        for l in self.layers:\n",
    "            _, attn_map = l.self_attn(x, mask=mask)\n",
    "            attention_maps.append(attn_map)\n",
    "            x = l(x, mask=mask, train=train)\n",
    "        return attention_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out (3, 16, 128)\n",
      "Attention maps 5 (3, 4, 16, 16)\n"
     ]
    }
   ],
   "source": [
    "## Test TransformerEncoder implementation\n",
    "# Example features as input\n",
    "main_rng, x_rng = random.split(main_rng)\n",
    "x = random.normal(x_rng, (3, 16, 128))\n",
    "# Create Transformer encoder\n",
    "transenc = TransformerEncoder(num_layers=5, \n",
    "                              input_dim=128,\n",
    "                              num_heads=4,\n",
    "                              dim_feedforward=256,\n",
    "                              dropout_prob=0.15)\n",
    "# Initialize parameters of transformer with random key and inputs\n",
    "main_rng, init_rng, dropout_init_rng = random.split(main_rng, 3)\n",
    "params = transenc.init({'params': init_rng, 'dropout': dropout_init_rng}, x, train=True)['params']\n",
    "# Apply transformer with parameters on the inputs\n",
    "# Since dropout is stochastic, we need to pass a rng to the forward\n",
    "main_rng, dropout_apply_rng = random.split(main_rng)\n",
    "# Instead of passing params and rngs every time to a function call, we can bind them to the module\n",
    "binded_mod = transenc.bind({'params': params}, rngs={'dropout': dropout_apply_rng})\n",
    "out = binded_mod(x, train=True)\n",
    "print('Out', out.shape)\n",
    "attn_maps = binded_mod.get_attention_maps(x, train=True)\n",
    "print('Attention maps', len(attn_maps), attn_maps[0].shape)\n",
    "\n",
    "del transenc, binded_mod, params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional encoding\n",
    "\n",
    "We have discussed before that the Multi-Head Attention block is permutation-equivariant, and cannot distinguish whether an input comes before another one in the sequence or not. In tasks like language understanding, however, the position is important for interpreting the input words. The position information can therefore be added via the input features. We could learn a embedding for every possible position, but this would not generalize to a dynamical input sequence length. Hence, the better option is to use feature patterns that the network can identify from the features and potentially generalize to larger sequences. The specific pattern chosen by Vaswani et al. are sine and cosine functions of different frequencies, as follows:\n",
    "\n",
    "$$\n",
    "PE_{(pos,i)} = \\begin{cases}\n",
    "    \\sin\\left(\\frac{pos}{10000^{i/d_{\\text{model}}}}\\right) & \\text{if}\\hspace{3mm} i \\text{ mod } 2=0\\\\\n",
    "    \\cos\\left(\\frac{pos}{10000^{(i-1)/d_{\\text{model}}}}\\right) & \\text{otherwise}\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$PE_{(pos,i)}$ represents the position encoding at position $pos$ in the sequence, and hidden dimensionality $i$. These values, concatenated for all hidden dimensions, are added to the original input features (in the Transformer visualization above, see \"Positional encoding\"), and constitute the position information. We distinguish between even ($i \\text{ mod } 2=0$) and uneven ($i \\text{ mod } 2=1$) hidden dimensionalities where we apply a sine/cosine respectively. The intuition behind this encoding is that you can represent $PE_{(pos+k,:)}$ as a linear function of $PE_{(pos,:)}$, which might allow the model to easily attend to relative positions. The wavelengths in different dimensions range from $2\\pi$ to $10000\\cdot 2\\pi$.\n",
    "\n",
    "The positional encoding is implemented below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    d_model : int         # Hidden dimensionality of the input.\n",
    "    max_len : int = 5000  # Maximum length of a sequence to expect.\n",
    "\n",
    "    def setup(self):\n",
    "        # Create matrix of [SeqLen, HiddenDim] representing the positional encoding for max_len inputs\n",
    "        pe = np.zeros((self.max_len, self.d_model))\n",
    "        position = np.arange(0, self.max_len, dtype=np.float32)[:,None]\n",
    "        div_term = np.exp(np.arange(0, self.d_model, 2) * (-math.log(10000.0) / self.d_model))\n",
    "        pe[:, 0::2] = np.sin(position * div_term)\n",
    "        pe[:, 1::2] = np.cos(position * div_term)\n",
    "        pe = pe[None]\n",
    "        self.pe = jax.device_put(pe)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = x + self.pe[:, :x.shape[1]]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the positional encoding, we can visualize it below. We will generate an image of the positional encoding over hidden dimensionality and position in a sequence. Each pixel, therefore, represents the change of the input feature we perform to encode the specific position. Let's do it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/pdf": "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\n",
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"448.724438pt\" height=\"226.194375pt\" viewBox=\"0 0 448.724438 226.194375\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2022-10-17T15:54:13.756479</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.6.1, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 226.194375 \n",
       "L 448.724438 226.194375 \n",
       "L 448.724438 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 40.603125 188.638125 \n",
       "L 373.243125 188.638125 \n",
       "L 373.243125 22.318125 \n",
       "L 40.603125 22.318125 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g clip-path=\"url(#pb83f263e01)\">\n",
       "    <image xlink:href=\"data:image/png;base64,\n",
       "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\" id=\"image46b3e64747\" transform=\"scale(1 -1) translate(0 -166.32)\" x=\"40.603125\" y=\"-22.318125\" width=\"332.64\" height=\"166.32\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"m3893d982cc\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m3893d982cc\" x=\"40.603125\" y=\"188.638125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 1 -->\n",
       "      <g transform=\"translate(37.421875 203.236563) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m3893d982cc\" x=\"71.788125\" y=\"188.638125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 10 -->\n",
       "      <g transform=\"translate(65.425625 203.236563) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m3893d982cc\" x=\"106.438125\" y=\"188.638125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 20 -->\n",
       "      <g transform=\"translate(100.075625 203.236563) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m3893d982cc\" x=\"141.088125\" y=\"188.638125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 30 -->\n",
       "      <g transform=\"translate(134.725625 203.236563) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \n",
       "Q 3050 2419 3304 2112 \n",
       "Q 3559 1806 3559 1356 \n",
       "Q 3559 666 3084 287 \n",
       "Q 2609 -91 1734 -91 \n",
       "Q 1441 -91 1130 -33 \n",
       "Q 819 25 488 141 \n",
       "L 488 750 \n",
       "Q 750 597 1062 519 \n",
       "Q 1375 441 1716 441 \n",
       "Q 2309 441 2620 675 \n",
       "Q 2931 909 2931 1356 \n",
       "Q 2931 1769 2642 2001 \n",
       "Q 2353 2234 1838 2234 \n",
       "L 1294 2234 \n",
       "L 1294 2753 \n",
       "L 1863 2753 \n",
       "Q 2328 2753 2575 2939 \n",
       "Q 2822 3125 2822 3475 \n",
       "Q 2822 3834 2567 4026 \n",
       "Q 2313 4219 1838 4219 \n",
       "Q 1578 4219 1281 4162 \n",
       "Q 984 4106 628 3988 \n",
       "L 628 4550 \n",
       "Q 988 4650 1302 4700 \n",
       "Q 1616 4750 1894 4750 \n",
       "Q 2613 4750 3031 4423 \n",
       "Q 3450 4097 3450 3541 \n",
       "Q 3450 3153 3228 2886 \n",
       "Q 3006 2619 2597 2516 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-33\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m3893d982cc\" x=\"175.738125\" y=\"188.638125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 40 -->\n",
       "      <g transform=\"translate(169.375625 203.236563) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m3893d982cc\" x=\"210.388125\" y=\"188.638125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 50 -->\n",
       "      <g transform=\"translate(204.025625 203.236563) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-35\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_7\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m3893d982cc\" x=\"245.038125\" y=\"188.638125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 60 -->\n",
       "      <g transform=\"translate(238.675625 203.236563) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \n",
       "Q 1688 2584 1439 2293 \n",
       "Q 1191 2003 1191 1497 \n",
       "Q 1191 994 1439 701 \n",
       "Q 1688 409 2113 409 \n",
       "Q 2538 409 2786 701 \n",
       "Q 3034 994 3034 1497 \n",
       "Q 3034 2003 2786 2293 \n",
       "Q 2538 2584 2113 2584 \n",
       "z\n",
       "M 3366 4563 \n",
       "L 3366 3988 \n",
       "Q 3128 4100 2886 4159 \n",
       "Q 2644 4219 2406 4219 \n",
       "Q 1781 4219 1451 3797 \n",
       "Q 1122 3375 1075 2522 \n",
       "Q 1259 2794 1537 2939 \n",
       "Q 1816 3084 2150 3084 \n",
       "Q 2853 3084 3261 2657 \n",
       "Q 3669 2231 3669 1497 \n",
       "Q 3669 778 3244 343 \n",
       "Q 2819 -91 2113 -91 \n",
       "Q 1303 -91 875 529 \n",
       "Q 447 1150 447 2328 \n",
       "Q 447 3434 972 4092 \n",
       "Q 1497 4750 2381 4750 \n",
       "Q 2619 4750 2861 4703 \n",
       "Q 3103 4656 3366 4563 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-36\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_8\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m3893d982cc\" x=\"279.688125\" y=\"188.638125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 70 -->\n",
       "      <g transform=\"translate(273.325625 203.236563) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-37\" d=\"M 525 4666 \n",
       "L 3525 4666 \n",
       "L 3525 4397 \n",
       "L 1831 0 \n",
       "L 1172 0 \n",
       "L 2766 4134 \n",
       "L 525 4134 \n",
       "L 525 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-37\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_9\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m3893d982cc\" x=\"314.338125\" y=\"188.638125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 80 -->\n",
       "      <g transform=\"translate(307.975625 203.236563) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \n",
       "Q 1584 2216 1326 1975 \n",
       "Q 1069 1734 1069 1313 \n",
       "Q 1069 891 1326 650 \n",
       "Q 1584 409 2034 409 \n",
       "Q 2484 409 2743 651 \n",
       "Q 3003 894 3003 1313 \n",
       "Q 3003 1734 2745 1975 \n",
       "Q 2488 2216 2034 2216 \n",
       "z\n",
       "M 1403 2484 \n",
       "Q 997 2584 770 2862 \n",
       "Q 544 3141 544 3541 \n",
       "Q 544 4100 942 4425 \n",
       "Q 1341 4750 2034 4750 \n",
       "Q 2731 4750 3128 4425 \n",
       "Q 3525 4100 3525 3541 \n",
       "Q 3525 3141 3298 2862 \n",
       "Q 3072 2584 2669 2484 \n",
       "Q 3125 2378 3379 2068 \n",
       "Q 3634 1759 3634 1313 \n",
       "Q 3634 634 3220 271 \n",
       "Q 2806 -91 2034 -91 \n",
       "Q 1263 -91 848 271 \n",
       "Q 434 634 434 1313 \n",
       "Q 434 1759 690 2068 \n",
       "Q 947 2378 1403 2484 \n",
       "z\n",
       "M 1172 3481 \n",
       "Q 1172 3119 1398 2916 \n",
       "Q 1625 2713 2034 2713 \n",
       "Q 2441 2713 2670 2916 \n",
       "Q 2900 3119 2900 3481 \n",
       "Q 2900 3844 2670 4047 \n",
       "Q 2441 4250 2034 4250 \n",
       "Q 1625 4250 1398 4047 \n",
       "Q 1172 3844 1172 3481 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-38\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_10\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m3893d982cc\" x=\"348.988125\" y=\"188.638125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 90 -->\n",
       "      <g transform=\"translate(342.625625 203.236563) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-39\" d=\"M 703 97 \n",
       "L 703 672 \n",
       "Q 941 559 1184 500 \n",
       "Q 1428 441 1663 441 \n",
       "Q 2288 441 2617 861 \n",
       "Q 2947 1281 2994 2138 \n",
       "Q 2813 1869 2534 1725 \n",
       "Q 2256 1581 1919 1581 \n",
       "Q 1219 1581 811 2004 \n",
       "Q 403 2428 403 3163 \n",
       "Q 403 3881 828 4315 \n",
       "Q 1253 4750 1959 4750 \n",
       "Q 2769 4750 3195 4129 \n",
       "Q 3622 3509 3622 2328 \n",
       "Q 3622 1225 3098 567 \n",
       "Q 2575 -91 1691 -91 \n",
       "Q 1453 -91 1209 -44 \n",
       "Q 966 3 703 97 \n",
       "z\n",
       "M 1959 2075 \n",
       "Q 2384 2075 2632 2365 \n",
       "Q 2881 2656 2881 3163 \n",
       "Q 2881 3666 2632 3958 \n",
       "Q 2384 4250 1959 4250 \n",
       "Q 1534 4250 1286 3958 \n",
       "Q 1038 3666 1038 3163 \n",
       "Q 1038 2656 1286 2365 \n",
       "Q 1534 2075 1959 2075 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-39\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_11\">\n",
       "     <!-- Position in sequence -->\n",
       "     <g transform=\"translate(155.627813 216.914688) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-50\" d=\"M 1259 4147 \n",
       "L 1259 2394 \n",
       "L 2053 2394 \n",
       "Q 2494 2394 2734 2622 \n",
       "Q 2975 2850 2975 3272 \n",
       "Q 2975 3691 2734 3919 \n",
       "Q 2494 4147 2053 4147 \n",
       "L 1259 4147 \n",
       "z\n",
       "M 628 4666 \n",
       "L 2053 4666 \n",
       "Q 2838 4666 3239 4311 \n",
       "Q 3641 3956 3641 3272 \n",
       "Q 3641 2581 3239 2228 \n",
       "Q 2838 1875 2053 1875 \n",
       "L 1259 1875 \n",
       "L 1259 0 \n",
       "L 628 0 \n",
       "L 628 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \n",
       "Q 1497 3097 1228 2736 \n",
       "Q 959 2375 959 1747 \n",
       "Q 959 1119 1226 758 \n",
       "Q 1494 397 1959 397 \n",
       "Q 2419 397 2687 759 \n",
       "Q 2956 1122 2956 1747 \n",
       "Q 2956 2369 2687 2733 \n",
       "Q 2419 3097 1959 3097 \n",
       "z\n",
       "M 1959 3584 \n",
       "Q 2709 3584 3137 3096 \n",
       "Q 3566 2609 3566 1747 \n",
       "Q 3566 888 3137 398 \n",
       "Q 2709 -91 1959 -91 \n",
       "Q 1206 -91 779 398 \n",
       "Q 353 888 353 1747 \n",
       "Q 353 2609 779 3096 \n",
       "Q 1206 3584 1959 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \n",
       "L 2834 2853 \n",
       "Q 2591 2978 2328 3040 \n",
       "Q 2066 3103 1784 3103 \n",
       "Q 1356 3103 1142 2972 \n",
       "Q 928 2841 928 2578 \n",
       "Q 928 2378 1081 2264 \n",
       "Q 1234 2150 1697 2047 \n",
       "L 1894 2003 \n",
       "Q 2506 1872 2764 1633 \n",
       "Q 3022 1394 3022 966 \n",
       "Q 3022 478 2636 193 \n",
       "Q 2250 -91 1575 -91 \n",
       "Q 1294 -91 989 -36 \n",
       "Q 684 19 347 128 \n",
       "L 347 722 \n",
       "Q 666 556 975 473 \n",
       "Q 1284 391 1588 391 \n",
       "Q 1994 391 2212 530 \n",
       "Q 2431 669 2431 922 \n",
       "Q 2431 1156 2273 1281 \n",
       "Q 2116 1406 1581 1522 \n",
       "L 1381 1569 \n",
       "Q 847 1681 609 1914 \n",
       "Q 372 2147 372 2553 \n",
       "Q 372 3047 722 3315 \n",
       "Q 1072 3584 1716 3584 \n",
       "Q 2034 3584 2315 3537 \n",
       "Q 2597 3491 2834 3397 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \n",
       "L 1178 3500 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 3500 \n",
       "z\n",
       "M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 4134 \n",
       "L 603 4134 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \n",
       "L 1172 3500 \n",
       "L 2356 3500 \n",
       "L 2356 3053 \n",
       "L 1172 3053 \n",
       "L 1172 1153 \n",
       "Q 1172 725 1289 603 \n",
       "Q 1406 481 1766 481 \n",
       "L 2356 481 \n",
       "L 2356 0 \n",
       "L 1766 0 \n",
       "Q 1100 0 847 248 \n",
       "Q 594 497 594 1153 \n",
       "L 594 3053 \n",
       "L 172 3053 \n",
       "L 172 3500 \n",
       "L 594 3500 \n",
       "L 594 4494 \n",
       "L 1172 4494 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-71\" d=\"M 947 1747 \n",
       "Q 947 1113 1208 752 \n",
       "Q 1469 391 1925 391 \n",
       "Q 2381 391 2643 752 \n",
       "Q 2906 1113 2906 1747 \n",
       "Q 2906 2381 2643 2742 \n",
       "Q 2381 3103 1925 3103 \n",
       "Q 1469 3103 1208 2742 \n",
       "Q 947 2381 947 1747 \n",
       "z\n",
       "M 2906 525 \n",
       "Q 2725 213 2448 61 \n",
       "Q 2172 -91 1784 -91 \n",
       "Q 1150 -91 751 415 \n",
       "Q 353 922 353 1747 \n",
       "Q 353 2572 751 3078 \n",
       "Q 1150 3584 1784 3584 \n",
       "Q 2172 3584 2448 3432 \n",
       "Q 2725 3281 2906 2969 \n",
       "L 2906 3500 \n",
       "L 3481 3500 \n",
       "L 3481 -1331 \n",
       "L 2906 -1331 \n",
       "L 2906 525 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-75\" d=\"M 544 1381 \n",
       "L 544 3500 \n",
       "L 1119 3500 \n",
       "L 1119 1403 \n",
       "Q 1119 906 1312 657 \n",
       "Q 1506 409 1894 409 \n",
       "Q 2359 409 2629 706 \n",
       "Q 2900 1003 2900 1516 \n",
       "L 2900 3500 \n",
       "L 3475 3500 \n",
       "L 3475 0 \n",
       "L 2900 0 \n",
       "L 2900 538 \n",
       "Q 2691 219 2414 64 \n",
       "Q 2138 -91 1772 -91 \n",
       "Q 1169 -91 856 284 \n",
       "Q 544 659 544 1381 \n",
       "z\n",
       "M 1991 3584 \n",
       "L 1991 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \n",
       "L 3122 2828 \n",
       "Q 2878 2963 2633 3030 \n",
       "Q 2388 3097 2138 3097 \n",
       "Q 1578 3097 1268 2742 \n",
       "Q 959 2388 959 1747 \n",
       "Q 959 1106 1268 751 \n",
       "Q 1578 397 2138 397 \n",
       "Q 2388 397 2633 464 \n",
       "Q 2878 531 3122 666 \n",
       "L 3122 134 \n",
       "Q 2881 22 2623 -34 \n",
       "Q 2366 -91 2075 -91 \n",
       "Q 1284 -91 818 406 \n",
       "Q 353 903 353 1747 \n",
       "Q 353 2603 823 3093 \n",
       "Q 1294 3584 2113 3584 \n",
       "Q 2378 3584 2631 3529 \n",
       "Q 2884 3475 3122 3366 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"56.677734\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"117.859375\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"169.958984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-74\" x=\"197.742188\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"236.951172\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"264.734375\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"325.916016\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"389.294922\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"421.082031\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"448.865234\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"512.244141\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"544.03125\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"596.130859\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-71\" x=\"657.654297\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-75\" x=\"721.130859\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"784.509766\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"846.033203\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"909.412109\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"964.392578\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <defs>\n",
       "       <path id=\"mc21a5ad8c8\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#mc21a5ad8c8\" x=\"40.603125\" y=\"22.318125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 1 -->\n",
       "      <g transform=\"translate(27.240625 26.117344) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_12\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mc21a5ad8c8\" x=\"40.603125\" y=\"53.503125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_13\">\n",
       "      <!-- 10 -->\n",
       "      <g transform=\"translate(20.878125 57.302344) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_13\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mc21a5ad8c8\" x=\"40.603125\" y=\"88.153125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_14\">\n",
       "      <!-- 20 -->\n",
       "      <g transform=\"translate(20.878125 91.952344) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_14\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mc21a5ad8c8\" x=\"40.603125\" y=\"122.803125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_15\">\n",
       "      <!-- 30 -->\n",
       "      <g transform=\"translate(20.878125 126.602344) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-33\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_15\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mc21a5ad8c8\" x=\"40.603125\" y=\"157.453125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_16\">\n",
       "      <!-- 40 -->\n",
       "      <g transform=\"translate(20.878125 161.252344) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_17\">\n",
       "     <!-- Hidden dimension -->\n",
       "     <g transform=\"translate(14.798438 150.710938) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-48\" d=\"M 628 4666 \n",
       "L 1259 4666 \n",
       "L 1259 2753 \n",
       "L 3553 2753 \n",
       "L 3553 4666 \n",
       "L 4184 4666 \n",
       "L 4184 0 \n",
       "L 3553 0 \n",
       "L 3553 2222 \n",
       "L 1259 2222 \n",
       "L 1259 0 \n",
       "L 628 0 \n",
       "L 628 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-64\" d=\"M 2906 2969 \n",
       "L 2906 4863 \n",
       "L 3481 4863 \n",
       "L 3481 0 \n",
       "L 2906 0 \n",
       "L 2906 525 \n",
       "Q 2725 213 2448 61 \n",
       "Q 2172 -91 1784 -91 \n",
       "Q 1150 -91 751 415 \n",
       "Q 353 922 353 1747 \n",
       "Q 353 2572 751 3078 \n",
       "Q 1150 3584 1784 3584 \n",
       "Q 2172 3584 2448 3432 \n",
       "Q 2725 3281 2906 2969 \n",
       "z\n",
       "M 947 1747 \n",
       "Q 947 1113 1208 752 \n",
       "Q 1469 391 1925 391 \n",
       "Q 2381 391 2643 752 \n",
       "Q 2906 1113 2906 1747 \n",
       "Q 2906 2381 2643 2742 \n",
       "Q 2381 3103 1925 3103 \n",
       "Q 1469 3103 1208 2742 \n",
       "Q 947 2381 947 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6d\" d=\"M 3328 2828 \n",
       "Q 3544 3216 3844 3400 \n",
       "Q 4144 3584 4550 3584 \n",
       "Q 5097 3584 5394 3201 \n",
       "Q 5691 2819 5691 2113 \n",
       "L 5691 0 \n",
       "L 5113 0 \n",
       "L 5113 2094 \n",
       "Q 5113 2597 4934 2840 \n",
       "Q 4756 3084 4391 3084 \n",
       "Q 3944 3084 3684 2787 \n",
       "Q 3425 2491 3425 1978 \n",
       "L 3425 0 \n",
       "L 2847 0 \n",
       "L 2847 2094 \n",
       "Q 2847 2600 2669 2842 \n",
       "Q 2491 3084 2119 3084 \n",
       "Q 1678 3084 1418 2786 \n",
       "Q 1159 2488 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1356 3278 1631 3431 \n",
       "Q 1906 3584 2284 3584 \n",
       "Q 2666 3584 2933 3390 \n",
       "Q 3200 3197 3328 2828 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"75.195312\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-64\" x=\"102.978516\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-64\" x=\"166.455078\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"229.931641\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"291.455078\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"354.833984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-64\" x=\"386.621094\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"450.097656\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6d\" x=\"477.880859\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"575.292969\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"636.816406\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"700.195312\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"752.294922\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"780.078125\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"841.259766\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 40.603125 188.638125 \n",
       "L 40.603125 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 373.243125 188.638125 \n",
       "L 373.243125 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 40.603125 188.638125 \n",
       "L 373.243125 188.638125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 40.603125 22.318125 \n",
       "L 373.243125 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_18\">\n",
       "    <!-- Positional encoding over hidden dimensions -->\n",
       "    <g transform=\"translate(74.873437 16.318125) scale(0.12 -0.12)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \n",
       "Q 1497 1759 1228 1600 \n",
       "Q 959 1441 959 1056 \n",
       "Q 959 750 1161 570 \n",
       "Q 1363 391 1709 391 \n",
       "Q 2188 391 2477 730 \n",
       "Q 2766 1069 2766 1631 \n",
       "L 2766 1759 \n",
       "L 2194 1759 \n",
       "z\n",
       "M 3341 1997 \n",
       "L 3341 0 \n",
       "L 2766 0 \n",
       "L 2766 531 \n",
       "Q 2569 213 2275 61 \n",
       "Q 1981 -91 1556 -91 \n",
       "Q 1019 -91 701 211 \n",
       "Q 384 513 384 1019 \n",
       "Q 384 1609 779 1909 \n",
       "Q 1175 2209 1959 2209 \n",
       "L 2766 2209 \n",
       "L 2766 2266 \n",
       "Q 2766 2663 2505 2880 \n",
       "Q 2244 3097 1772 3097 \n",
       "Q 1472 3097 1187 3025 \n",
       "Q 903 2953 641 2809 \n",
       "L 641 3341 \n",
       "Q 956 3463 1253 3523 \n",
       "Q 1550 3584 1831 3584 \n",
       "Q 2591 3584 2966 3190 \n",
       "Q 3341 2797 3341 1997 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-67\" d=\"M 2906 1791 \n",
       "Q 2906 2416 2648 2759 \n",
       "Q 2391 3103 1925 3103 \n",
       "Q 1463 3103 1205 2759 \n",
       "Q 947 2416 947 1791 \n",
       "Q 947 1169 1205 825 \n",
       "Q 1463 481 1925 481 \n",
       "Q 2391 481 2648 825 \n",
       "Q 2906 1169 2906 1791 \n",
       "z\n",
       "M 3481 434 \n",
       "Q 3481 -459 3084 -895 \n",
       "Q 2688 -1331 1869 -1331 \n",
       "Q 1566 -1331 1297 -1286 \n",
       "Q 1028 -1241 775 -1147 \n",
       "L 775 -588 \n",
       "Q 1028 -725 1275 -790 \n",
       "Q 1522 -856 1778 -856 \n",
       "Q 2344 -856 2625 -561 \n",
       "Q 2906 -266 2906 331 \n",
       "L 2906 616 \n",
       "Q 2728 306 2450 153 \n",
       "Q 2172 0 1784 0 \n",
       "Q 1141 0 747 490 \n",
       "Q 353 981 353 1791 \n",
       "Q 353 2603 747 3093 \n",
       "Q 1141 3584 1784 3584 \n",
       "Q 2172 3584 2450 3431 \n",
       "Q 2728 3278 2906 2969 \n",
       "L 2906 3500 \n",
       "L 3481 3500 \n",
       "L 3481 434 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-76\" d=\"M 191 3500 \n",
       "L 800 3500 \n",
       "L 1894 563 \n",
       "L 2988 3500 \n",
       "L 3597 3500 \n",
       "L 2284 0 \n",
       "L 1503 0 \n",
       "L 191 3500 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \n",
       "Q 2534 3019 2420 3045 \n",
       "Q 2306 3072 2169 3072 \n",
       "Q 1681 3072 1420 2755 \n",
       "Q 1159 2438 1159 1844 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1341 3275 1631 3429 \n",
       "Q 1922 3584 2338 3584 \n",
       "Q 2397 3584 2469 3576 \n",
       "Q 2541 3569 2628 3553 \n",
       "L 2631 2963 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"56.677734\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-73\" x=\"117.859375\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"169.958984\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-74\" x=\"197.742188\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"236.951172\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"264.734375\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"325.916016\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-61\" x=\"389.294922\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6c\" x=\"450.574219\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"478.357422\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" x=\"510.144531\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"571.667969\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-63\" x=\"635.046875\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"690.027344\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-64\" x=\"751.208984\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"814.685547\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"842.46875\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-67\" x=\"905.847656\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"969.324219\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"1001.111328\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-76\" x=\"1062.292969\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" x=\"1121.472656\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-72\" x=\"1182.996094\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"1224.109375\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-68\" x=\"1255.896484\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"1319.275391\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-64\" x=\"1347.058594\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-64\" x=\"1410.535156\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" x=\"1474.011719\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"1535.535156\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"1598.914062\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-64\" x=\"1630.701172\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"1694.177734\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6d\" x=\"1721.960938\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" x=\"1819.373047\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"1880.896484\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-73\" x=\"1944.275391\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"1996.375\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"2024.158203\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"2085.339844\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-73\" x=\"2148.71875\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_2\">\n",
       "   <g id=\"patch_7\">\n",
       "    <path d=\"M 395.563125 188.638125 \n",
       "L 403.879125 188.638125 \n",
       "L 403.879125 22.318125 \n",
       "L 395.563125 22.318125 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_8\">\n",
       "    <path clip-path=\"url(#p171228e815)\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.01; stroke-linejoin: miter\"/>\n",
       "   </g>\n",
       "   <image xlink:href=\"data:image/png;base64,\n",
       "iVBORw0KGgoAAAANSUhEUgAAAAwAAADnCAYAAAAq9remAAABLElEQVR4nO2YSw6DMBBDZ5KQa/R4vf+aX6/Qh2RkQrrGsj1vElDzG58zwK/VJI9HtJpM4ejQC3aAgoIeH2Sshg4dgnDsMIJDwZEguZYVC2gHKsgCI3EHLlBHqktlggsdMDj5atAXip4D7lA7BUdL6znwS4AfUf29RCMti9qBTklfGoO7gYN++XpTO1R8e7+QNI/UOhMELw3BRVEL8JQcI1FwFziop5R8rPg86JePRlrwifNb78QOVJDybcUdqCAr/Py5ocMNU/IDx8+DGlxOcH8I0B/GEdGO44AOJ/PgAsNIhh1GcDAUGJbe913sMMJYDR304AxLj+AwwT3UYds2scMUeAgm6YcKRgCH3w8jdNCDMxyrYSQsWNdV7DDCWA0jvfJufaVggnuowA/cD++JW/ZYqqiKAAAAAElFTkSuQmCC\" id=\"imageb23236176e\" transform=\"scale(1 -1) translate(0 -166.32)\" x=\"395.28\" y=\"-22.32\" width=\"8.64\" height=\"166.32\"/>\n",
       "   <g id=\"matplotlib.axis_3\"/>\n",
       "   <g id=\"matplotlib.axis_4\">\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_16\">\n",
       "      <defs>\n",
       "       <path id=\"m636dd757f9\" d=\"M 0 0 \n",
       "L 3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m636dd757f9\" x=\"403.879125\" y=\"167.848836\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_19\">\n",
       "      <!-- 0.75 -->\n",
       "      <g transform=\"translate(410.879125 171.648055) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-2212\" d=\"M 678 2272 \n",
       "L 4684 2272 \n",
       "L 4684 1741 \n",
       "L 678 1741 \n",
       "L 678 2272 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \n",
       "L 1344 794 \n",
       "L 1344 0 \n",
       "L 684 0 \n",
       "L 684 794 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"83.789062\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-37\" x=\"179.199219\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"242.822266\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_7\">\n",
       "     <g id=\"line2d_17\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m636dd757f9\" x=\"403.879125\" y=\"147.058735\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_20\">\n",
       "      <!-- 0.50 -->\n",
       "      <g transform=\"translate(410.879125 150.857953) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"83.789062\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"179.199219\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"242.822266\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_8\">\n",
       "     <g id=\"line2d_18\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m636dd757f9\" x=\"403.879125\" y=\"126.268633\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_21\">\n",
       "      <!-- 0.25 -->\n",
       "      <g transform=\"translate(410.879125 130.067852) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"83.789062\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"179.199219\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"242.822266\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_9\">\n",
       "     <g id=\"line2d_19\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m636dd757f9\" x=\"403.879125\" y=\"105.478531\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_22\">\n",
       "      <!-- 0.00 -->\n",
       "      <g transform=\"translate(410.879125 109.27775) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_10\">\n",
       "     <g id=\"line2d_20\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m636dd757f9\" x=\"403.879125\" y=\"84.68843\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_23\">\n",
       "      <!-- 0.25 -->\n",
       "      <g transform=\"translate(410.879125 88.487649) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_11\">\n",
       "     <g id=\"line2d_21\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m636dd757f9\" x=\"403.879125\" y=\"63.898328\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_24\">\n",
       "      <!-- 0.50 -->\n",
       "      <g transform=\"translate(410.879125 67.697547) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_12\">\n",
       "     <g id=\"line2d_22\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m636dd757f9\" x=\"403.879125\" y=\"43.108227\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_25\">\n",
       "      <!-- 0.75 -->\n",
       "      <g transform=\"translate(410.879125 46.907445) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-37\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_13\">\n",
       "     <g id=\"line2d_23\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m636dd757f9\" x=\"403.879125\" y=\"22.318125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_26\">\n",
       "      <!-- 1.00 -->\n",
       "      <g transform=\"translate(410.879125 26.117344) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"LineCollection_1\"/>\n",
       "   <g id=\"patch_9\">\n",
       "    <path d=\"M 395.563125 188.638125 \n",
       "L 399.721125 188.638125 \n",
       "L 403.879125 188.638125 \n",
       "L 403.879125 22.318125 \n",
       "L 399.721125 22.318125 \n",
       "L 395.563125 22.318125 \n",
       "L 395.563125 188.638125 \n",
       "z\n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"pb83f263e01\">\n",
       "   <rect x=\"40.603125\" y=\"22.318125\" width=\"332.64\" height=\"166.32\"/>\n",
       "  </clipPath>\n",
       "  <clipPath id=\"p171228e815\">\n",
       "   <rect x=\"395.563125\" y=\"22.318125\" width=\"8.316\" height=\"166.32\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 800x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create encoding block, bind to access positional encoding (module has no parameters)\n",
    "encod_block = PositionalEncoding(d_model=48, max_len=96).bind({})\n",
    "# Obtain positional encodings as numpy array\n",
    "pe = jax.device_get(encod_block.pe.squeeze().T)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8,3))\n",
    "pos = ax.imshow(pe, cmap=\"RdGy\", extent=(1,pe.shape[1]+1,pe.shape[0]+1,1))\n",
    "fig.colorbar(pos, ax=ax)\n",
    "ax.set_xlabel(\"Position in sequence\")\n",
    "ax.set_ylabel(\"Hidden dimension\")\n",
    "ax.set_title(\"Positional encoding over hidden dimensions\")\n",
    "ax.set_xticks([1]+[i*10 for i in range(1,1+pe.shape[1]//10)])\n",
    "ax.set_yticks([1]+[i*10 for i in range(1,1+pe.shape[0]//10)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can clearly see the sine and cosine waves with different wavelengths that encode the position in the hidden dimensions. Specifically, we can look at the sine/cosine wave for each hidden dimension separately, to get a better intuition of the pattern. Below we visualize the positional encoding for the hidden dimensions $1$, $2$, $3$ and $4$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/pdf": "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\n",
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"721.920312pt\" height=\"284.134375pt\" viewBox=\"0 0 721.920312 284.134375\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2022-10-17T15:54:14.305134</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.6.1, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 284.134375 \n",
       "L 721.920312 284.134375 \n",
       "L 721.920312 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 45.120313 101.518125 \n",
       "L 349.483949 101.518125 \n",
       "L 349.483949 22.318125 \n",
       "L 45.120313 22.318125 \n",
       "z\n",
       "\" style=\"fill: #eaeaf2\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <path d=\"M 58.955023 101.518125 \n",
       "L 58.955023 22.318125 \n",
       "\" clip-path=\"url(#pb2b368aede)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 1 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(55.773773 118.616563) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <path d=\"M 77.401304 101.518125 \n",
       "L 77.401304 22.318125 \n",
       "\" clip-path=\"url(#pb2b368aede)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 2 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(74.220054 118.616563) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <path d=\"M 95.847585 101.518125 \n",
       "L 95.847585 22.318125 \n",
       "\" clip-path=\"url(#pb2b368aede)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 3 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(92.666335 118.616563) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \n",
       "Q 3050 2419 3304 2112 \n",
       "Q 3559 1806 3559 1356 \n",
       "Q 3559 666 3084 287 \n",
       "Q 2609 -91 1734 -91 \n",
       "Q 1441 -91 1130 -33 \n",
       "Q 819 25 488 141 \n",
       "L 488 750 \n",
       "Q 750 597 1062 519 \n",
       "Q 1375 441 1716 441 \n",
       "Q 2309 441 2620 675 \n",
       "Q 2931 909 2931 1356 \n",
       "Q 2931 1769 2642 2001 \n",
       "Q 2353 2234 1838 2234 \n",
       "L 1294 2234 \n",
       "L 1294 2753 \n",
       "L 1863 2753 \n",
       "Q 2328 2753 2575 2939 \n",
       "Q 2822 3125 2822 3475 \n",
       "Q 2822 3834 2567 4026 \n",
       "Q 2313 4219 1838 4219 \n",
       "Q 1578 4219 1281 4162 \n",
       "Q 984 4106 628 3988 \n",
       "L 628 4550 \n",
       "Q 988 4650 1302 4700 \n",
       "Q 1616 4750 1894 4750 \n",
       "Q 2613 4750 3031 4423 \n",
       "Q 3450 4097 3450 3541 \n",
       "Q 3450 3153 3228 2886 \n",
       "Q 3006 2619 2597 2516 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-33\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <path d=\"M 114.293866 101.518125 \n",
       "L 114.293866 22.318125 \n",
       "\" clip-path=\"url(#pb2b368aede)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 4 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(111.112616 118.616563) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <path d=\"M 132.740147 101.518125 \n",
       "L 132.740147 22.318125 \n",
       "\" clip-path=\"url(#pb2b368aede)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 5 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(129.558897 118.616563) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-35\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <path d=\"M 151.186428 101.518125 \n",
       "L 151.186428 22.318125 \n",
       "\" clip-path=\"url(#pb2b368aede)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 6 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(148.005178 118.616563) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \n",
       "Q 1688 2584 1439 2293 \n",
       "Q 1191 2003 1191 1497 \n",
       "Q 1191 994 1439 701 \n",
       "Q 1688 409 2113 409 \n",
       "Q 2538 409 2786 701 \n",
       "Q 3034 994 3034 1497 \n",
       "Q 3034 2003 2786 2293 \n",
       "Q 2538 2584 2113 2584 \n",
       "z\n",
       "M 3366 4563 \n",
       "L 3366 3988 \n",
       "Q 3128 4100 2886 4159 \n",
       "Q 2644 4219 2406 4219 \n",
       "Q 1781 4219 1451 3797 \n",
       "Q 1122 3375 1075 2522 \n",
       "Q 1259 2794 1537 2939 \n",
       "Q 1816 3084 2150 3084 \n",
       "Q 2853 3084 3261 2657 \n",
       "Q 3669 2231 3669 1497 \n",
       "Q 3669 778 3244 343 \n",
       "Q 2819 -91 2113 -91 \n",
       "Q 1303 -91 875 529 \n",
       "Q 447 1150 447 2328 \n",
       "Q 447 3434 972 4092 \n",
       "Q 1497 4750 2381 4750 \n",
       "Q 2619 4750 2861 4703 \n",
       "Q 3103 4656 3366 4563 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-36\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_7\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <path d=\"M 169.632709 101.518125 \n",
       "L 169.632709 22.318125 \n",
       "\" clip-path=\"url(#pb2b368aede)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 7 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(166.451459 118.616563) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-37\" d=\"M 525 4666 \n",
       "L 3525 4666 \n",
       "L 3525 4397 \n",
       "L 1831 0 \n",
       "L 1172 0 \n",
       "L 2766 4134 \n",
       "L 525 4134 \n",
       "L 525 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-37\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_8\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <path d=\"M 188.07899 101.518125 \n",
       "L 188.07899 22.318125 \n",
       "\" clip-path=\"url(#pb2b368aede)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 8 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(184.89774 118.616563) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \n",
       "Q 1584 2216 1326 1975 \n",
       "Q 1069 1734 1069 1313 \n",
       "Q 1069 891 1326 650 \n",
       "Q 1584 409 2034 409 \n",
       "Q 2484 409 2743 651 \n",
       "Q 3003 894 3003 1313 \n",
       "Q 3003 1734 2745 1975 \n",
       "Q 2488 2216 2034 2216 \n",
       "z\n",
       "M 1403 2484 \n",
       "Q 997 2584 770 2862 \n",
       "Q 544 3141 544 3541 \n",
       "Q 544 4100 942 4425 \n",
       "Q 1341 4750 2034 4750 \n",
       "Q 2731 4750 3128 4425 \n",
       "Q 3525 4100 3525 3541 \n",
       "Q 3525 3141 3298 2862 \n",
       "Q 3072 2584 2669 2484 \n",
       "Q 3125 2378 3379 2068 \n",
       "Q 3634 1759 3634 1313 \n",
       "Q 3634 634 3220 271 \n",
       "Q 2806 -91 2034 -91 \n",
       "Q 1263 -91 848 271 \n",
       "Q 434 634 434 1313 \n",
       "Q 434 1759 690 2068 \n",
       "Q 947 2378 1403 2484 \n",
       "z\n",
       "M 1172 3481 \n",
       "Q 1172 3119 1398 2916 \n",
       "Q 1625 2713 2034 2713 \n",
       "Q 2441 2713 2670 2916 \n",
       "Q 2900 3119 2900 3481 \n",
       "Q 2900 3844 2670 4047 \n",
       "Q 2441 4250 2034 4250 \n",
       "Q 1625 4250 1398 4047 \n",
       "Q 1172 3844 1172 3481 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-38\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_9\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <path d=\"M 206.525271 101.518125 \n",
       "L 206.525271 22.318125 \n",
       "\" clip-path=\"url(#pb2b368aede)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 9 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(203.344021 118.616563) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-39\" d=\"M 703 97 \n",
       "L 703 672 \n",
       "Q 941 559 1184 500 \n",
       "Q 1428 441 1663 441 \n",
       "Q 2288 441 2617 861 \n",
       "Q 2947 1281 2994 2138 \n",
       "Q 2813 1869 2534 1725 \n",
       "Q 2256 1581 1919 1581 \n",
       "Q 1219 1581 811 2004 \n",
       "Q 403 2428 403 3163 \n",
       "Q 403 3881 828 4315 \n",
       "Q 1253 4750 1959 4750 \n",
       "Q 2769 4750 3195 4129 \n",
       "Q 3622 3509 3622 2328 \n",
       "Q 3622 1225 3098 567 \n",
       "Q 2575 -91 1691 -91 \n",
       "Q 1453 -91 1209 -44 \n",
       "Q 966 3 703 97 \n",
       "z\n",
       "M 1959 2075 \n",
       "Q 2384 2075 2632 2365 \n",
       "Q 2881 2656 2881 3163 \n",
       "Q 2881 3666 2632 3958 \n",
       "Q 2384 4250 1959 4250 \n",
       "Q 1534 4250 1286 3958 \n",
       "Q 1038 3666 1038 3163 \n",
       "Q 1038 2656 1286 2365 \n",
       "Q 1534 2075 1959 2075 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-39\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_10\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <path d=\"M 224.971552 101.518125 \n",
       "L 224.971552 22.318125 \n",
       "\" clip-path=\"url(#pb2b368aede)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 10 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(218.609052 118.616563) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_11\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <path d=\"M 243.417833 101.518125 \n",
       "L 243.417833 22.318125 \n",
       "\" clip-path=\"url(#pb2b368aede)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 11 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(237.055333 118.616563) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_12\">\n",
       "     <g id=\"line2d_12\">\n",
       "      <path d=\"M 261.864114 101.518125 \n",
       "L 261.864114 22.318125 \n",
       "\" clip-path=\"url(#pb2b368aede)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 12 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(255.501614 118.616563) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_13\">\n",
       "     <g id=\"line2d_13\">\n",
       "      <path d=\"M 280.310395 101.518125 \n",
       "L 280.310395 22.318125 \n",
       "\" clip-path=\"url(#pb2b368aede)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_13\">\n",
       "      <!-- 13 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(273.947895 118.616563) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-33\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_14\">\n",
       "     <g id=\"line2d_14\">\n",
       "      <path d=\"M 298.756676 101.518125 \n",
       "L 298.756676 22.318125 \n",
       "\" clip-path=\"url(#pb2b368aede)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_14\">\n",
       "      <!-- 14 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(292.394176 118.616563) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-34\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_15\">\n",
       "     <g id=\"line2d_15\">\n",
       "      <path d=\"M 317.202957 101.518125 \n",
       "L 317.202957 22.318125 \n",
       "\" clip-path=\"url(#pb2b368aede)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_15\">\n",
       "      <!-- 15 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(310.840457 118.616563) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_16\">\n",
       "     <g id=\"line2d_16\">\n",
       "      <path d=\"M 335.649238 101.518125 \n",
       "L 335.649238 22.318125 \n",
       "\" clip-path=\"url(#pb2b368aede)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_16\">\n",
       "      <!-- 16 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(329.286738 118.616563) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-36\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_17\">\n",
       "     <!-- Position in sequence -->\n",
       "     <g style=\"fill: #262626\" transform=\"translate(146.006818 132.294688) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-50\" d=\"M 1259 4147 \n",
       "L 1259 2394 \n",
       "L 2053 2394 \n",
       "Q 2494 2394 2734 2622 \n",
       "Q 2975 2850 2975 3272 \n",
       "Q 2975 3691 2734 3919 \n",
       "Q 2494 4147 2053 4147 \n",
       "L 1259 4147 \n",
       "z\n",
       "M 628 4666 \n",
       "L 2053 4666 \n",
       "Q 2838 4666 3239 4311 \n",
       "Q 3641 3956 3641 3272 \n",
       "Q 3641 2581 3239 2228 \n",
       "Q 2838 1875 2053 1875 \n",
       "L 1259 1875 \n",
       "L 1259 0 \n",
       "L 628 0 \n",
       "L 628 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \n",
       "Q 1497 3097 1228 2736 \n",
       "Q 959 2375 959 1747 \n",
       "Q 959 1119 1226 758 \n",
       "Q 1494 397 1959 397 \n",
       "Q 2419 397 2687 759 \n",
       "Q 2956 1122 2956 1747 \n",
       "Q 2956 2369 2687 2733 \n",
       "Q 2419 3097 1959 3097 \n",
       "z\n",
       "M 1959 3584 \n",
       "Q 2709 3584 3137 3096 \n",
       "Q 3566 2609 3566 1747 \n",
       "Q 3566 888 3137 398 \n",
       "Q 2709 -91 1959 -91 \n",
       "Q 1206 -91 779 398 \n",
       "Q 353 888 353 1747 \n",
       "Q 353 2609 779 3096 \n",
       "Q 1206 3584 1959 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \n",
       "L 2834 2853 \n",
       "Q 2591 2978 2328 3040 \n",
       "Q 2066 3103 1784 3103 \n",
       "Q 1356 3103 1142 2972 \n",
       "Q 928 2841 928 2578 \n",
       "Q 928 2378 1081 2264 \n",
       "Q 1234 2150 1697 2047 \n",
       "L 1894 2003 \n",
       "Q 2506 1872 2764 1633 \n",
       "Q 3022 1394 3022 966 \n",
       "Q 3022 478 2636 193 \n",
       "Q 2250 -91 1575 -91 \n",
       "Q 1294 -91 989 -36 \n",
       "Q 684 19 347 128 \n",
       "L 347 722 \n",
       "Q 666 556 975 473 \n",
       "Q 1284 391 1588 391 \n",
       "Q 1994 391 2212 530 \n",
       "Q 2431 669 2431 922 \n",
       "Q 2431 1156 2273 1281 \n",
       "Q 2116 1406 1581 1522 \n",
       "L 1381 1569 \n",
       "Q 847 1681 609 1914 \n",
       "Q 372 2147 372 2553 \n",
       "Q 372 3047 722 3315 \n",
       "Q 1072 3584 1716 3584 \n",
       "Q 2034 3584 2315 3537 \n",
       "Q 2597 3491 2834 3397 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \n",
       "L 1178 3500 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 3500 \n",
       "z\n",
       "M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 4134 \n",
       "L 603 4134 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \n",
       "L 1172 3500 \n",
       "L 2356 3500 \n",
       "L 2356 3053 \n",
       "L 1172 3053 \n",
       "L 1172 1153 \n",
       "Q 1172 725 1289 603 \n",
       "Q 1406 481 1766 481 \n",
       "L 2356 481 \n",
       "L 2356 0 \n",
       "L 1766 0 \n",
       "Q 1100 0 847 248 \n",
       "Q 594 497 594 1153 \n",
       "L 594 3053 \n",
       "L 172 3053 \n",
       "L 172 3500 \n",
       "L 594 3500 \n",
       "L 594 4494 \n",
       "L 1172 4494 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-71\" d=\"M 947 1747 \n",
       "Q 947 1113 1208 752 \n",
       "Q 1469 391 1925 391 \n",
       "Q 2381 391 2643 752 \n",
       "Q 2906 1113 2906 1747 \n",
       "Q 2906 2381 2643 2742 \n",
       "Q 2381 3103 1925 3103 \n",
       "Q 1469 3103 1208 2742 \n",
       "Q 947 2381 947 1747 \n",
       "z\n",
       "M 2906 525 \n",
       "Q 2725 213 2448 61 \n",
       "Q 2172 -91 1784 -91 \n",
       "Q 1150 -91 751 415 \n",
       "Q 353 922 353 1747 \n",
       "Q 353 2572 751 3078 \n",
       "Q 1150 3584 1784 3584 \n",
       "Q 2172 3584 2448 3432 \n",
       "Q 2725 3281 2906 2969 \n",
       "L 2906 3500 \n",
       "L 3481 3500 \n",
       "L 3481 -1331 \n",
       "L 2906 -1331 \n",
       "L 2906 525 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-75\" d=\"M 544 1381 \n",
       "L 544 3500 \n",
       "L 1119 3500 \n",
       "L 1119 1403 \n",
       "Q 1119 906 1312 657 \n",
       "Q 1506 409 1894 409 \n",
       "Q 2359 409 2629 706 \n",
       "Q 2900 1003 2900 1516 \n",
       "L 2900 3500 \n",
       "L 3475 3500 \n",
       "L 3475 0 \n",
       "L 2900 0 \n",
       "L 2900 538 \n",
       "Q 2691 219 2414 64 \n",
       "Q 2138 -91 1772 -91 \n",
       "Q 1169 -91 856 284 \n",
       "Q 544 659 544 1381 \n",
       "z\n",
       "M 1991 3584 \n",
       "L 1991 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \n",
       "L 3122 2828 \n",
       "Q 2878 2963 2633 3030 \n",
       "Q 2388 3097 2138 3097 \n",
       "Q 1578 3097 1268 2742 \n",
       "Q 959 2388 959 1747 \n",
       "Q 959 1106 1268 751 \n",
       "Q 1578 397 2138 397 \n",
       "Q 2388 397 2633 464 \n",
       "Q 2878 531 3122 666 \n",
       "L 3122 134 \n",
       "Q 2881 22 2623 -34 \n",
       "Q 2366 -91 2075 -91 \n",
       "Q 1284 -91 818 406 \n",
       "Q 353 903 353 1747 \n",
       "Q 353 2603 823 3093 \n",
       "Q 1294 3584 2113 3584 \n",
       "Q 2378 3584 2631 3529 \n",
       "Q 2884 3475 3122 3366 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"56.677734\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"117.859375\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"169.958984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-74\" x=\"197.742188\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"236.951172\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"264.734375\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"325.916016\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"389.294922\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"421.082031\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"448.865234\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"512.244141\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"544.03125\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"596.130859\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-71\" x=\"657.654297\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-75\" x=\"721.130859\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"784.509766\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"846.033203\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"909.412109\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"964.392578\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_17\">\n",
       "      <path d=\"M 45.120313 94.918125 \n",
       "L 349.483949 94.918125 \n",
       "\" clip-path=\"url(#pb2b368aede)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_18\">\n",
       "      <!-- 1 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(20.878125 98.717344) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-2212\" d=\"M 678 2272 \n",
       "L 4684 2272 \n",
       "L 4684 1741 \n",
       "L 678 1741 \n",
       "L 678 2272 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"83.789062\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_18\">\n",
       "      <path d=\"M 45.120313 61.918125 \n",
       "L 349.483949 61.918125 \n",
       "\" clip-path=\"url(#pb2b368aede)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_19\">\n",
       "      <!-- 0 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(29.257813 65.717344) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_19\">\n",
       "      <path d=\"M 45.120313 28.918125 \n",
       "L 349.483949 28.918125 \n",
       "\" clip-path=\"url(#pb2b368aede)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_20\">\n",
       "      <!-- 1 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(29.257813 32.717344) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_21\">\n",
       "     <!-- Positional encoding -->\n",
       "     <g style=\"fill: #262626\" transform=\"translate(14.798438 110.384531) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \n",
       "Q 1497 1759 1228 1600 \n",
       "Q 959 1441 959 1056 \n",
       "Q 959 750 1161 570 \n",
       "Q 1363 391 1709 391 \n",
       "Q 2188 391 2477 730 \n",
       "Q 2766 1069 2766 1631 \n",
       "L 2766 1759 \n",
       "L 2194 1759 \n",
       "z\n",
       "M 3341 1997 \n",
       "L 3341 0 \n",
       "L 2766 0 \n",
       "L 2766 531 \n",
       "Q 2569 213 2275 61 \n",
       "Q 1981 -91 1556 -91 \n",
       "Q 1019 -91 701 211 \n",
       "Q 384 513 384 1019 \n",
       "Q 384 1609 779 1909 \n",
       "Q 1175 2209 1959 2209 \n",
       "L 2766 2209 \n",
       "L 2766 2266 \n",
       "Q 2766 2663 2505 2880 \n",
       "Q 2244 3097 1772 3097 \n",
       "Q 1472 3097 1187 3025 \n",
       "Q 903 2953 641 2809 \n",
       "L 641 3341 \n",
       "Q 956 3463 1253 3523 \n",
       "Q 1550 3584 1831 3584 \n",
       "Q 2591 3584 2966 3190 \n",
       "Q 3341 2797 3341 1997 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-64\" d=\"M 2906 2969 \n",
       "L 2906 4863 \n",
       "L 3481 4863 \n",
       "L 3481 0 \n",
       "L 2906 0 \n",
       "L 2906 525 \n",
       "Q 2725 213 2448 61 \n",
       "Q 2172 -91 1784 -91 \n",
       "Q 1150 -91 751 415 \n",
       "Q 353 922 353 1747 \n",
       "Q 353 2572 751 3078 \n",
       "Q 1150 3584 1784 3584 \n",
       "Q 2172 3584 2448 3432 \n",
       "Q 2725 3281 2906 2969 \n",
       "z\n",
       "M 947 1747 \n",
       "Q 947 1113 1208 752 \n",
       "Q 1469 391 1925 391 \n",
       "Q 2381 391 2643 752 \n",
       "Q 2906 1113 2906 1747 \n",
       "Q 2906 2381 2643 2742 \n",
       "Q 2381 3103 1925 3103 \n",
       "Q 1469 3103 1208 2742 \n",
       "Q 947 2381 947 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-67\" d=\"M 2906 1791 \n",
       "Q 2906 2416 2648 2759 \n",
       "Q 2391 3103 1925 3103 \n",
       "Q 1463 3103 1205 2759 \n",
       "Q 947 2416 947 1791 \n",
       "Q 947 1169 1205 825 \n",
       "Q 1463 481 1925 481 \n",
       "Q 2391 481 2648 825 \n",
       "Q 2906 1169 2906 1791 \n",
       "z\n",
       "M 3481 434 \n",
       "Q 3481 -459 3084 -895 \n",
       "Q 2688 -1331 1869 -1331 \n",
       "Q 1566 -1331 1297 -1286 \n",
       "Q 1028 -1241 775 -1147 \n",
       "L 775 -588 \n",
       "Q 1028 -725 1275 -790 \n",
       "Q 1522 -856 1778 -856 \n",
       "Q 2344 -856 2625 -561 \n",
       "Q 2906 -266 2906 331 \n",
       "L 2906 616 \n",
       "Q 2728 306 2450 153 \n",
       "Q 2172 0 1784 0 \n",
       "Q 1141 0 747 490 \n",
       "Q 353 981 353 1791 \n",
       "Q 353 2603 747 3093 \n",
       "Q 1141 3584 1784 3584 \n",
       "Q 2172 3584 2450 3431 \n",
       "Q 2728 3278 2906 2969 \n",
       "L 2906 3500 \n",
       "L 3481 3500 \n",
       "L 3481 434 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"56.677734\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"117.859375\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"169.958984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-74\" x=\"197.742188\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"236.951172\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"264.734375\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"325.916016\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"389.294922\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"450.574219\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"478.357422\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"510.144531\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"571.667969\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"635.046875\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"690.027344\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-64\" x=\"751.208984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"814.685547\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"842.46875\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-67\" x=\"905.847656\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_20\">\n",
       "    <path d=\"M 58.955023 61.918125 \n",
       "L 77.401304 34.149583 \n",
       "L 95.847585 31.911311 \n",
       "L 114.293866 57.261165 \n",
       "L 132.740147 86.892607 \n",
       "L 151.186428 93.562627 \n",
       "L 169.632709 71.138836 \n",
       "L 188.07899 40.237567 \n",
       "L 206.525271 29.269303 \n",
       "L 224.971552 48.318215 \n",
       "L 243.417833 79.870822 \n",
       "L 261.864114 94.917802 \n",
       "L 280.310395 79.625032 \n",
       "L 298.756676 48.052613 \n",
       "L 317.202957 29.228081 \n",
       "L 335.649238 40.458625 \n",
       "\" clip-path=\"url(#pb2b368aede)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: round\"/>\n",
       "    <defs>\n",
       "     <path id=\"m02846df96d\" d=\"M 0 3 \n",
       "C 0.795609 3 1.55874 2.683901 2.12132 2.12132 \n",
       "C 2.683901 1.55874 3 0.795609 3 0 \n",
       "C 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \n",
       "C 1.55874 -2.683901 0.795609 -3 0 -3 \n",
       "C -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \n",
       "C -2.683901 -1.55874 -3 -0.795609 -3 0 \n",
       "C -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \n",
       "C -1.55874 2.683901 -0.795609 3 0 3 \n",
       "z\n",
       "\" style=\"stroke: #000000\"/>\n",
       "    </defs>\n",
       "    <g clip-path=\"url(#pb2b368aede)\">\n",
       "     <use xlink:href=\"#m02846df96d\" x=\"58.955023\" y=\"61.918125\" style=\"fill: #1f77b4; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m02846df96d\" x=\"77.401304\" y=\"34.149583\" style=\"fill: #1f77b4; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m02846df96d\" x=\"95.847585\" y=\"31.911311\" style=\"fill: #1f77b4; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m02846df96d\" x=\"114.293866\" y=\"57.261165\" style=\"fill: #1f77b4; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m02846df96d\" x=\"132.740147\" y=\"86.892607\" style=\"fill: #1f77b4; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m02846df96d\" x=\"151.186428\" y=\"93.562627\" style=\"fill: #1f77b4; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m02846df96d\" x=\"169.632709\" y=\"71.138836\" style=\"fill: #1f77b4; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m02846df96d\" x=\"188.07899\" y=\"40.237567\" style=\"fill: #1f77b4; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m02846df96d\" x=\"206.525271\" y=\"29.269303\" style=\"fill: #1f77b4; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m02846df96d\" x=\"224.971552\" y=\"48.318215\" style=\"fill: #1f77b4; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m02846df96d\" x=\"243.417833\" y=\"79.870822\" style=\"fill: #1f77b4; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m02846df96d\" x=\"261.864114\" y=\"94.917802\" style=\"fill: #1f77b4; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m02846df96d\" x=\"280.310395\" y=\"79.625032\" style=\"fill: #1f77b4; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m02846df96d\" x=\"298.756676\" y=\"48.052613\" style=\"fill: #1f77b4; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m02846df96d\" x=\"317.202957\" y=\"29.228081\" style=\"fill: #1f77b4; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m02846df96d\" x=\"335.649238\" y=\"40.458625\" style=\"fill: #1f77b4; stroke: #000000\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 45.120313 101.518125 \n",
       "L 45.120313 22.318125 \n",
       "\" style=\"fill: none; stroke: #ffffff; stroke-width: 1.25; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 349.483949 101.518125 \n",
       "L 349.483949 22.318125 \n",
       "\" style=\"fill: none; stroke: #ffffff; stroke-width: 1.25; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 45.120313 101.518125 \n",
       "L 349.483949 101.518125 \n",
       "\" style=\"fill: none; stroke: #ffffff; stroke-width: 1.25; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 45.120313 22.318125 \n",
       "L 349.483949 22.318125 \n",
       "\" style=\"fill: none; stroke: #ffffff; stroke-width: 1.25; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_22\">\n",
       "    <!-- Encoding in hidden dimension 1 -->\n",
       "    <g style=\"fill: #262626\" transform=\"translate(101.073381 16.318125) scale(0.12 -0.12)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-45\" d=\"M 628 4666 \n",
       "L 3578 4666 \n",
       "L 3578 4134 \n",
       "L 1259 4134 \n",
       "L 1259 2753 \n",
       "L 3481 2753 \n",
       "L 3481 2222 \n",
       "L 1259 2222 \n",
       "L 1259 531 \n",
       "L 3634 531 \n",
       "L 3634 0 \n",
       "L 628 0 \n",
       "L 628 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-6d\" d=\"M 3328 2828 \n",
       "Q 3544 3216 3844 3400 \n",
       "Q 4144 3584 4550 3584 \n",
       "Q 5097 3584 5394 3201 \n",
       "Q 5691 2819 5691 2113 \n",
       "L 5691 0 \n",
       "L 5113 0 \n",
       "L 5113 2094 \n",
       "Q 5113 2597 4934 2840 \n",
       "Q 4756 3084 4391 3084 \n",
       "Q 3944 3084 3684 2787 \n",
       "Q 3425 2491 3425 1978 \n",
       "L 3425 0 \n",
       "L 2847 0 \n",
       "L 2847 2094 \n",
       "Q 2847 2600 2669 2842 \n",
       "Q 2491 3084 2119 3084 \n",
       "Q 1678 3084 1418 2786 \n",
       "Q 1159 2488 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1356 3278 1631 3431 \n",
       "Q 1906 3584 2284 3584 \n",
       "Q 2666 3584 2933 3390 \n",
       "Q 3200 3197 3328 2828 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-45\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"63.183594\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-63\" x=\"126.5625\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"181.542969\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-64\" x=\"242.724609\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"306.201172\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"333.984375\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-67\" x=\"397.363281\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"460.839844\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"492.626953\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"520.410156\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"583.789062\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-68\" x=\"615.576172\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"678.955078\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-64\" x=\"706.738281\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-64\" x=\"770.214844\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" x=\"833.691406\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"895.214844\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"958.59375\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-64\" x=\"990.380859\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"1053.857422\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6d\" x=\"1081.640625\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" x=\"1179.052734\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"1240.576172\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-73\" x=\"1303.955078\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"1356.054688\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"1383.837891\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"1445.019531\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"1508.398438\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-31\" x=\"1540.185547\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_2\">\n",
       "   <g id=\"patch_7\">\n",
       "    <path d=\"M 410.356676 101.518125 \n",
       "L 714.720312 101.518125 \n",
       "L 714.720312 22.318125 \n",
       "L 410.356676 22.318125 \n",
       "z\n",
       "\" style=\"fill: #eaeaf2\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_3\">\n",
       "    <g id=\"xtick_17\">\n",
       "     <g id=\"line2d_21\">\n",
       "      <path d=\"M 424.191387 101.518125 \n",
       "L 424.191387 22.318125 \n",
       "\" clip-path=\"url(#pa32695a371)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_23\">\n",
       "      <!-- 1 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(421.010137 118.616563) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_18\">\n",
       "     <g id=\"line2d_22\">\n",
       "      <path d=\"M 442.637668 101.518125 \n",
       "L 442.637668 22.318125 \n",
       "\" clip-path=\"url(#pa32695a371)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_24\">\n",
       "      <!-- 2 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(439.456418 118.616563) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_19\">\n",
       "     <g id=\"line2d_23\">\n",
       "      <path d=\"M 461.083949 101.518125 \n",
       "L 461.083949 22.318125 \n",
       "\" clip-path=\"url(#pa32695a371)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_25\">\n",
       "      <!-- 3 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(457.902699 118.616563) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-33\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_20\">\n",
       "     <g id=\"line2d_24\">\n",
       "      <path d=\"M 479.53023 101.518125 \n",
       "L 479.53023 22.318125 \n",
       "\" clip-path=\"url(#pa32695a371)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_26\">\n",
       "      <!-- 4 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(476.34898 118.616563) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_21\">\n",
       "     <g id=\"line2d_25\">\n",
       "      <path d=\"M 497.976511 101.518125 \n",
       "L 497.976511 22.318125 \n",
       "\" clip-path=\"url(#pa32695a371)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_27\">\n",
       "      <!-- 5 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(494.795261 118.616563) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-35\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_22\">\n",
       "     <g id=\"line2d_26\">\n",
       "      <path d=\"M 516.422792 101.518125 \n",
       "L 516.422792 22.318125 \n",
       "\" clip-path=\"url(#pa32695a371)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_28\">\n",
       "      <!-- 6 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(513.241542 118.616563) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-36\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_23\">\n",
       "     <g id=\"line2d_27\">\n",
       "      <path d=\"M 534.869073 101.518125 \n",
       "L 534.869073 22.318125 \n",
       "\" clip-path=\"url(#pa32695a371)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_29\">\n",
       "      <!-- 7 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(531.687823 118.616563) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-37\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_24\">\n",
       "     <g id=\"line2d_28\">\n",
       "      <path d=\"M 553.315354 101.518125 \n",
       "L 553.315354 22.318125 \n",
       "\" clip-path=\"url(#pa32695a371)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_30\">\n",
       "      <!-- 8 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(550.134104 118.616563) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-38\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_25\">\n",
       "     <g id=\"line2d_29\">\n",
       "      <path d=\"M 571.761635 101.518125 \n",
       "L 571.761635 22.318125 \n",
       "\" clip-path=\"url(#pa32695a371)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_31\">\n",
       "      <!-- 9 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(568.580385 118.616563) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-39\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_26\">\n",
       "     <g id=\"line2d_30\">\n",
       "      <path d=\"M 590.207916 101.518125 \n",
       "L 590.207916 22.318125 \n",
       "\" clip-path=\"url(#pa32695a371)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_32\">\n",
       "      <!-- 10 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(583.845416 118.616563) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_27\">\n",
       "     <g id=\"line2d_31\">\n",
       "      <path d=\"M 608.654197 101.518125 \n",
       "L 608.654197 22.318125 \n",
       "\" clip-path=\"url(#pa32695a371)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_33\">\n",
       "      <!-- 11 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(602.291697 118.616563) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_28\">\n",
       "     <g id=\"line2d_32\">\n",
       "      <path d=\"M 627.100478 101.518125 \n",
       "L 627.100478 22.318125 \n",
       "\" clip-path=\"url(#pa32695a371)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_34\">\n",
       "      <!-- 12 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(620.737978 118.616563) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_29\">\n",
       "     <g id=\"line2d_33\">\n",
       "      <path d=\"M 645.546759 101.518125 \n",
       "L 645.546759 22.318125 \n",
       "\" clip-path=\"url(#pa32695a371)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_35\">\n",
       "      <!-- 13 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(639.184259 118.616563) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-33\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_30\">\n",
       "     <g id=\"line2d_34\">\n",
       "      <path d=\"M 663.99304 101.518125 \n",
       "L 663.99304 22.318125 \n",
       "\" clip-path=\"url(#pa32695a371)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_36\">\n",
       "      <!-- 14 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(657.63054 118.616563) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-34\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_31\">\n",
       "     <g id=\"line2d_35\">\n",
       "      <path d=\"M 682.439321 101.518125 \n",
       "L 682.439321 22.318125 \n",
       "\" clip-path=\"url(#pa32695a371)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_37\">\n",
       "      <!-- 15 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(676.076821 118.616563) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_32\">\n",
       "     <g id=\"line2d_36\">\n",
       "      <path d=\"M 700.885602 101.518125 \n",
       "L 700.885602 22.318125 \n",
       "\" clip-path=\"url(#pa32695a371)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_38\">\n",
       "      <!-- 16 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(694.523102 118.616563) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-36\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_39\">\n",
       "     <!-- Position in sequence -->\n",
       "     <g style=\"fill: #262626\" transform=\"translate(511.243182 132.294688) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"56.677734\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"117.859375\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"169.958984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-74\" x=\"197.742188\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"236.951172\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"264.734375\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"325.916016\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"389.294922\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"421.082031\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"448.865234\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"512.244141\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"544.03125\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"596.130859\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-71\" x=\"657.654297\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-75\" x=\"721.130859\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"784.509766\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"846.033203\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"909.412109\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"964.392578\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_4\">\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_37\">\n",
       "      <path d=\"M 410.356676 94.918125 \n",
       "L 714.720312 94.918125 \n",
       "\" clip-path=\"url(#pa32695a371)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_40\">\n",
       "      <!-- 1 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(386.114489 98.717344) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"83.789062\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_38\">\n",
       "      <path d=\"M 410.356676 61.918125 \n",
       "L 714.720312 61.918125 \n",
       "\" clip-path=\"url(#pa32695a371)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_41\">\n",
       "      <!-- 0 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(394.494176 65.717344) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_39\">\n",
       "      <path d=\"M 410.356676 28.918125 \n",
       "L 714.720312 28.918125 \n",
       "\" clip-path=\"url(#pa32695a371)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_42\">\n",
       "      <!-- 1 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(394.494176 32.717344) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_43\">\n",
       "     <!-- Positional encoding -->\n",
       "     <g style=\"fill: #262626\" transform=\"translate(380.034801 110.384531) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"56.677734\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"117.859375\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"169.958984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-74\" x=\"197.742188\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"236.951172\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"264.734375\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"325.916016\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"389.294922\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"450.574219\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"478.357422\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"510.144531\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"571.667969\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"635.046875\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"690.027344\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-64\" x=\"751.208984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"814.685547\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"842.46875\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-67\" x=\"905.847656\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_40\">\n",
       "    <path d=\"M 424.191387 28.918125 \n",
       "L 442.637668 44.08815 \n",
       "L 461.083949 75.650971 \n",
       "L 479.53023 94.587877 \n",
       "L 497.976511 83.488364 \n",
       "L 516.422792 52.557272 \n",
       "L 534.869073 30.232506 \n",
       "L 553.315354 37.039351 \n",
       "L 571.761635 66.719626 \n",
       "L 590.207916 91.985423 \n",
       "L 608.654197 89.607485 \n",
       "L 627.100478 61.772077 \n",
       "L 645.546759 34.070945 \n",
       "L 663.99304 31.972381 \n",
       "L 682.439321 57.405797 \n",
       "L 700.885602 86.987826 \n",
       "\" clip-path=\"url(#pa32695a371)\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: round\"/>\n",
       "    <defs>\n",
       "     <path id=\"m5d53970679\" d=\"M 0 3 \n",
       "C 0.795609 3 1.55874 2.683901 2.12132 2.12132 \n",
       "C 2.683901 1.55874 3 0.795609 3 0 \n",
       "C 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \n",
       "C 1.55874 -2.683901 0.795609 -3 0 -3 \n",
       "C -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \n",
       "C -2.683901 -1.55874 -3 -0.795609 -3 0 \n",
       "C -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \n",
       "C -1.55874 2.683901 -0.795609 3 0 3 \n",
       "z\n",
       "\" style=\"stroke: #000000\"/>\n",
       "    </defs>\n",
       "    <g clip-path=\"url(#pa32695a371)\">\n",
       "     <use xlink:href=\"#m5d53970679\" x=\"424.191387\" y=\"28.918125\" style=\"fill: #ff7f0e; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m5d53970679\" x=\"442.637668\" y=\"44.08815\" style=\"fill: #ff7f0e; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m5d53970679\" x=\"461.083949\" y=\"75.650971\" style=\"fill: #ff7f0e; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m5d53970679\" x=\"479.53023\" y=\"94.587877\" style=\"fill: #ff7f0e; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m5d53970679\" x=\"497.976511\" y=\"83.488364\" style=\"fill: #ff7f0e; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m5d53970679\" x=\"516.422792\" y=\"52.557272\" style=\"fill: #ff7f0e; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m5d53970679\" x=\"534.869073\" y=\"30.232506\" style=\"fill: #ff7f0e; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m5d53970679\" x=\"553.315354\" y=\"37.039351\" style=\"fill: #ff7f0e; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m5d53970679\" x=\"571.761635\" y=\"66.719626\" style=\"fill: #ff7f0e; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m5d53970679\" x=\"590.207916\" y=\"91.985423\" style=\"fill: #ff7f0e; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m5d53970679\" x=\"608.654197\" y=\"89.607485\" style=\"fill: #ff7f0e; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m5d53970679\" x=\"627.100478\" y=\"61.772077\" style=\"fill: #ff7f0e; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m5d53970679\" x=\"645.546759\" y=\"34.070945\" style=\"fill: #ff7f0e; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m5d53970679\" x=\"663.99304\" y=\"31.972381\" style=\"fill: #ff7f0e; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m5d53970679\" x=\"682.439321\" y=\"57.405797\" style=\"fill: #ff7f0e; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m5d53970679\" x=\"700.885602\" y=\"86.987826\" style=\"fill: #ff7f0e; stroke: #000000\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"patch_8\">\n",
       "    <path d=\"M 410.356676 101.518125 \n",
       "L 410.356676 22.318125 \n",
       "\" style=\"fill: none; stroke: #ffffff; stroke-width: 1.25; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_9\">\n",
       "    <path d=\"M 714.720312 101.518125 \n",
       "L 714.720312 22.318125 \n",
       "\" style=\"fill: none; stroke: #ffffff; stroke-width: 1.25; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_10\">\n",
       "    <path d=\"M 410.356676 101.518125 \n",
       "L 714.720312 101.518125 \n",
       "\" style=\"fill: none; stroke: #ffffff; stroke-width: 1.25; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_11\">\n",
       "    <path d=\"M 410.356676 22.318125 \n",
       "L 714.720312 22.318125 \n",
       "\" style=\"fill: none; stroke: #ffffff; stroke-width: 1.25; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_44\">\n",
       "    <!-- Encoding in hidden dimension 2 -->\n",
       "    <g style=\"fill: #262626\" transform=\"translate(466.309744 16.318125) scale(0.12 -0.12)\">\n",
       "     <use xlink:href=\"#DejaVuSans-45\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"63.183594\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-63\" x=\"126.5625\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"181.542969\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-64\" x=\"242.724609\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"306.201172\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"333.984375\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-67\" x=\"397.363281\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"460.839844\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"492.626953\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"520.410156\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"583.789062\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-68\" x=\"615.576172\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"678.955078\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-64\" x=\"706.738281\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-64\" x=\"770.214844\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" x=\"833.691406\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"895.214844\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"958.59375\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-64\" x=\"990.380859\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"1053.857422\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6d\" x=\"1081.640625\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" x=\"1179.052734\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"1240.576172\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-73\" x=\"1303.955078\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"1356.054688\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"1383.837891\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"1445.019531\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"1508.398438\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-32\" x=\"1540.185547\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_3\">\n",
       "   <g id=\"patch_12\">\n",
       "    <path d=\"M 45.120313 244.078125 \n",
       "L 349.483949 244.078125 \n",
       "L 349.483949 164.878125 \n",
       "L 45.120313 164.878125 \n",
       "z\n",
       "\" style=\"fill: #eaeaf2\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_5\">\n",
       "    <g id=\"xtick_33\">\n",
       "     <g id=\"line2d_41\">\n",
       "      <path d=\"M 58.955023 244.078125 \n",
       "L 58.955023 164.878125 \n",
       "\" clip-path=\"url(#p16c174443a)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_45\">\n",
       "      <!-- 1 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(55.773773 261.176563) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_34\">\n",
       "     <g id=\"line2d_42\">\n",
       "      <path d=\"M 77.401304 244.078125 \n",
       "L 77.401304 164.878125 \n",
       "\" clip-path=\"url(#p16c174443a)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_46\">\n",
       "      <!-- 2 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(74.220054 261.176563) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_35\">\n",
       "     <g id=\"line2d_43\">\n",
       "      <path d=\"M 95.847585 244.078125 \n",
       "L 95.847585 164.878125 \n",
       "\" clip-path=\"url(#p16c174443a)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_47\">\n",
       "      <!-- 3 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(92.666335 261.176563) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-33\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_36\">\n",
       "     <g id=\"line2d_44\">\n",
       "      <path d=\"M 114.293866 244.078125 \n",
       "L 114.293866 164.878125 \n",
       "\" clip-path=\"url(#p16c174443a)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_48\">\n",
       "      <!-- 4 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(111.112616 261.176563) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_37\">\n",
       "     <g id=\"line2d_45\">\n",
       "      <path d=\"M 132.740147 244.078125 \n",
       "L 132.740147 164.878125 \n",
       "\" clip-path=\"url(#p16c174443a)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_49\">\n",
       "      <!-- 5 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(129.558897 261.176563) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-35\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_38\">\n",
       "     <g id=\"line2d_46\">\n",
       "      <path d=\"M 151.186428 244.078125 \n",
       "L 151.186428 164.878125 \n",
       "\" clip-path=\"url(#p16c174443a)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_50\">\n",
       "      <!-- 6 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(148.005178 261.176563) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-36\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_39\">\n",
       "     <g id=\"line2d_47\">\n",
       "      <path d=\"M 169.632709 244.078125 \n",
       "L 169.632709 164.878125 \n",
       "\" clip-path=\"url(#p16c174443a)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_51\">\n",
       "      <!-- 7 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(166.451459 261.176563) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-37\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_40\">\n",
       "     <g id=\"line2d_48\">\n",
       "      <path d=\"M 188.07899 244.078125 \n",
       "L 188.07899 164.878125 \n",
       "\" clip-path=\"url(#p16c174443a)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_52\">\n",
       "      <!-- 8 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(184.89774 261.176563) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-38\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_41\">\n",
       "     <g id=\"line2d_49\">\n",
       "      <path d=\"M 206.525271 244.078125 \n",
       "L 206.525271 164.878125 \n",
       "\" clip-path=\"url(#p16c174443a)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_53\">\n",
       "      <!-- 9 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(203.344021 261.176563) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-39\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_42\">\n",
       "     <g id=\"line2d_50\">\n",
       "      <path d=\"M 224.971552 244.078125 \n",
       "L 224.971552 164.878125 \n",
       "\" clip-path=\"url(#p16c174443a)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_54\">\n",
       "      <!-- 10 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(218.609052 261.176563) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_43\">\n",
       "     <g id=\"line2d_51\">\n",
       "      <path d=\"M 243.417833 244.078125 \n",
       "L 243.417833 164.878125 \n",
       "\" clip-path=\"url(#p16c174443a)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_55\">\n",
       "      <!-- 11 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(237.055333 261.176563) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_44\">\n",
       "     <g id=\"line2d_52\">\n",
       "      <path d=\"M 261.864114 244.078125 \n",
       "L 261.864114 164.878125 \n",
       "\" clip-path=\"url(#p16c174443a)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_56\">\n",
       "      <!-- 12 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(255.501614 261.176563) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_45\">\n",
       "     <g id=\"line2d_53\">\n",
       "      <path d=\"M 280.310395 244.078125 \n",
       "L 280.310395 164.878125 \n",
       "\" clip-path=\"url(#p16c174443a)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_57\">\n",
       "      <!-- 13 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(273.947895 261.176563) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-33\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_46\">\n",
       "     <g id=\"line2d_54\">\n",
       "      <path d=\"M 298.756676 244.078125 \n",
       "L 298.756676 164.878125 \n",
       "\" clip-path=\"url(#p16c174443a)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_58\">\n",
       "      <!-- 14 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(292.394176 261.176563) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-34\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_47\">\n",
       "     <g id=\"line2d_55\">\n",
       "      <path d=\"M 317.202957 244.078125 \n",
       "L 317.202957 164.878125 \n",
       "\" clip-path=\"url(#p16c174443a)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_59\">\n",
       "      <!-- 15 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(310.840457 261.176563) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_48\">\n",
       "     <g id=\"line2d_56\">\n",
       "      <path d=\"M 335.649238 244.078125 \n",
       "L 335.649238 164.878125 \n",
       "\" clip-path=\"url(#p16c174443a)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_60\">\n",
       "      <!-- 16 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(329.286738 261.176563) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-36\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_61\">\n",
       "     <!-- Position in sequence -->\n",
       "     <g style=\"fill: #262626\" transform=\"translate(146.006818 274.854688) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"56.677734\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"117.859375\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"169.958984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-74\" x=\"197.742188\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"236.951172\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"264.734375\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"325.916016\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"389.294922\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"421.082031\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"448.865234\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"512.244141\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"544.03125\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"596.130859\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-71\" x=\"657.654297\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-75\" x=\"721.130859\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"784.509766\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"846.033203\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"909.412109\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"964.392578\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_6\">\n",
       "    <g id=\"ytick_7\">\n",
       "     <g id=\"line2d_57\">\n",
       "      <path d=\"M 45.120313 237.478125 \n",
       "L 349.483949 237.478125 \n",
       "\" clip-path=\"url(#p16c174443a)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_62\">\n",
       "      <!-- 1 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(20.878125 241.277344) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"83.789062\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_8\">\n",
       "     <g id=\"line2d_58\">\n",
       "      <path d=\"M 45.120313 204.478125 \n",
       "L 349.483949 204.478125 \n",
       "\" clip-path=\"url(#p16c174443a)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_63\">\n",
       "      <!-- 0 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(29.257813 208.277344) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_9\">\n",
       "     <g id=\"line2d_59\">\n",
       "      <path d=\"M 45.120313 171.478125 \n",
       "L 349.483949 171.478125 \n",
       "\" clip-path=\"url(#p16c174443a)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_64\">\n",
       "      <!-- 1 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(29.257813 175.277344) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_65\">\n",
       "     <!-- Positional encoding -->\n",
       "     <g style=\"fill: #262626\" transform=\"translate(14.798438 252.944531) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"56.677734\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"117.859375\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"169.958984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-74\" x=\"197.742188\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"236.951172\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"264.734375\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"325.916016\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"389.294922\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"450.574219\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"478.357422\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"510.144531\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"571.667969\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"635.046875\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"690.027344\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-64\" x=\"751.208984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"814.685547\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"842.46875\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-67\" x=\"905.847656\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_60\">\n",
       "    <path d=\"M 58.955023 204.478125 \n",
       "L 77.401304 183.694819 \n",
       "L 95.847585 172.190858 \n",
       "L 114.293866 175.102541 \n",
       "L 132.740147 191.129856 \n",
       "L 151.186428 213.116917 \n",
       "L 169.632709 231.246923 \n",
       "L 188.07899 237.425177 \n",
       "L 206.525271 228.893206 \n",
       "L 224.971552 209.460371 \n",
       "L 243.417833 187.80306 \n",
       "L 261.864114 173.590843 \n",
       "L 280.310395 173.169205 \n",
       "L 298.756676 186.726395 \n",
       "L 317.202957 208.209392 \n",
       "L 335.649238 228.02645 \n",
       "\" clip-path=\"url(#p16c174443a)\" style=\"fill: none; stroke: #2ca02c; stroke-width: 1.5; stroke-linecap: round\"/>\n",
       "    <defs>\n",
       "     <path id=\"mdec2922c16\" d=\"M 0 3 \n",
       "C 0.795609 3 1.55874 2.683901 2.12132 2.12132 \n",
       "C 2.683901 1.55874 3 0.795609 3 0 \n",
       "C 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \n",
       "C 1.55874 -2.683901 0.795609 -3 0 -3 \n",
       "C -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \n",
       "C -2.683901 -1.55874 -3 -0.795609 -3 0 \n",
       "C -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \n",
       "C -1.55874 2.683901 -0.795609 3 0 3 \n",
       "z\n",
       "\" style=\"stroke: #000000\"/>\n",
       "    </defs>\n",
       "    <g clip-path=\"url(#p16c174443a)\">\n",
       "     <use xlink:href=\"#mdec2922c16\" x=\"58.955023\" y=\"204.478125\" style=\"fill: #2ca02c; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#mdec2922c16\" x=\"77.401304\" y=\"183.694819\" style=\"fill: #2ca02c; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#mdec2922c16\" x=\"95.847585\" y=\"172.190858\" style=\"fill: #2ca02c; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#mdec2922c16\" x=\"114.293866\" y=\"175.102541\" style=\"fill: #2ca02c; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#mdec2922c16\" x=\"132.740147\" y=\"191.129856\" style=\"fill: #2ca02c; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#mdec2922c16\" x=\"151.186428\" y=\"213.116917\" style=\"fill: #2ca02c; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#mdec2922c16\" x=\"169.632709\" y=\"231.246923\" style=\"fill: #2ca02c; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#mdec2922c16\" x=\"188.07899\" y=\"237.425177\" style=\"fill: #2ca02c; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#mdec2922c16\" x=\"206.525271\" y=\"228.893206\" style=\"fill: #2ca02c; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#mdec2922c16\" x=\"224.971552\" y=\"209.460371\" style=\"fill: #2ca02c; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#mdec2922c16\" x=\"243.417833\" y=\"187.80306\" style=\"fill: #2ca02c; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#mdec2922c16\" x=\"261.864114\" y=\"173.590843\" style=\"fill: #2ca02c; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#mdec2922c16\" x=\"280.310395\" y=\"173.169205\" style=\"fill: #2ca02c; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#mdec2922c16\" x=\"298.756676\" y=\"186.726395\" style=\"fill: #2ca02c; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#mdec2922c16\" x=\"317.202957\" y=\"208.209392\" style=\"fill: #2ca02c; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#mdec2922c16\" x=\"335.649238\" y=\"228.02645\" style=\"fill: #2ca02c; stroke: #000000\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"patch_13\">\n",
       "    <path d=\"M 45.120313 244.078125 \n",
       "L 45.120313 164.878125 \n",
       "\" style=\"fill: none; stroke: #ffffff; stroke-width: 1.25; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_14\">\n",
       "    <path d=\"M 349.483949 244.078125 \n",
       "L 349.483949 164.878125 \n",
       "\" style=\"fill: none; stroke: #ffffff; stroke-width: 1.25; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_15\">\n",
       "    <path d=\"M 45.120313 244.078125 \n",
       "L 349.483949 244.078125 \n",
       "\" style=\"fill: none; stroke: #ffffff; stroke-width: 1.25; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_16\">\n",
       "    <path d=\"M 45.120313 164.878125 \n",
       "L 349.483949 164.878125 \n",
       "\" style=\"fill: none; stroke: #ffffff; stroke-width: 1.25; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_66\">\n",
       "    <!-- Encoding in hidden dimension 3 -->\n",
       "    <g style=\"fill: #262626\" transform=\"translate(101.073381 158.878125) scale(0.12 -0.12)\">\n",
       "     <use xlink:href=\"#DejaVuSans-45\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"63.183594\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-63\" x=\"126.5625\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"181.542969\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-64\" x=\"242.724609\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"306.201172\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"333.984375\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-67\" x=\"397.363281\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"460.839844\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"492.626953\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"520.410156\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"583.789062\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-68\" x=\"615.576172\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"678.955078\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-64\" x=\"706.738281\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-64\" x=\"770.214844\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" x=\"833.691406\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"895.214844\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"958.59375\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-64\" x=\"990.380859\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"1053.857422\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6d\" x=\"1081.640625\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" x=\"1179.052734\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"1240.576172\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-73\" x=\"1303.955078\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"1356.054688\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"1383.837891\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"1445.019531\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"1508.398438\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-33\" x=\"1540.185547\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_4\">\n",
       "   <g id=\"patch_17\">\n",
       "    <path d=\"M 410.356676 244.078125 \n",
       "L 714.720312 244.078125 \n",
       "L 714.720312 164.878125 \n",
       "L 410.356676 164.878125 \n",
       "z\n",
       "\" style=\"fill: #eaeaf2\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_7\">\n",
       "    <g id=\"xtick_49\">\n",
       "     <g id=\"line2d_61\">\n",
       "      <path d=\"M 424.191387 244.078125 \n",
       "L 424.191387 164.878125 \n",
       "\" clip-path=\"url(#pae9c3a07ed)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_67\">\n",
       "      <!-- 1 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(421.010137 261.176563) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_50\">\n",
       "     <g id=\"line2d_62\">\n",
       "      <path d=\"M 442.637668 244.078125 \n",
       "L 442.637668 164.878125 \n",
       "\" clip-path=\"url(#pae9c3a07ed)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_68\">\n",
       "      <!-- 2 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(439.456418 261.176563) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_51\">\n",
       "     <g id=\"line2d_63\">\n",
       "      <path d=\"M 461.083949 244.078125 \n",
       "L 461.083949 164.878125 \n",
       "\" clip-path=\"url(#pae9c3a07ed)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_69\">\n",
       "      <!-- 3 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(457.902699 261.176563) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-33\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_52\">\n",
       "     <g id=\"line2d_64\">\n",
       "      <path d=\"M 479.53023 244.078125 \n",
       "L 479.53023 164.878125 \n",
       "\" clip-path=\"url(#pae9c3a07ed)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_70\">\n",
       "      <!-- 4 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(476.34898 261.176563) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_53\">\n",
       "     <g id=\"line2d_65\">\n",
       "      <path d=\"M 497.976511 244.078125 \n",
       "L 497.976511 164.878125 \n",
       "\" clip-path=\"url(#pae9c3a07ed)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_71\">\n",
       "      <!-- 5 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(494.795261 261.176563) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-35\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_54\">\n",
       "     <g id=\"line2d_66\">\n",
       "      <path d=\"M 516.422792 244.078125 \n",
       "L 516.422792 164.878125 \n",
       "\" clip-path=\"url(#pae9c3a07ed)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_72\">\n",
       "      <!-- 6 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(513.241542 261.176563) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-36\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_55\">\n",
       "     <g id=\"line2d_67\">\n",
       "      <path d=\"M 534.869073 244.078125 \n",
       "L 534.869073 164.878125 \n",
       "\" clip-path=\"url(#pae9c3a07ed)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_73\">\n",
       "      <!-- 7 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(531.687823 261.176563) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-37\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_56\">\n",
       "     <g id=\"line2d_68\">\n",
       "      <path d=\"M 553.315354 244.078125 \n",
       "L 553.315354 164.878125 \n",
       "\" clip-path=\"url(#pae9c3a07ed)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_74\">\n",
       "      <!-- 8 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(550.134104 261.176563) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-38\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_57\">\n",
       "     <g id=\"line2d_69\">\n",
       "      <path d=\"M 571.761635 244.078125 \n",
       "L 571.761635 164.878125 \n",
       "\" clip-path=\"url(#pae9c3a07ed)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_75\">\n",
       "      <!-- 9 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(568.580385 261.176563) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-39\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_58\">\n",
       "     <g id=\"line2d_70\">\n",
       "      <path d=\"M 590.207916 244.078125 \n",
       "L 590.207916 164.878125 \n",
       "\" clip-path=\"url(#pae9c3a07ed)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_76\">\n",
       "      <!-- 10 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(583.845416 261.176563) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_59\">\n",
       "     <g id=\"line2d_71\">\n",
       "      <path d=\"M 608.654197 244.078125 \n",
       "L 608.654197 164.878125 \n",
       "\" clip-path=\"url(#pae9c3a07ed)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_77\">\n",
       "      <!-- 11 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(602.291697 261.176563) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_60\">\n",
       "     <g id=\"line2d_72\">\n",
       "      <path d=\"M 627.100478 244.078125 \n",
       "L 627.100478 164.878125 \n",
       "\" clip-path=\"url(#pae9c3a07ed)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_78\">\n",
       "      <!-- 12 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(620.737978 261.176563) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_61\">\n",
       "     <g id=\"line2d_73\">\n",
       "      <path d=\"M 645.546759 244.078125 \n",
       "L 645.546759 164.878125 \n",
       "\" clip-path=\"url(#pae9c3a07ed)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_79\">\n",
       "      <!-- 13 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(639.184259 261.176563) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-33\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_62\">\n",
       "     <g id=\"line2d_74\">\n",
       "      <path d=\"M 663.99304 244.078125 \n",
       "L 663.99304 164.878125 \n",
       "\" clip-path=\"url(#pae9c3a07ed)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_80\">\n",
       "      <!-- 14 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(657.63054 261.176563) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-34\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_63\">\n",
       "     <g id=\"line2d_75\">\n",
       "      <path d=\"M 682.439321 244.078125 \n",
       "L 682.439321 164.878125 \n",
       "\" clip-path=\"url(#pae9c3a07ed)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_81\">\n",
       "      <!-- 15 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(676.076821 261.176563) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_64\">\n",
       "     <g id=\"line2d_76\">\n",
       "      <path d=\"M 700.885602 244.078125 \n",
       "L 700.885602 164.878125 \n",
       "\" clip-path=\"url(#pae9c3a07ed)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_82\">\n",
       "      <!-- 16 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(694.523102 261.176563) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-36\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_83\">\n",
       "     <!-- Position in sequence -->\n",
       "     <g style=\"fill: #262626\" transform=\"translate(511.243182 274.854688) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"56.677734\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"117.859375\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"169.958984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-74\" x=\"197.742188\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"236.951172\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"264.734375\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"325.916016\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"389.294922\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"421.082031\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"448.865234\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"512.244141\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"544.03125\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"596.130859\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-71\" x=\"657.654297\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-75\" x=\"721.130859\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"784.509766\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"846.033203\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"909.412109\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"964.392578\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_8\">\n",
       "    <g id=\"ytick_10\">\n",
       "     <g id=\"line2d_77\">\n",
       "      <path d=\"M 410.356676 237.478125 \n",
       "L 714.720312 237.478125 \n",
       "\" clip-path=\"url(#pae9c3a07ed)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_84\">\n",
       "      <!-- 1 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(386.114489 241.277344) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"83.789062\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_11\">\n",
       "     <g id=\"line2d_78\">\n",
       "      <path d=\"M 410.356676 204.478125 \n",
       "L 714.720312 204.478125 \n",
       "\" clip-path=\"url(#pae9c3a07ed)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_85\">\n",
       "      <!-- 0 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(394.494176 208.277344) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_12\">\n",
       "     <g id=\"line2d_79\">\n",
       "      <path d=\"M 410.356676 171.478125 \n",
       "L 714.720312 171.478125 \n",
       "\" clip-path=\"url(#pae9c3a07ed)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_86\">\n",
       "      <!-- 1 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(394.494176 175.277344) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_87\">\n",
       "     <!-- Positional encoding -->\n",
       "     <g style=\"fill: #262626\" transform=\"translate(380.034801 252.944531) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"56.677734\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"117.859375\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"169.958984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-74\" x=\"197.742188\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"236.951172\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"264.734375\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"325.916016\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"389.294922\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"450.574219\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"478.357422\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"510.144531\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"571.667969\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"635.046875\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"690.027344\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-64\" x=\"751.208984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"814.685547\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"842.46875\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-67\" x=\"905.847656\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_80\">\n",
       "    <path d=\"M 424.191387 171.478125 \n",
       "L 442.637668 178.845057 \n",
       "L 461.083949 197.656661 \n",
       "L 479.53023 219.513917 \n",
       "L 497.976511 234.657982 \n",
       "L 516.422792 236.327322 \n",
       "L 534.869073 223.776608 \n",
       "L 553.315354 202.609493 \n",
       "L 571.761635 182.276687 \n",
       "L 590.207916 171.856396 \n",
       "L 608.654197 176.001079 \n",
       "L 627.100478 192.860214 \n",
       "L 645.546759 214.906523 \n",
       "L 663.99304 232.296753 \n",
       "L 682.439321 237.266502 \n",
       "L 700.885602 227.596872 \n",
       "\" clip-path=\"url(#pae9c3a07ed)\" style=\"fill: none; stroke: #d62728; stroke-width: 1.5; stroke-linecap: round\"/>\n",
       "    <defs>\n",
       "     <path id=\"m65d798b0c6\" d=\"M 0 3 \n",
       "C 0.795609 3 1.55874 2.683901 2.12132 2.12132 \n",
       "C 2.683901 1.55874 3 0.795609 3 0 \n",
       "C 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \n",
       "C 1.55874 -2.683901 0.795609 -3 0 -3 \n",
       "C -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \n",
       "C -2.683901 -1.55874 -3 -0.795609 -3 0 \n",
       "C -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \n",
       "C -1.55874 2.683901 -0.795609 3 0 3 \n",
       "z\n",
       "\" style=\"stroke: #000000\"/>\n",
       "    </defs>\n",
       "    <g clip-path=\"url(#pae9c3a07ed)\">\n",
       "     <use xlink:href=\"#m65d798b0c6\" x=\"424.191387\" y=\"171.478125\" style=\"fill: #d62728; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m65d798b0c6\" x=\"442.637668\" y=\"178.845057\" style=\"fill: #d62728; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m65d798b0c6\" x=\"461.083949\" y=\"197.656661\" style=\"fill: #d62728; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m65d798b0c6\" x=\"479.53023\" y=\"219.513917\" style=\"fill: #d62728; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m65d798b0c6\" x=\"497.976511\" y=\"234.657982\" style=\"fill: #d62728; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m65d798b0c6\" x=\"516.422792\" y=\"236.327322\" style=\"fill: #d62728; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m65d798b0c6\" x=\"534.869073\" y=\"223.776608\" style=\"fill: #d62728; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m65d798b0c6\" x=\"553.315354\" y=\"202.609493\" style=\"fill: #d62728; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m65d798b0c6\" x=\"571.761635\" y=\"182.276687\" style=\"fill: #d62728; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m65d798b0c6\" x=\"590.207916\" y=\"171.856396\" style=\"fill: #d62728; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m65d798b0c6\" x=\"608.654197\" y=\"176.001079\" style=\"fill: #d62728; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m65d798b0c6\" x=\"627.100478\" y=\"192.860214\" style=\"fill: #d62728; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m65d798b0c6\" x=\"645.546759\" y=\"214.906523\" style=\"fill: #d62728; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m65d798b0c6\" x=\"663.99304\" y=\"232.296753\" style=\"fill: #d62728; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m65d798b0c6\" x=\"682.439321\" y=\"237.266502\" style=\"fill: #d62728; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m65d798b0c6\" x=\"700.885602\" y=\"227.596872\" style=\"fill: #d62728; stroke: #000000\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"patch_18\">\n",
       "    <path d=\"M 410.356676 244.078125 \n",
       "L 410.356676 164.878125 \n",
       "\" style=\"fill: none; stroke: #ffffff; stroke-width: 1.25; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_19\">\n",
       "    <path d=\"M 714.720312 244.078125 \n",
       "L 714.720312 164.878125 \n",
       "\" style=\"fill: none; stroke: #ffffff; stroke-width: 1.25; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_20\">\n",
       "    <path d=\"M 410.356676 244.078125 \n",
       "L 714.720312 244.078125 \n",
       "\" style=\"fill: none; stroke: #ffffff; stroke-width: 1.25; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_21\">\n",
       "    <path d=\"M 410.356676 164.878125 \n",
       "L 714.720312 164.878125 \n",
       "\" style=\"fill: none; stroke: #ffffff; stroke-width: 1.25; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_88\">\n",
       "    <!-- Encoding in hidden dimension 4 -->\n",
       "    <g style=\"fill: #262626\" transform=\"translate(466.309744 158.878125) scale(0.12 -0.12)\">\n",
       "     <use xlink:href=\"#DejaVuSans-45\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"63.183594\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-63\" x=\"126.5625\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"181.542969\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-64\" x=\"242.724609\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"306.201172\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"333.984375\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-67\" x=\"397.363281\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"460.839844\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"492.626953\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"520.410156\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"583.789062\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-68\" x=\"615.576172\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"678.955078\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-64\" x=\"706.738281\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-64\" x=\"770.214844\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" x=\"833.691406\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"895.214844\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"958.59375\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-64\" x=\"990.380859\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"1053.857422\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6d\" x=\"1081.640625\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" x=\"1179.052734\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"1240.576172\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-73\" x=\"1303.955078\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"1356.054688\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"1383.837891\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"1445.019531\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"1508.398438\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-34\" x=\"1540.185547\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"pb2b368aede\">\n",
       "   <rect x=\"45.120313\" y=\"22.318125\" width=\"304.363636\" height=\"79.2\"/>\n",
       "  </clipPath>\n",
       "  <clipPath id=\"pa32695a371\">\n",
       "   <rect x=\"410.356676\" y=\"22.318125\" width=\"304.363636\" height=\"79.2\"/>\n",
       "  </clipPath>\n",
       "  <clipPath id=\"p16c174443a\">\n",
       "   <rect x=\"45.120313\" y=\"164.878125\" width=\"304.363636\" height=\"79.2\"/>\n",
       "  </clipPath>\n",
       "  <clipPath id=\"pae9c3a07ed\">\n",
       "   <rect x=\"410.356676\" y=\"164.878125\" width=\"304.363636\" height=\"79.2\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 1200x400 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_theme()\n",
    "fig, ax = plt.subplots(2, 2, figsize=(12,4))\n",
    "ax = [a for a_list in ax for a in a_list]\n",
    "for i in range(len(ax)):\n",
    "    ax[i].plot(np.arange(1,17), pe[i,:16], color=f'C{i}', marker=\"o\", markersize=6, markeredgecolor=\"black\")\n",
    "    ax[i].set_title(f\"Encoding in hidden dimension {i+1}\")\n",
    "    ax[i].set_xlabel(\"Position in sequence\", fontsize=10)\n",
    "    ax[i].set_ylabel(\"Positional encoding\", fontsize=10)\n",
    "    ax[i].set_xticks(np.arange(1,17))\n",
    "    ax[i].tick_params(axis='both', which='major', labelsize=10)\n",
    "    ax[i].tick_params(axis='both', which='minor', labelsize=8)\n",
    "    ax[i].set_ylim(-1.2, 1.2)\n",
    "fig.subplots_adjust(hspace=0.8)\n",
    "sns.reset_orig()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the patterns between the hidden dimension $1$ and $2$ only differ in the starting angle. The wavelength is $2\\pi$, hence the repetition after position $6$. The hidden dimensions $2$ and $3$ have about twice the wavelength. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate warm-up\n",
    "\n",
    "One commonly used technique for training a Transformer is learning rate warm-up. This means that we gradually increase the learning rate from 0 on to our originally specified learning rate in the first few iterations. Thus, we slowly start learning instead of taking very large steps from the beginning. In fact, training a deep Transformer without learning rate warm-up can make the model diverge and achieve a much worse performance on training and testing. Take for instance the following plot by [Liu et al. (2019)](https://arxiv.org/pdf/1908.03265.pdf) comparing Adam-vanilla (i.e. Adam without warm-up) vs Adam with a warm-up:\n",
    "\n",
    "<center width=\"100%\"><img src=\"../../tutorial6/warmup_loss_plot.svg\" width=\"350px\"></center>\n",
    "\n",
    "Clearly, the warm-up is a crucial hyperparameter in the Transformer architecture. Why is it so important? There are currently two common explanations. Firstly, Adam uses the bias correction factors which however can lead to a higher variance in the adaptive learning rate during the first iterations. Improved optimizers like [RAdam](https://arxiv.org/abs/1908.03265) have been shown to overcome this issue, not requiring warm-up for training Transformers. Secondly, the iteratively applied Layer Normalization across layers can lead to very high gradients during the first iterations, which can be solved by using [Pre-Layer Normalization](https://proceedings.icml.cc/static/paper_files/icml/2020/328-Paper.pdf) (similar to Pre-Activation ResNet), or replacing Layer Normalization by other techniques ([Adaptive Normalization](https://proceedings.icml.cc/static/paper_files/icml/2020/328-Paper.pdf), [Power Normalization](https://arxiv.org/abs/2003.07845)). \n",
    "\n",
    "Nevertheless, many applications and papers still use the original Transformer architecture with Adam, because warm-up is a simple, yet effective way of solving the gradient problem in the first iterations. There are many different schedulers we could use. For instance, the original Transformer paper used an exponential decay scheduler with a warm-up. However, the currently most popular scheduler is the cosine warm-up scheduler, which combines warm-up with a cosine-shaped learning rate decay. In Optax, this learning rate scheduler is also implemented in `optax.warmup_cosine_decay_schedule`, but let's manually implement it below, and visualize the learning rate factor over epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_warmup_schedule(base_lr: float, warmup: int, max_iters: int):\n",
    "    assert warmup > 0 and max_iters > 0\n",
    "    # Create function to return lr based on iteration count\n",
    "    def get_lr(train_iter):\n",
    "        lr_factor = 0.5 * (1 + np.cos(np.pi * train_iter / max_iters))\n",
    "        if train_iter <= warmup:\n",
    "            lr_factor *= train_iter * 1.0 / warmup\n",
    "        return lr_factor * base_lr\n",
    "    return get_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/pdf": "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\n",
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"500.85125pt\" height=\"230.276719pt\" viewBox=\"0 0 500.85125 230.276719\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2022-10-17T15:54:14.911762</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.6.1, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 230.276719 \n",
       "L 500.85125 230.276719 \n",
       "L 500.85125 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 47.25125 188.255625 \n",
       "L 493.65125 188.255625 \n",
       "L 493.65125 21.935625 \n",
       "L 47.25125 21.935625 \n",
       "z\n",
       "\" style=\"fill: #eaeaf2\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <path d=\"M 67.542159 188.255625 \n",
       "L 67.542159 21.935625 \n",
       "\" clip-path=\"url(#p4ec29e35dd)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(64.483643 205.629219) scale(0.11 -0.11)\">\n",
       "       <defs>\n",
       "        <path id=\"ArialMT-30\" d=\"M 266 2259 \n",
       "Q 266 3072 433 3567 \n",
       "Q 600 4063 929 4331 \n",
       "Q 1259 4600 1759 4600 \n",
       "Q 2128 4600 2406 4451 \n",
       "Q 2684 4303 2865 4023 \n",
       "Q 3047 3744 3150 3342 \n",
       "Q 3253 2941 3253 2259 \n",
       "Q 3253 1453 3087 958 \n",
       "Q 2922 463 2592 192 \n",
       "Q 2263 -78 1759 -78 \n",
       "Q 1097 -78 719 397 \n",
       "Q 266 969 266 2259 \n",
       "z\n",
       "M 844 2259 \n",
       "Q 844 1131 1108 757 \n",
       "Q 1372 384 1759 384 \n",
       "Q 2147 384 2411 759 \n",
       "Q 2675 1134 2675 2259 \n",
       "Q 2675 3391 2411 3762 \n",
       "Q 2147 4134 1753 4134 \n",
       "Q 1366 4134 1134 3806 \n",
       "Q 844 3388 844 2259 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#ArialMT-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <path d=\"M 118.294808 188.255625 \n",
       "L 118.294808 21.935625 \n",
       "\" clip-path=\"url(#p4ec29e35dd)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 250 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(109.119261 205.629219) scale(0.11 -0.11)\">\n",
       "       <defs>\n",
       "        <path id=\"ArialMT-32\" d=\"M 3222 541 \n",
       "L 3222 0 \n",
       "L 194 0 \n",
       "Q 188 203 259 391 \n",
       "Q 375 700 629 1000 \n",
       "Q 884 1300 1366 1694 \n",
       "Q 2113 2306 2375 2664 \n",
       "Q 2638 3022 2638 3341 \n",
       "Q 2638 3675 2398 3904 \n",
       "Q 2159 4134 1775 4134 \n",
       "Q 1369 4134 1125 3890 \n",
       "Q 881 3647 878 3216 \n",
       "L 300 3275 \n",
       "Q 359 3922 746 4261 \n",
       "Q 1134 4600 1788 4600 \n",
       "Q 2447 4600 2831 4234 \n",
       "Q 3216 3869 3216 3328 \n",
       "Q 3216 3053 3103 2787 \n",
       "Q 2991 2522 2730 2228 \n",
       "Q 2469 1934 1863 1422 \n",
       "Q 1356 997 1212 845 \n",
       "Q 1069 694 975 541 \n",
       "L 3222 541 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"ArialMT-35\" d=\"M 266 1200 \n",
       "L 856 1250 \n",
       "Q 922 819 1161 601 \n",
       "Q 1400 384 1738 384 \n",
       "Q 2144 384 2425 690 \n",
       "Q 2706 997 2706 1503 \n",
       "Q 2706 1984 2436 2262 \n",
       "Q 2166 2541 1728 2541 \n",
       "Q 1456 2541 1237 2417 \n",
       "Q 1019 2294 894 2097 \n",
       "L 366 2166 \n",
       "L 809 4519 \n",
       "L 3088 4519 \n",
       "L 3088 3981 \n",
       "L 1259 3981 \n",
       "L 1013 2750 \n",
       "Q 1425 3038 1878 3038 \n",
       "Q 2478 3038 2890 2622 \n",
       "Q 3303 2206 3303 1553 \n",
       "Q 3303 931 2941 478 \n",
       "Q 2500 -78 1738 -78 \n",
       "Q 1113 -78 717 272 \n",
       "Q 322 622 266 1200 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#ArialMT-32\"/>\n",
       "       <use xlink:href=\"#ArialMT-35\" x=\"55.615234\"/>\n",
       "       <use xlink:href=\"#ArialMT-30\" x=\"111.230469\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <path d=\"M 169.047457 188.255625 \n",
       "L 169.047457 21.935625 \n",
       "\" clip-path=\"url(#p4ec29e35dd)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 500 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(159.87191 205.629219) scale(0.11 -0.11)\">\n",
       "       <use xlink:href=\"#ArialMT-35\"/>\n",
       "       <use xlink:href=\"#ArialMT-30\" x=\"55.615234\"/>\n",
       "       <use xlink:href=\"#ArialMT-30\" x=\"111.230469\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <path d=\"M 219.800106 188.255625 \n",
       "L 219.800106 21.935625 \n",
       "\" clip-path=\"url(#p4ec29e35dd)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 750 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(210.624559 205.629219) scale(0.11 -0.11)\">\n",
       "       <defs>\n",
       "        <path id=\"ArialMT-37\" d=\"M 303 3981 \n",
       "L 303 4522 \n",
       "L 3269 4522 \n",
       "L 3269 4084 \n",
       "Q 2831 3619 2401 2847 \n",
       "Q 1972 2075 1738 1259 \n",
       "Q 1569 684 1522 0 \n",
       "L 944 0 \n",
       "Q 953 541 1156 1306 \n",
       "Q 1359 2072 1739 2783 \n",
       "Q 2119 3494 2547 3981 \n",
       "L 303 3981 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#ArialMT-37\"/>\n",
       "       <use xlink:href=\"#ArialMT-35\" x=\"55.615234\"/>\n",
       "       <use xlink:href=\"#ArialMT-30\" x=\"111.230469\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <path d=\"M 270.552755 188.255625 \n",
       "L 270.552755 21.935625 \n",
       "\" clip-path=\"url(#p4ec29e35dd)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 1000 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(258.318693 205.629219) scale(0.11 -0.11)\">\n",
       "       <defs>\n",
       "        <path id=\"ArialMT-31\" d=\"M 2384 0 \n",
       "L 1822 0 \n",
       "L 1822 3584 \n",
       "Q 1619 3391 1289 3197 \n",
       "Q 959 3003 697 2906 \n",
       "L 697 3450 \n",
       "Q 1169 3672 1522 3987 \n",
       "Q 1875 4303 2022 4600 \n",
       "L 2384 4600 \n",
       "L 2384 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#ArialMT-31\"/>\n",
       "       <use xlink:href=\"#ArialMT-30\" x=\"55.615234\"/>\n",
       "       <use xlink:href=\"#ArialMT-30\" x=\"111.230469\"/>\n",
       "       <use xlink:href=\"#ArialMT-30\" x=\"166.845703\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <path d=\"M 321.305404 188.255625 \n",
       "L 321.305404 21.935625 \n",
       "\" clip-path=\"url(#p4ec29e35dd)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 1250 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(309.071342 205.629219) scale(0.11 -0.11)\">\n",
       "       <use xlink:href=\"#ArialMT-31\"/>\n",
       "       <use xlink:href=\"#ArialMT-32\" x=\"55.615234\"/>\n",
       "       <use xlink:href=\"#ArialMT-35\" x=\"111.230469\"/>\n",
       "       <use xlink:href=\"#ArialMT-30\" x=\"166.845703\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_7\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <path d=\"M 372.058053 188.255625 \n",
       "L 372.058053 21.935625 \n",
       "\" clip-path=\"url(#p4ec29e35dd)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 1500 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(359.823991 205.629219) scale(0.11 -0.11)\">\n",
       "       <use xlink:href=\"#ArialMT-31\"/>\n",
       "       <use xlink:href=\"#ArialMT-35\" x=\"55.615234\"/>\n",
       "       <use xlink:href=\"#ArialMT-30\" x=\"111.230469\"/>\n",
       "       <use xlink:href=\"#ArialMT-30\" x=\"166.845703\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_8\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <path d=\"M 422.810702 188.255625 \n",
       "L 422.810702 21.935625 \n",
       "\" clip-path=\"url(#p4ec29e35dd)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 1750 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(410.57664 205.629219) scale(0.11 -0.11)\">\n",
       "       <use xlink:href=\"#ArialMT-31\"/>\n",
       "       <use xlink:href=\"#ArialMT-37\" x=\"55.615234\"/>\n",
       "       <use xlink:href=\"#ArialMT-35\" x=\"111.230469\"/>\n",
       "       <use xlink:href=\"#ArialMT-30\" x=\"166.845703\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_9\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <path d=\"M 473.563352 188.255625 \n",
       "L 473.563352 21.935625 \n",
       "\" clip-path=\"url(#p4ec29e35dd)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 2000 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(461.329289 205.629219) scale(0.11 -0.11)\">\n",
       "       <use xlink:href=\"#ArialMT-32\"/>\n",
       "       <use xlink:href=\"#ArialMT-30\" x=\"55.615234\"/>\n",
       "       <use xlink:href=\"#ArialMT-30\" x=\"111.230469\"/>\n",
       "       <use xlink:href=\"#ArialMT-30\" x=\"166.845703\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_10\">\n",
       "     <!-- Iterations (in batches) -->\n",
       "     <g style=\"fill: #262626\" transform=\"translate(212.76125 220.551094) scale(0.12 -0.12)\">\n",
       "      <defs>\n",
       "       <path id=\"ArialMT-49\" d=\"M 597 0 \n",
       "L 597 4581 \n",
       "L 1203 4581 \n",
       "L 1203 0 \n",
       "L 597 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"ArialMT-74\" d=\"M 1650 503 \n",
       "L 1731 6 \n",
       "Q 1494 -44 1306 -44 \n",
       "Q 1000 -44 831 53 \n",
       "Q 663 150 594 308 \n",
       "Q 525 466 525 972 \n",
       "L 525 2881 \n",
       "L 113 2881 \n",
       "L 113 3319 \n",
       "L 525 3319 \n",
       "L 525 4141 \n",
       "L 1084 4478 \n",
       "L 1084 3319 \n",
       "L 1650 3319 \n",
       "L 1650 2881 \n",
       "L 1084 2881 \n",
       "L 1084 941 \n",
       "Q 1084 700 1114 631 \n",
       "Q 1144 563 1211 522 \n",
       "Q 1278 481 1403 481 \n",
       "Q 1497 481 1650 503 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"ArialMT-65\" d=\"M 2694 1069 \n",
       "L 3275 997 \n",
       "Q 3138 488 2766 206 \n",
       "Q 2394 -75 1816 -75 \n",
       "Q 1088 -75 661 373 \n",
       "Q 234 822 234 1631 \n",
       "Q 234 2469 665 2931 \n",
       "Q 1097 3394 1784 3394 \n",
       "Q 2450 3394 2872 2941 \n",
       "Q 3294 2488 3294 1666 \n",
       "Q 3294 1616 3291 1516 \n",
       "L 816 1516 \n",
       "Q 847 969 1125 678 \n",
       "Q 1403 388 1819 388 \n",
       "Q 2128 388 2347 550 \n",
       "Q 2566 713 2694 1069 \n",
       "z\n",
       "M 847 1978 \n",
       "L 2700 1978 \n",
       "Q 2663 2397 2488 2606 \n",
       "Q 2219 2931 1791 2931 \n",
       "Q 1403 2931 1139 2672 \n",
       "Q 875 2413 847 1978 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"ArialMT-72\" d=\"M 416 0 \n",
       "L 416 3319 \n",
       "L 922 3319 \n",
       "L 922 2816 \n",
       "Q 1116 3169 1280 3281 \n",
       "Q 1444 3394 1641 3394 \n",
       "Q 1925 3394 2219 3213 \n",
       "L 2025 2691 \n",
       "Q 1819 2813 1613 2813 \n",
       "Q 1428 2813 1281 2702 \n",
       "Q 1134 2591 1072 2394 \n",
       "Q 978 2094 978 1738 \n",
       "L 978 0 \n",
       "L 416 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"ArialMT-61\" d=\"M 2588 409 \n",
       "Q 2275 144 1986 34 \n",
       "Q 1697 -75 1366 -75 \n",
       "Q 819 -75 525 192 \n",
       "Q 231 459 231 875 \n",
       "Q 231 1119 342 1320 \n",
       "Q 453 1522 633 1644 \n",
       "Q 813 1766 1038 1828 \n",
       "Q 1203 1872 1538 1913 \n",
       "Q 2219 1994 2541 2106 \n",
       "Q 2544 2222 2544 2253 \n",
       "Q 2544 2597 2384 2738 \n",
       "Q 2169 2928 1744 2928 \n",
       "Q 1347 2928 1158 2789 \n",
       "Q 969 2650 878 2297 \n",
       "L 328 2372 \n",
       "Q 403 2725 575 2942 \n",
       "Q 747 3159 1072 3276 \n",
       "Q 1397 3394 1825 3394 \n",
       "Q 2250 3394 2515 3294 \n",
       "Q 2781 3194 2906 3042 \n",
       "Q 3031 2891 3081 2659 \n",
       "Q 3109 2516 3109 2141 \n",
       "L 3109 1391 \n",
       "Q 3109 606 3145 398 \n",
       "Q 3181 191 3288 0 \n",
       "L 2700 0 \n",
       "Q 2613 175 2588 409 \n",
       "z\n",
       "M 2541 1666 \n",
       "Q 2234 1541 1622 1453 \n",
       "Q 1275 1403 1131 1340 \n",
       "Q 988 1278 909 1158 \n",
       "Q 831 1038 831 891 \n",
       "Q 831 666 1001 516 \n",
       "Q 1172 366 1500 366 \n",
       "Q 1825 366 2078 508 \n",
       "Q 2331 650 2450 897 \n",
       "Q 2541 1088 2541 1459 \n",
       "L 2541 1666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"ArialMT-69\" d=\"M 425 3934 \n",
       "L 425 4581 \n",
       "L 988 4581 \n",
       "L 988 3934 \n",
       "L 425 3934 \n",
       "z\n",
       "M 425 0 \n",
       "L 425 3319 \n",
       "L 988 3319 \n",
       "L 988 0 \n",
       "L 425 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"ArialMT-6f\" d=\"M 213 1659 \n",
       "Q 213 2581 725 3025 \n",
       "Q 1153 3394 1769 3394 \n",
       "Q 2453 3394 2887 2945 \n",
       "Q 3322 2497 3322 1706 \n",
       "Q 3322 1066 3130 698 \n",
       "Q 2938 331 2570 128 \n",
       "Q 2203 -75 1769 -75 \n",
       "Q 1072 -75 642 372 \n",
       "Q 213 819 213 1659 \n",
       "z\n",
       "M 791 1659 \n",
       "Q 791 1022 1069 705 \n",
       "Q 1347 388 1769 388 \n",
       "Q 2188 388 2466 706 \n",
       "Q 2744 1025 2744 1678 \n",
       "Q 2744 2294 2464 2611 \n",
       "Q 2184 2928 1769 2928 \n",
       "Q 1347 2928 1069 2612 \n",
       "Q 791 2297 791 1659 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"ArialMT-6e\" d=\"M 422 0 \n",
       "L 422 3319 \n",
       "L 928 3319 \n",
       "L 928 2847 \n",
       "Q 1294 3394 1984 3394 \n",
       "Q 2284 3394 2536 3286 \n",
       "Q 2788 3178 2913 3003 \n",
       "Q 3038 2828 3088 2588 \n",
       "Q 3119 2431 3119 2041 \n",
       "L 3119 0 \n",
       "L 2556 0 \n",
       "L 2556 2019 \n",
       "Q 2556 2363 2490 2533 \n",
       "Q 2425 2703 2258 2804 \n",
       "Q 2091 2906 1866 2906 \n",
       "Q 1506 2906 1245 2678 \n",
       "Q 984 2450 984 1813 \n",
       "L 984 0 \n",
       "L 422 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"ArialMT-73\" d=\"M 197 991 \n",
       "L 753 1078 \n",
       "Q 800 744 1014 566 \n",
       "Q 1228 388 1613 388 \n",
       "Q 2000 388 2187 545 \n",
       "Q 2375 703 2375 916 \n",
       "Q 2375 1106 2209 1216 \n",
       "Q 2094 1291 1634 1406 \n",
       "Q 1016 1563 777 1677 \n",
       "Q 538 1791 414 1992 \n",
       "Q 291 2194 291 2438 \n",
       "Q 291 2659 392 2848 \n",
       "Q 494 3038 669 3163 \n",
       "Q 800 3259 1026 3326 \n",
       "Q 1253 3394 1513 3394 \n",
       "Q 1903 3394 2198 3281 \n",
       "Q 2494 3169 2634 2976 \n",
       "Q 2775 2784 2828 2463 \n",
       "L 2278 2388 \n",
       "Q 2241 2644 2061 2787 \n",
       "Q 1881 2931 1553 2931 \n",
       "Q 1166 2931 1000 2803 \n",
       "Q 834 2675 834 2503 \n",
       "Q 834 2394 903 2306 \n",
       "Q 972 2216 1119 2156 \n",
       "Q 1203 2125 1616 2013 \n",
       "Q 2213 1853 2448 1751 \n",
       "Q 2684 1650 2818 1456 \n",
       "Q 2953 1263 2953 975 \n",
       "Q 2953 694 2789 445 \n",
       "Q 2625 197 2315 61 \n",
       "Q 2006 -75 1616 -75 \n",
       "Q 969 -75 630 194 \n",
       "Q 291 463 197 991 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"ArialMT-20\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"ArialMT-28\" d=\"M 1497 -1347 \n",
       "Q 1031 -759 709 28 \n",
       "Q 388 816 388 1659 \n",
       "Q 388 2403 628 3084 \n",
       "Q 909 3875 1497 4659 \n",
       "L 1900 4659 \n",
       "Q 1522 4009 1400 3731 \n",
       "Q 1209 3300 1100 2831 \n",
       "Q 966 2247 966 1656 \n",
       "Q 966 153 1900 -1347 \n",
       "L 1497 -1347 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"ArialMT-62\" d=\"M 941 0 \n",
       "L 419 0 \n",
       "L 419 4581 \n",
       "L 981 4581 \n",
       "L 981 2947 \n",
       "Q 1338 3394 1891 3394 \n",
       "Q 2197 3394 2470 3270 \n",
       "Q 2744 3147 2920 2923 \n",
       "Q 3097 2700 3197 2384 \n",
       "Q 3297 2069 3297 1709 \n",
       "Q 3297 856 2875 390 \n",
       "Q 2453 -75 1863 -75 \n",
       "Q 1275 -75 941 416 \n",
       "L 941 0 \n",
       "z\n",
       "M 934 1684 \n",
       "Q 934 1088 1097 822 \n",
       "Q 1363 388 1816 388 \n",
       "Q 2184 388 2453 708 \n",
       "Q 2722 1028 2722 1663 \n",
       "Q 2722 2313 2464 2622 \n",
       "Q 2206 2931 1841 2931 \n",
       "Q 1472 2931 1203 2611 \n",
       "Q 934 2291 934 1684 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"ArialMT-63\" d=\"M 2588 1216 \n",
       "L 3141 1144 \n",
       "Q 3050 572 2676 248 \n",
       "Q 2303 -75 1759 -75 \n",
       "Q 1078 -75 664 370 \n",
       "Q 250 816 250 1647 \n",
       "Q 250 2184 428 2587 \n",
       "Q 606 2991 970 3192 \n",
       "Q 1334 3394 1763 3394 \n",
       "Q 2303 3394 2647 3120 \n",
       "Q 2991 2847 3088 2344 \n",
       "L 2541 2259 \n",
       "Q 2463 2594 2264 2762 \n",
       "Q 2066 2931 1784 2931 \n",
       "Q 1359 2931 1093 2626 \n",
       "Q 828 2322 828 1663 \n",
       "Q 828 994 1084 691 \n",
       "Q 1341 388 1753 388 \n",
       "Q 2084 388 2306 591 \n",
       "Q 2528 794 2588 1216 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"ArialMT-68\" d=\"M 422 0 \n",
       "L 422 4581 \n",
       "L 984 4581 \n",
       "L 984 2938 \n",
       "Q 1378 3394 1978 3394 \n",
       "Q 2347 3394 2619 3248 \n",
       "Q 2891 3103 3008 2847 \n",
       "Q 3125 2591 3125 2103 \n",
       "L 3125 0 \n",
       "L 2563 0 \n",
       "L 2563 2103 \n",
       "Q 2563 2525 2380 2717 \n",
       "Q 2197 2909 1863 2909 \n",
       "Q 1613 2909 1392 2779 \n",
       "Q 1172 2650 1078 2428 \n",
       "Q 984 2206 984 1816 \n",
       "L 984 0 \n",
       "L 422 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"ArialMT-29\" d=\"M 791 -1347 \n",
       "L 388 -1347 \n",
       "Q 1322 153 1322 1656 \n",
       "Q 1322 2244 1188 2822 \n",
       "Q 1081 3291 891 3722 \n",
       "Q 769 4003 388 4659 \n",
       "L 791 4659 \n",
       "Q 1378 3875 1659 3084 \n",
       "Q 1900 2403 1900 1659 \n",
       "Q 1900 816 1576 28 \n",
       "Q 1253 -759 791 -1347 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#ArialMT-49\"/>\n",
       "      <use xlink:href=\"#ArialMT-74\" x=\"27.783203\"/>\n",
       "      <use xlink:href=\"#ArialMT-65\" x=\"55.566406\"/>\n",
       "      <use xlink:href=\"#ArialMT-72\" x=\"111.181641\"/>\n",
       "      <use xlink:href=\"#ArialMT-61\" x=\"144.482422\"/>\n",
       "      <use xlink:href=\"#ArialMT-74\" x=\"200.097656\"/>\n",
       "      <use xlink:href=\"#ArialMT-69\" x=\"227.880859\"/>\n",
       "      <use xlink:href=\"#ArialMT-6f\" x=\"250.097656\"/>\n",
       "      <use xlink:href=\"#ArialMT-6e\" x=\"305.712891\"/>\n",
       "      <use xlink:href=\"#ArialMT-73\" x=\"361.328125\"/>\n",
       "      <use xlink:href=\"#ArialMT-20\" x=\"411.328125\"/>\n",
       "      <use xlink:href=\"#ArialMT-28\" x=\"439.111328\"/>\n",
       "      <use xlink:href=\"#ArialMT-69\" x=\"472.412109\"/>\n",
       "      <use xlink:href=\"#ArialMT-6e\" x=\"494.628906\"/>\n",
       "      <use xlink:href=\"#ArialMT-20\" x=\"550.244141\"/>\n",
       "      <use xlink:href=\"#ArialMT-62\" x=\"578.027344\"/>\n",
       "      <use xlink:href=\"#ArialMT-61\" x=\"633.642578\"/>\n",
       "      <use xlink:href=\"#ArialMT-74\" x=\"689.257812\"/>\n",
       "      <use xlink:href=\"#ArialMT-63\" x=\"717.041016\"/>\n",
       "      <use xlink:href=\"#ArialMT-68\" x=\"767.041016\"/>\n",
       "      <use xlink:href=\"#ArialMT-65\" x=\"822.65625\"/>\n",
       "      <use xlink:href=\"#ArialMT-73\" x=\"878.271484\"/>\n",
       "      <use xlink:href=\"#ArialMT-29\" x=\"928.271484\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <path d=\"M 47.25125 180.695625 \n",
       "L 493.65125 180.695625 \n",
       "\" clip-path=\"url(#p4ec29e35dd)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 0.0 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(22.46125 184.632422) scale(0.11 -0.11)\">\n",
       "       <defs>\n",
       "        <path id=\"ArialMT-2e\" d=\"M 581 0 \n",
       "L 581 641 \n",
       "L 1222 641 \n",
       "L 1222 0 \n",
       "L 581 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#ArialMT-30\"/>\n",
       "       <use xlink:href=\"#ArialMT-2e\" x=\"55.615234\"/>\n",
       "       <use xlink:href=\"#ArialMT-30\" x=\"83.398438\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <path d=\"M 47.25125 150.26832 \n",
       "L 493.65125 150.26832 \n",
       "\" clip-path=\"url(#p4ec29e35dd)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 0.2 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(22.46125 154.205117) scale(0.11 -0.11)\">\n",
       "       <use xlink:href=\"#ArialMT-30\"/>\n",
       "       <use xlink:href=\"#ArialMT-2e\" x=\"55.615234\"/>\n",
       "       <use xlink:href=\"#ArialMT-32\" x=\"83.398438\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_12\">\n",
       "      <path d=\"M 47.25125 119.841014 \n",
       "L 493.65125 119.841014 \n",
       "\" clip-path=\"url(#p4ec29e35dd)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_13\">\n",
       "      <!-- 0.4 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(22.46125 123.777811) scale(0.11 -0.11)\">\n",
       "       <defs>\n",
       "        <path id=\"ArialMT-34\" d=\"M 2069 0 \n",
       "L 2069 1097 \n",
       "L 81 1097 \n",
       "L 81 1613 \n",
       "L 2172 4581 \n",
       "L 2631 4581 \n",
       "L 2631 1613 \n",
       "L 3250 1613 \n",
       "L 3250 1097 \n",
       "L 2631 1097 \n",
       "L 2631 0 \n",
       "L 2069 0 \n",
       "z\n",
       "M 2069 1613 \n",
       "L 2069 3678 \n",
       "L 634 1613 \n",
       "L 2069 1613 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#ArialMT-30\"/>\n",
       "       <use xlink:href=\"#ArialMT-2e\" x=\"55.615234\"/>\n",
       "       <use xlink:href=\"#ArialMT-34\" x=\"83.398438\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_13\">\n",
       "      <path d=\"M 47.25125 89.413709 \n",
       "L 493.65125 89.413709 \n",
       "\" clip-path=\"url(#p4ec29e35dd)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_14\">\n",
       "      <!-- 0.6 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(22.46125 93.350506) scale(0.11 -0.11)\">\n",
       "       <defs>\n",
       "        <path id=\"ArialMT-36\" d=\"M 3184 3459 \n",
       "L 2625 3416 \n",
       "Q 2550 3747 2413 3897 \n",
       "Q 2184 4138 1850 4138 \n",
       "Q 1581 4138 1378 3988 \n",
       "Q 1113 3794 959 3422 \n",
       "Q 806 3050 800 2363 \n",
       "Q 1003 2672 1297 2822 \n",
       "Q 1591 2972 1913 2972 \n",
       "Q 2475 2972 2870 2558 \n",
       "Q 3266 2144 3266 1488 \n",
       "Q 3266 1056 3080 686 \n",
       "Q 2894 316 2569 119 \n",
       "Q 2244 -78 1831 -78 \n",
       "Q 1128 -78 684 439 \n",
       "Q 241 956 241 2144 \n",
       "Q 241 3472 731 4075 \n",
       "Q 1159 4600 1884 4600 \n",
       "Q 2425 4600 2770 4297 \n",
       "Q 3116 3994 3184 3459 \n",
       "z\n",
       "M 888 1484 \n",
       "Q 888 1194 1011 928 \n",
       "Q 1134 663 1356 523 \n",
       "Q 1578 384 1822 384 \n",
       "Q 2178 384 2434 671 \n",
       "Q 2691 959 2691 1453 \n",
       "Q 2691 1928 2437 2201 \n",
       "Q 2184 2475 1800 2475 \n",
       "Q 1419 2475 1153 2201 \n",
       "Q 888 1928 888 1484 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#ArialMT-30\"/>\n",
       "       <use xlink:href=\"#ArialMT-2e\" x=\"55.615234\"/>\n",
       "       <use xlink:href=\"#ArialMT-36\" x=\"83.398438\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_14\">\n",
       "      <path d=\"M 47.25125 58.986404 \n",
       "L 493.65125 58.986404 \n",
       "\" clip-path=\"url(#p4ec29e35dd)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_15\">\n",
       "      <!-- 0.8 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(22.46125 62.923201) scale(0.11 -0.11)\">\n",
       "       <defs>\n",
       "        <path id=\"ArialMT-38\" d=\"M 1131 2484 \n",
       "Q 781 2613 612 2850 \n",
       "Q 444 3088 444 3419 \n",
       "Q 444 3919 803 4259 \n",
       "Q 1163 4600 1759 4600 \n",
       "Q 2359 4600 2725 4251 \n",
       "Q 3091 3903 3091 3403 \n",
       "Q 3091 3084 2923 2848 \n",
       "Q 2756 2613 2416 2484 \n",
       "Q 2838 2347 3058 2040 \n",
       "Q 3278 1734 3278 1309 \n",
       "Q 3278 722 2862 322 \n",
       "Q 2447 -78 1769 -78 \n",
       "Q 1091 -78 675 323 \n",
       "Q 259 725 259 1325 \n",
       "Q 259 1772 486 2073 \n",
       "Q 713 2375 1131 2484 \n",
       "z\n",
       "M 1019 3438 \n",
       "Q 1019 3113 1228 2906 \n",
       "Q 1438 2700 1772 2700 \n",
       "Q 2097 2700 2305 2904 \n",
       "Q 2513 3109 2513 3406 \n",
       "Q 2513 3716 2298 3927 \n",
       "Q 2084 4138 1766 4138 \n",
       "Q 1444 4138 1231 3931 \n",
       "Q 1019 3725 1019 3438 \n",
       "z\n",
       "M 838 1322 \n",
       "Q 838 1081 952 856 \n",
       "Q 1066 631 1291 507 \n",
       "Q 1516 384 1775 384 \n",
       "Q 2178 384 2440 643 \n",
       "Q 2703 903 2703 1303 \n",
       "Q 2703 1709 2433 1975 \n",
       "Q 2163 2241 1756 2241 \n",
       "Q 1359 2241 1098 1978 \n",
       "Q 838 1716 838 1322 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#ArialMT-30\"/>\n",
       "       <use xlink:href=\"#ArialMT-2e\" x=\"55.615234\"/>\n",
       "       <use xlink:href=\"#ArialMT-38\" x=\"83.398438\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_15\">\n",
       "      <path d=\"M 47.25125 28.559098 \n",
       "L 493.65125 28.559098 \n",
       "\" clip-path=\"url(#p4ec29e35dd)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_16\">\n",
       "      <!-- 1.0 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(22.46125 32.495895) scale(0.11 -0.11)\">\n",
       "       <use xlink:href=\"#ArialMT-31\"/>\n",
       "       <use xlink:href=\"#ArialMT-2e\" x=\"55.615234\"/>\n",
       "       <use xlink:href=\"#ArialMT-30\" x=\"83.398438\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_17\">\n",
       "     <!-- Learning rate factor -->\n",
       "     <g style=\"fill: #262626\" transform=\"translate(15.935625 157.122188) rotate(-90) scale(0.12 -0.12)\">\n",
       "      <defs>\n",
       "       <path id=\"ArialMT-4c\" d=\"M 469 0 \n",
       "L 469 4581 \n",
       "L 1075 4581 \n",
       "L 1075 541 \n",
       "L 3331 541 \n",
       "L 3331 0 \n",
       "L 469 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"ArialMT-67\" d=\"M 319 -275 \n",
       "L 866 -356 \n",
       "Q 900 -609 1056 -725 \n",
       "Q 1266 -881 1628 -881 \n",
       "Q 2019 -881 2231 -725 \n",
       "Q 2444 -569 2519 -288 \n",
       "Q 2563 -116 2559 434 \n",
       "Q 2191 0 1641 0 \n",
       "Q 956 0 581 494 \n",
       "Q 206 988 206 1678 \n",
       "Q 206 2153 378 2554 \n",
       "Q 550 2956 876 3175 \n",
       "Q 1203 3394 1644 3394 \n",
       "Q 2231 3394 2613 2919 \n",
       "L 2613 3319 \n",
       "L 3131 3319 \n",
       "L 3131 450 \n",
       "Q 3131 -325 2973 -648 \n",
       "Q 2816 -972 2473 -1159 \n",
       "Q 2131 -1347 1631 -1347 \n",
       "Q 1038 -1347 672 -1080 \n",
       "Q 306 -813 319 -275 \n",
       "z\n",
       "M 784 1719 \n",
       "Q 784 1066 1043 766 \n",
       "Q 1303 466 1694 466 \n",
       "Q 2081 466 2343 764 \n",
       "Q 2606 1063 2606 1700 \n",
       "Q 2606 2309 2336 2618 \n",
       "Q 2066 2928 1684 2928 \n",
       "Q 1309 2928 1046 2623 \n",
       "Q 784 2319 784 1719 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"ArialMT-66\" d=\"M 556 0 \n",
       "L 556 2881 \n",
       "L 59 2881 \n",
       "L 59 3319 \n",
       "L 556 3319 \n",
       "L 556 3672 \n",
       "Q 556 4006 616 4169 \n",
       "Q 697 4388 901 4523 \n",
       "Q 1106 4659 1475 4659 \n",
       "Q 1713 4659 2000 4603 \n",
       "L 1916 4113 \n",
       "Q 1741 4144 1584 4144 \n",
       "Q 1328 4144 1222 4034 \n",
       "Q 1116 3925 1116 3625 \n",
       "L 1116 3319 \n",
       "L 1763 3319 \n",
       "L 1763 2881 \n",
       "L 1116 2881 \n",
       "L 1116 0 \n",
       "L 556 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#ArialMT-4c\"/>\n",
       "      <use xlink:href=\"#ArialMT-65\" x=\"55.615234\"/>\n",
       "      <use xlink:href=\"#ArialMT-61\" x=\"111.230469\"/>\n",
       "      <use xlink:href=\"#ArialMT-72\" x=\"166.845703\"/>\n",
       "      <use xlink:href=\"#ArialMT-6e\" x=\"200.146484\"/>\n",
       "      <use xlink:href=\"#ArialMT-69\" x=\"255.761719\"/>\n",
       "      <use xlink:href=\"#ArialMT-6e\" x=\"277.978516\"/>\n",
       "      <use xlink:href=\"#ArialMT-67\" x=\"333.59375\"/>\n",
       "      <use xlink:href=\"#ArialMT-20\" x=\"389.208984\"/>\n",
       "      <use xlink:href=\"#ArialMT-72\" x=\"416.992188\"/>\n",
       "      <use xlink:href=\"#ArialMT-61\" x=\"450.292969\"/>\n",
       "      <use xlink:href=\"#ArialMT-74\" x=\"505.908203\"/>\n",
       "      <use xlink:href=\"#ArialMT-65\" x=\"533.691406\"/>\n",
       "      <use xlink:href=\"#ArialMT-20\" x=\"589.306641\"/>\n",
       "      <use xlink:href=\"#ArialMT-66\" x=\"617.089844\"/>\n",
       "      <use xlink:href=\"#ArialMT-61\" x=\"644.873047\"/>\n",
       "      <use xlink:href=\"#ArialMT-63\" x=\"700.488281\"/>\n",
       "      <use xlink:href=\"#ArialMT-74\" x=\"750.488281\"/>\n",
       "      <use xlink:href=\"#ArialMT-6f\" x=\"778.271484\"/>\n",
       "      <use xlink:href=\"#ArialMT-72\" x=\"833.886719\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_16\">\n",
       "    <path d=\"M 67.542159 180.695625 \n",
       "L 87.031176 35.473272 \n",
       "L 87.843219 29.495625 \n",
       "L 94.94859 30.263032 \n",
       "L 102.05396 31.255154 \n",
       "L 109.159331 32.468992 \n",
       "L 116.467713 33.944953 \n",
       "L 123.776094 35.646878 \n",
       "L 131.287486 37.625818 \n",
       "L 139.001889 39.893689 \n",
       "L 146.716292 42.392134 \n",
       "L 154.633705 45.18674 \n",
       "L 162.957139 48.364909 \n",
       "L 171.483584 51.862359 \n",
       "L 180.416051 55.772318 \n",
       "L 189.754538 60.109114 \n",
       "L 199.499047 64.881803 \n",
       "L 210.055598 70.306192 \n",
       "L 221.424191 76.402711 \n",
       "L 234.213859 83.519762 \n",
       "L 249.642664 92.373747 \n",
       "L 273.394904 106.300055 \n",
       "L 298.771228 121.104509 \n",
       "L 313.591002 129.493166 \n",
       "L 326.177659 136.36553 \n",
       "L 337.343241 142.212205 \n",
       "L 347.696782 147.384068 \n",
       "L 357.44129 152.001747 \n",
       "L 366.576767 156.086816 \n",
       "L 375.306223 159.750467 \n",
       "L 383.832668 163.086348 \n",
       "L 391.953092 166.02711 \n",
       "L 399.870505 168.661134 \n",
       "L 407.584908 170.996618 \n",
       "L 415.0963 173.043557 \n",
       "L 422.607692 174.859461 \n",
       "L 429.916073 176.398693 \n",
       "L 437.224455 177.708481 \n",
       "L 444.329826 178.757928 \n",
       "L 451.435197 179.583368 \n",
       "L 458.540567 180.182306 \n",
       "L 465.645938 180.552931 \n",
       "L 472.548299 180.693279 \n",
       "L 473.360341 180.695531 \n",
       "L 473.360341 180.695531 \n",
       "\" clip-path=\"url(#p4ec29e35dd)\" style=\"fill: none; stroke: #4c72b0; stroke-width: 1.5; stroke-linecap: round\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 47.25125 188.255625 \n",
       "L 47.25125 21.935625 \n",
       "\" style=\"fill: none; stroke: #ffffff; stroke-width: 1.25; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 493.65125 188.255625 \n",
       "L 493.65125 21.935625 \n",
       "\" style=\"fill: none; stroke: #ffffff; stroke-width: 1.25; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 47.25125 188.255625 \n",
       "L 493.65125 188.255625 \n",
       "\" style=\"fill: none; stroke: #ffffff; stroke-width: 1.25; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 47.25125 21.935625 \n",
       "L 493.65125 21.935625 \n",
       "\" style=\"fill: none; stroke: #ffffff; stroke-width: 1.25; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_18\">\n",
       "    <!-- Cosine Warm-up Learning Rate Scheduler -->\n",
       "    <g style=\"fill: #262626\" transform=\"translate(157.626875 15.935625) scale(0.12 -0.12)\">\n",
       "     <defs>\n",
       "      <path id=\"ArialMT-43\" d=\"M 3763 1606 \n",
       "L 4369 1453 \n",
       "Q 4178 706 3683 314 \n",
       "Q 3188 -78 2472 -78 \n",
       "Q 1731 -78 1267 223 \n",
       "Q 803 525 561 1097 \n",
       "Q 319 1669 319 2325 \n",
       "Q 319 3041 592 3573 \n",
       "Q 866 4106 1370 4382 \n",
       "Q 1875 4659 2481 4659 \n",
       "Q 3169 4659 3637 4309 \n",
       "Q 4106 3959 4291 3325 \n",
       "L 3694 3184 \n",
       "Q 3534 3684 3231 3912 \n",
       "Q 2928 4141 2469 4141 \n",
       "Q 1941 4141 1586 3887 \n",
       "Q 1231 3634 1087 3207 \n",
       "Q 944 2781 944 2328 \n",
       "Q 944 1744 1114 1308 \n",
       "Q 1284 872 1643 656 \n",
       "Q 2003 441 2422 441 \n",
       "Q 2931 441 3284 734 \n",
       "Q 3638 1028 3763 1606 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"ArialMT-57\" d=\"M 1294 0 \n",
       "L 78 4581 \n",
       "L 700 4581 \n",
       "L 1397 1578 \n",
       "Q 1509 1106 1591 641 \n",
       "Q 1766 1375 1797 1488 \n",
       "L 2669 4581 \n",
       "L 3400 4581 \n",
       "L 4056 2263 \n",
       "Q 4303 1400 4413 641 \n",
       "Q 4500 1075 4641 1638 \n",
       "L 5359 4581 \n",
       "L 5969 4581 \n",
       "L 4713 0 \n",
       "L 4128 0 \n",
       "L 3163 3491 \n",
       "Q 3041 3928 3019 4028 \n",
       "Q 2947 3713 2884 3491 \n",
       "L 1913 0 \n",
       "L 1294 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"ArialMT-6d\" d=\"M 422 0 \n",
       "L 422 3319 \n",
       "L 925 3319 \n",
       "L 925 2853 \n",
       "Q 1081 3097 1340 3245 \n",
       "Q 1600 3394 1931 3394 \n",
       "Q 2300 3394 2536 3241 \n",
       "Q 2772 3088 2869 2813 \n",
       "Q 3263 3394 3894 3394 \n",
       "Q 4388 3394 4653 3120 \n",
       "Q 4919 2847 4919 2278 \n",
       "L 4919 0 \n",
       "L 4359 0 \n",
       "L 4359 2091 \n",
       "Q 4359 2428 4304 2576 \n",
       "Q 4250 2725 4106 2815 \n",
       "Q 3963 2906 3769 2906 \n",
       "Q 3419 2906 3187 2673 \n",
       "Q 2956 2441 2956 1928 \n",
       "L 2956 0 \n",
       "L 2394 0 \n",
       "L 2394 2156 \n",
       "Q 2394 2531 2256 2718 \n",
       "Q 2119 2906 1806 2906 \n",
       "Q 1569 2906 1367 2781 \n",
       "Q 1166 2656 1075 2415 \n",
       "Q 984 2175 984 1722 \n",
       "L 984 0 \n",
       "L 422 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"ArialMT-2d\" d=\"M 203 1375 \n",
       "L 203 1941 \n",
       "L 1931 1941 \n",
       "L 1931 1375 \n",
       "L 203 1375 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"ArialMT-75\" d=\"M 2597 0 \n",
       "L 2597 488 \n",
       "Q 2209 -75 1544 -75 \n",
       "Q 1250 -75 995 37 \n",
       "Q 741 150 617 320 \n",
       "Q 494 491 444 738 \n",
       "Q 409 903 409 1263 \n",
       "L 409 3319 \n",
       "L 972 3319 \n",
       "L 972 1478 \n",
       "Q 972 1038 1006 884 \n",
       "Q 1059 663 1231 536 \n",
       "Q 1403 409 1656 409 \n",
       "Q 1909 409 2131 539 \n",
       "Q 2353 669 2445 892 \n",
       "Q 2538 1116 2538 1541 \n",
       "L 2538 3319 \n",
       "L 3100 3319 \n",
       "L 3100 0 \n",
       "L 2597 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"ArialMT-70\" d=\"M 422 -1272 \n",
       "L 422 3319 \n",
       "L 934 3319 \n",
       "L 934 2888 \n",
       "Q 1116 3141 1344 3267 \n",
       "Q 1572 3394 1897 3394 \n",
       "Q 2322 3394 2647 3175 \n",
       "Q 2972 2956 3137 2557 \n",
       "Q 3303 2159 3303 1684 \n",
       "Q 3303 1175 3120 767 \n",
       "Q 2938 359 2589 142 \n",
       "Q 2241 -75 1856 -75 \n",
       "Q 1575 -75 1351 44 \n",
       "Q 1128 163 984 344 \n",
       "L 984 -1272 \n",
       "L 422 -1272 \n",
       "z\n",
       "M 931 1641 \n",
       "Q 931 1000 1190 694 \n",
       "Q 1450 388 1819 388 \n",
       "Q 2194 388 2461 705 \n",
       "Q 2728 1022 2728 1688 \n",
       "Q 2728 2322 2467 2637 \n",
       "Q 2206 2953 1844 2953 \n",
       "Q 1484 2953 1207 2617 \n",
       "Q 931 2281 931 1641 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"ArialMT-52\" d=\"M 503 0 \n",
       "L 503 4581 \n",
       "L 2534 4581 \n",
       "Q 3147 4581 3465 4457 \n",
       "Q 3784 4334 3975 4021 \n",
       "Q 4166 3709 4166 3331 \n",
       "Q 4166 2844 3850 2509 \n",
       "Q 3534 2175 2875 2084 \n",
       "Q 3116 1969 3241 1856 \n",
       "Q 3506 1613 3744 1247 \n",
       "L 4541 0 \n",
       "L 3778 0 \n",
       "L 3172 953 \n",
       "Q 2906 1366 2734 1584 \n",
       "Q 2563 1803 2427 1890 \n",
       "Q 2291 1978 2150 2013 \n",
       "Q 2047 2034 1813 2034 \n",
       "L 1109 2034 \n",
       "L 1109 0 \n",
       "L 503 0 \n",
       "z\n",
       "M 1109 2559 \n",
       "L 2413 2559 \n",
       "Q 2828 2559 3062 2645 \n",
       "Q 3297 2731 3419 2920 \n",
       "Q 3541 3109 3541 3331 \n",
       "Q 3541 3656 3305 3865 \n",
       "Q 3069 4075 2559 4075 \n",
       "L 1109 4075 \n",
       "L 1109 2559 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"ArialMT-53\" d=\"M 288 1472 \n",
       "L 859 1522 \n",
       "Q 900 1178 1048 958 \n",
       "Q 1197 738 1509 602 \n",
       "Q 1822 466 2213 466 \n",
       "Q 2559 466 2825 569 \n",
       "Q 3091 672 3220 851 \n",
       "Q 3350 1031 3350 1244 \n",
       "Q 3350 1459 3225 1620 \n",
       "Q 3100 1781 2813 1891 \n",
       "Q 2628 1963 1997 2114 \n",
       "Q 1366 2266 1113 2400 \n",
       "Q 784 2572 623 2826 \n",
       "Q 463 3081 463 3397 \n",
       "Q 463 3744 659 4045 \n",
       "Q 856 4347 1234 4503 \n",
       "Q 1613 4659 2075 4659 \n",
       "Q 2584 4659 2973 4495 \n",
       "Q 3363 4331 3572 4012 \n",
       "Q 3781 3694 3797 3291 \n",
       "L 3216 3247 \n",
       "Q 3169 3681 2898 3903 \n",
       "Q 2628 4125 2100 4125 \n",
       "Q 1550 4125 1298 3923 \n",
       "Q 1047 3722 1047 3438 \n",
       "Q 1047 3191 1225 3031 \n",
       "Q 1400 2872 2139 2705 \n",
       "Q 2878 2538 3153 2413 \n",
       "Q 3553 2228 3743 1945 \n",
       "Q 3934 1663 3934 1294 \n",
       "Q 3934 928 3725 604 \n",
       "Q 3516 281 3123 101 \n",
       "Q 2731 -78 2241 -78 \n",
       "Q 1619 -78 1198 103 \n",
       "Q 778 284 539 648 \n",
       "Q 300 1013 288 1472 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"ArialMT-64\" d=\"M 2575 0 \n",
       "L 2575 419 \n",
       "Q 2259 -75 1647 -75 \n",
       "Q 1250 -75 917 144 \n",
       "Q 584 363 401 755 \n",
       "Q 219 1147 219 1656 \n",
       "Q 219 2153 384 2558 \n",
       "Q 550 2963 881 3178 \n",
       "Q 1213 3394 1622 3394 \n",
       "Q 1922 3394 2156 3267 \n",
       "Q 2391 3141 2538 2938 \n",
       "L 2538 4581 \n",
       "L 3097 4581 \n",
       "L 3097 0 \n",
       "L 2575 0 \n",
       "z\n",
       "M 797 1656 \n",
       "Q 797 1019 1065 703 \n",
       "Q 1334 388 1700 388 \n",
       "Q 2069 388 2326 689 \n",
       "Q 2584 991 2584 1609 \n",
       "Q 2584 2291 2321 2609 \n",
       "Q 2059 2928 1675 2928 \n",
       "Q 1300 2928 1048 2622 \n",
       "Q 797 2316 797 1656 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"ArialMT-6c\" d=\"M 409 0 \n",
       "L 409 4581 \n",
       "L 972 4581 \n",
       "L 972 0 \n",
       "L 409 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#ArialMT-43\"/>\n",
       "     <use xlink:href=\"#ArialMT-6f\" x=\"72.216797\"/>\n",
       "     <use xlink:href=\"#ArialMT-73\" x=\"127.832031\"/>\n",
       "     <use xlink:href=\"#ArialMT-69\" x=\"177.832031\"/>\n",
       "     <use xlink:href=\"#ArialMT-6e\" x=\"200.048828\"/>\n",
       "     <use xlink:href=\"#ArialMT-65\" x=\"255.664062\"/>\n",
       "     <use xlink:href=\"#ArialMT-20\" x=\"311.279297\"/>\n",
       "     <use xlink:href=\"#ArialMT-57\" x=\"339.0625\"/>\n",
       "     <use xlink:href=\"#ArialMT-61\" x=\"429.697266\"/>\n",
       "     <use xlink:href=\"#ArialMT-72\" x=\"485.3125\"/>\n",
       "     <use xlink:href=\"#ArialMT-6d\" x=\"518.613281\"/>\n",
       "     <use xlink:href=\"#ArialMT-2d\" x=\"601.914062\"/>\n",
       "     <use xlink:href=\"#ArialMT-75\" x=\"635.214844\"/>\n",
       "     <use xlink:href=\"#ArialMT-70\" x=\"690.830078\"/>\n",
       "     <use xlink:href=\"#ArialMT-20\" x=\"746.445312\"/>\n",
       "     <use xlink:href=\"#ArialMT-4c\" x=\"774.228516\"/>\n",
       "     <use xlink:href=\"#ArialMT-65\" x=\"829.84375\"/>\n",
       "     <use xlink:href=\"#ArialMT-61\" x=\"885.458984\"/>\n",
       "     <use xlink:href=\"#ArialMT-72\" x=\"941.074219\"/>\n",
       "     <use xlink:href=\"#ArialMT-6e\" x=\"974.375\"/>\n",
       "     <use xlink:href=\"#ArialMT-69\" x=\"1029.990234\"/>\n",
       "     <use xlink:href=\"#ArialMT-6e\" x=\"1052.207031\"/>\n",
       "     <use xlink:href=\"#ArialMT-67\" x=\"1107.822266\"/>\n",
       "     <use xlink:href=\"#ArialMT-20\" x=\"1163.4375\"/>\n",
       "     <use xlink:href=\"#ArialMT-52\" x=\"1191.220703\"/>\n",
       "     <use xlink:href=\"#ArialMT-61\" x=\"1263.4375\"/>\n",
       "     <use xlink:href=\"#ArialMT-74\" x=\"1319.052734\"/>\n",
       "     <use xlink:href=\"#ArialMT-65\" x=\"1346.835938\"/>\n",
       "     <use xlink:href=\"#ArialMT-20\" x=\"1402.451172\"/>\n",
       "     <use xlink:href=\"#ArialMT-53\" x=\"1430.234375\"/>\n",
       "     <use xlink:href=\"#ArialMT-63\" x=\"1496.933594\"/>\n",
       "     <use xlink:href=\"#ArialMT-68\" x=\"1546.933594\"/>\n",
       "     <use xlink:href=\"#ArialMT-65\" x=\"1602.548828\"/>\n",
       "     <use xlink:href=\"#ArialMT-64\" x=\"1658.164062\"/>\n",
       "     <use xlink:href=\"#ArialMT-75\" x=\"1713.779297\"/>\n",
       "     <use xlink:href=\"#ArialMT-6c\" x=\"1769.394531\"/>\n",
       "     <use xlink:href=\"#ArialMT-65\" x=\"1791.611328\"/>\n",
       "     <use xlink:href=\"#ArialMT-72\" x=\"1847.226562\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p4ec29e35dd\">\n",
       "   <rect x=\"47.25125\" y=\"21.935625\" width=\"446.4\" height=\"166.32\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 800x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr_scheduler = cosine_warmup_schedule(base_lr=1.0, warmup=100, max_iters=2000)\n",
    "\n",
    "# Plotting\n",
    "epochs = list(range(2000))\n",
    "sns.set()\n",
    "plt.figure(figsize=(8,3))\n",
    "plt.plot(epochs, [lr_scheduler(e) for e in epochs])\n",
    "plt.ylabel(\"Learning rate factor\")\n",
    "plt.xlabel(\"Iterations (in batches)\")\n",
    "plt.title(\"Cosine Warm-up Learning Rate Scheduler\")\n",
    "plt.show()\n",
    "sns.reset_orig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first 100 iterations, we increase the learning rate factor from 0 to 1, whereas for all later iterations, we decay it using the cosine wave."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Transformer model\n",
    "\n",
    "Finally, we can embed the Transformer architecture into a full architecture. We will implement a template for a classifier based on the Transformer encoder. Thereby, we have a prediction output per sequence element. If we would need a classifier over the whole sequence, the common approach is to add an additional `[CLS]` token to the sequence, representing the classifier token. However, here we focus on tasks where we have one output per element. \n",
    "\n",
    "Additionally to the Transformer architecture, we add a small input network (maps input dimensions to model dimensions), the positional encoding, and an output network (transforms output encodings to predictions). We also add the learning rate scheduler, which takes a step each iteration instead of once per epoch. This is needed for the warmup and the smooth cosine decay. The training, validation, and test step is left empty for now and will be filled for our task-specific models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerPredictor(nn.Module):\n",
    "    model_dim : int                   # Hidden dimensionality to use inside the Transformer\n",
    "    num_classes : int                 # Number of classes to predict per sequence element\n",
    "    num_heads : int                   # Number of heads to use in the Multi-Head Attention blocks\n",
    "    num_layers : int                  # Number of encoder blocks to use\n",
    "    dropout_prob : float = 0.0        # Dropout to apply inside the model\n",
    "    input_dropout_prob : float = 0.0  # Dropout to apply on the input features\n",
    "\n",
    "    def setup(self):\n",
    "        # Input dim -> Model dim\n",
    "        self.input_dropout = nn.Dropout(self.input_dropout_prob)\n",
    "        self.input_layer = nn.Dense(self.model_dim)\n",
    "        # Positional encoding for sequences\n",
    "        self.positional_encoding = PositionalEncoding(self.model_dim)\n",
    "        # Transformer\n",
    "        self.transformer = TransformerEncoder(num_layers=self.num_layers,\n",
    "                                              input_dim=self.model_dim,\n",
    "                                              dim_feedforward=2*self.model_dim,\n",
    "                                              num_heads=self.num_heads,\n",
    "                                              dropout_prob=self.dropout_prob)\n",
    "        # Output classifier per sequence lement\n",
    "        self.output_net = [\n",
    "            nn.Dense(self.model_dim),\n",
    "            nn.LayerNorm(),\n",
    "            nn.relu,\n",
    "            nn.Dropout(self.dropout_prob),\n",
    "            nn.Dense(self.num_classes)\n",
    "        ]\n",
    "\n",
    "    def __call__(self, x, mask=None, add_positional_encoding=True, train=True):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            x - Input features of shape [Batch, SeqLen, input_dim]\n",
    "            mask - Mask to apply on the attention outputs (optional)\n",
    "            add_positional_encoding - If True, we add the positional encoding to the input.\n",
    "                                      Might not be desired for some tasks.\n",
    "            train - If True, dropout is stochastic\n",
    "        \"\"\"\n",
    "        x = self.input_dropout(x, deterministic=not train)\n",
    "        x = self.input_layer(x)\n",
    "        if add_positional_encoding:\n",
    "            x = self.positional_encoding(x)\n",
    "        x = self.transformer(x, mask=mask, train=train)\n",
    "        for l in self.output_net:\n",
    "            x = l(x) if not isinstance(l, nn.Dropout) else l(x, deterministic=not train)\n",
    "        return x\n",
    "\n",
    "    def get_attention_maps(self, x, mask=None, add_positional_encoding=True, train=True):\n",
    "        \"\"\"\n",
    "        Function for extracting the attention matrices of the whole Transformer for a single batch.\n",
    "        Input arguments same as the forward pass.\n",
    "        \"\"\"\n",
    "        x = self.input_dropout(x, deterministic=not train)\n",
    "        x = self.input_layer(x)\n",
    "        if add_positional_encoding:\n",
    "            x = self.positional_encoding(x)\n",
    "        attention_maps = self.transformer.get_attention_maps(x, mask=mask, train=train)\n",
    "        return attention_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out (3, 16, 10)\n",
      "Attention maps 5 (3, 4, 16, 16)\n"
     ]
    }
   ],
   "source": [
    "## Test TransformerPredictor implementation\n",
    "# Example features as input\n",
    "main_rng, x_rng = random.split(main_rng)\n",
    "x = random.normal(x_rng, (3, 16, 64))\n",
    "# Create Transformer encoder\n",
    "transpre = TransformerPredictor(num_layers=5, \n",
    "                                model_dim=128,\n",
    "                                num_classes=10,\n",
    "                                num_heads=4,\n",
    "                                dropout_prob=0.15,\n",
    "                                input_dropout_prob=0.05)\n",
    "# Initialize parameters of transformer predictor with random key and inputs\n",
    "main_rng, init_rng, dropout_init_rng = random.split(main_rng, 3)\n",
    "params = transpre.init({'params': init_rng, 'dropout': dropout_init_rng}, x, train=True)['params']\n",
    "# Apply transformer predictor with parameters on the inputs\n",
    "# Since dropout is stochastic, we need to pass a rng to the forward\n",
    "main_rng, dropout_apply_rng = random.split(main_rng)\n",
    "# Instead of passing params and rngs every time to a function call, we can bind them to the module\n",
    "binded_mod = transpre.bind({'params': params}, rngs={'dropout': dropout_apply_rng})\n",
    "out = binded_mod(x, train=True)\n",
    "print('Out', out.shape)\n",
    "attn_maps = binded_mod.get_attention_maps(x, train=True)\n",
    "print('Attention maps', len(attn_maps), attn_maps[0].shape)\n",
    "\n",
    "del transpre, binded_mod, params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer module\n",
    "\n",
    "Finally, we add the missing parts needed for training a model in JAX and Flax. Note that we leave the specific loss function unimplemented, since this function depends on different tasks we do below.\n",
    "\n",
    "In the optimization, we use the Adam optimizer with our previously discussed cosine scheduler. Additionally, we use the optax transformation `optax.clip_by_global_norm` ([documentation](https://optax.readthedocs.io/en/latest/api.html#optax.clip_by_global_norm)). This clips the norm of the gradients for all parameters before taking an optimizer step and prevents the model from diverging if we obtain very high gradients at, for instance, sharp loss surfaces (see many good blog posts on gradient clipping, like [DeepAI glossary](https://deepai.org/machine-learning-glossary-and-terms/gradient-clipping)). For Transformers, gradient clipping can help to further stabilize the training during the first few iterations, and also afterward. The clip value is usually between 0.5 and 10, depending on how harsh you want to clip large gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainerModule:\n",
    "    \n",
    "    def __init__(self, model_name, exmp_batch, max_iters, lr=1e-3, warmup=100, seed=42, **model_kwargs):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            model_name - Name of the model. Used for saving and checkpointing\n",
    "            exmp_batch - Example batch to the model for initialization\n",
    "            max_iters - Number of maximum iterations the model is trained for. This is needed for the CosineWarmup scheduler\n",
    "            lr - Learning rate in the optimizer\n",
    "            warmup - Number of warmup steps. Usually between 50 and 500\n",
    "            seed - Seed to use for model init\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        self.max_iters = max_iters\n",
    "        self.lr = lr\n",
    "        self.warmup = warmup\n",
    "        self.seed = seed\n",
    "        # Create empty model. Note: no parameters yet\n",
    "        self.model = TransformerPredictor(**model_kwargs)\n",
    "        # Prepare logging\n",
    "        self.log_dir = os.path.join(CHECKPOINT_PATH, self.model_name)\n",
    "        self.logger = SummaryWriter(log_dir=self.log_dir)\n",
    "        # Create jitted training and eval functions\n",
    "        self.create_functions()\n",
    "        # Initialize model\n",
    "        self.init_model(exmp_batch)\n",
    "        \n",
    "    def batch_to_input(self, exmp_batch):\n",
    "        # Map batch to input data to the model\n",
    "        # To be implemented in a task specific sub-class\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def get_loss_function(self):\n",
    "        # Return a function that calculates the loss for a batch\n",
    "        # To be implemented in a task specific sub-class\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def create_functions(self):\n",
    "        # Create jitted train and eval functions\n",
    "        calculate_loss = self.get_loss_function()\n",
    "        \n",
    "        # Training function\n",
    "        def train_step(state, rng, batch):\n",
    "            loss_fn = lambda params: calculate_loss(params, rng, batch, train=True)\n",
    "            ret, grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)\n",
    "            loss, acc, rng = ret[0], *ret[1]\n",
    "            state = state.apply_gradients(grads=grads)\n",
    "            return state, rng, loss, acc\n",
    "        self.train_step = jax.jit(train_step)\n",
    "        \n",
    "        # Evaluation function\n",
    "        def eval_step(state, rng, batch):\n",
    "            _, (acc, rng) = calculate_loss(state.params, rng, batch, train=False)\n",
    "            return acc, rng\n",
    "        self.eval_step = jax.jit(eval_step)\n",
    "        \n",
    "    def init_model(self, exmp_batch):\n",
    "        # Initialize model\n",
    "        self.rng = jax.random.PRNGKey(self.seed)\n",
    "        self.rng, init_rng, dropout_init_rng = jax.random.split(self.rng, 3)\n",
    "        exmp_input = self.batch_to_input(exmp_batch)\n",
    "        params = self.model.init({'params': init_rng, 'dropout': dropout_init_rng}, exmp_input, train=True)['params']\n",
    "        # Initialize learning rate schedule and optimizer\n",
    "        lr_schedule = optax.warmup_cosine_decay_schedule(\n",
    "            init_value=0.0,\n",
    "            peak_value=self.lr,\n",
    "            warmup_steps=self.warmup,\n",
    "            decay_steps=self.max_iters,\n",
    "            end_value=0.0\n",
    "        )\n",
    "        optimizer = optax.chain(\n",
    "            optax.clip_by_global_norm(1.0),  # Clip gradients at norm 1\n",
    "            optax.adam(lr_schedule)\n",
    "        )\n",
    "        # Initialize training state\n",
    "        self.state = train_state.TrainState.create(apply_fn=self.model.apply, params=params, tx=optimizer)\n",
    "        \n",
    "    def train_model(self, train_loader, val_loader, num_epochs=500):\n",
    "        # Train model for defined number of epochs\n",
    "        best_acc = 0.0\n",
    "        for epoch_idx in tqdm(range(1, num_epochs+1)):\n",
    "            self.train_epoch(train_loader, epoch=epoch_idx)\n",
    "            if epoch_idx % 5 == 0:\n",
    "                eval_acc = self.eval_model(val_loader)\n",
    "                self.logger.add_scalar('val/accuracy', eval_acc, global_step=epoch_idx)\n",
    "                if eval_acc >= best_acc:\n",
    "                    best_acc = eval_acc\n",
    "                    self.save_model(step=epoch_idx)\n",
    "                self.logger.flush()\n",
    "                \n",
    "    def train_epoch(self, train_loader, epoch):\n",
    "        # Train model for one epoch, and log avg loss and accuracy\n",
    "        accs, losses = [], []\n",
    "        for batch in tqdm(train_loader, desc='Training', leave=False):\n",
    "            self.state, self.rng, loss, accuracy = self.train_step(self.state, self.rng, batch)\n",
    "            losses.append(loss)\n",
    "            accs.append(accuracy)\n",
    "        avg_loss = np.stack(jax.device_get(losses)).mean()\n",
    "        avg_acc = np.stack(jax.device_get(accs)).mean()\n",
    "        self.logger.add_scalar('train/loss', avg_loss, global_step=epoch)\n",
    "        self.logger.add_scalar('train/accuracy', avg_acc, global_step=epoch)\n",
    "        \n",
    "    def eval_model(self, data_loader):\n",
    "        # Test model on all data points of a data loader and return avg accuracy\n",
    "        correct_class, count = 0, 0\n",
    "        for batch in data_loader:\n",
    "            acc, self.rng = self.eval_step(self.state, self.rng, batch)\n",
    "            correct_class += acc * batch[0].shape[0]\n",
    "            count += batch[0].shape[0]\n",
    "        eval_acc = (correct_class / count).item()\n",
    "        return eval_acc\n",
    "    \n",
    "    def save_model(self, step=0):\n",
    "        # Save current model at certain training iteration\n",
    "        checkpoints.save_checkpoint(ckpt_dir=self.log_dir, target=self.state.params, step=step)\n",
    "        \n",
    "    def load_model(self, pretrained=False):\n",
    "        # Load model. We use different checkpoint for the pretrained model\n",
    "        if not pretrained:\n",
    "            params = checkpoints.restore_checkpoint(ckpt_dir=self.log_dir, target=self.state.params)\n",
    "        else:\n",
    "            params = checkpoints.restore_checkpoint(ckpt_dir=os.path.join(CHECKPOINT_PATH, f'{self.model_name}.ckpt'), target=self.state.params)\n",
    "        self.state = train_state.TrainState.create(apply_fn=self.model.apply, params=params, tx=self.state.tx)\n",
    "        \n",
    "    def checkpoint_exists(self):\n",
    "        # Check whether a pretrained model exist for this Transformer\n",
    "        return os.path.isfile(os.path.join(CHECKPOINT_PATH, f'{self.model_name}.ckpt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "After having finished the implementation of the Transformer architecture, we can start experimenting and apply it to various tasks. In this notebook, we will focus on two tasks: parallel Sequence-to-Sequence, and set anomaly detection. The two tasks focus on different properties of the Transformer architecture, and we go through them below.\n",
    "\n",
    "### Sequence to Sequence\n",
    "\n",
    "A Sequence-to-Sequence task represents a task where the input _and_ the output is a sequence, not necessarily of the same length. Popular tasks in this domain include machine translation and summarization. For this, we usually have a Transformer encoder for interpreting the input sequence, and a decoder for generating the output in an autoregressive manner. Here, however, we will go back to a much simpler example task and use only the encoder. Given a sequence of $N$ numbers between $0$ and $M$, the task is to reverse the input sequence. In Numpy notation, if our input is $x$, the output should be $x$[::-1]. Although this task sounds very simple, RNNs can have issues with such because the task requires long-term dependencies. Transformers are built to support such, and hence, we expect it to perform very well. \n",
    "\n",
    "First, let's create a dataset class below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReverseDataset(data.Dataset):\n",
    "    \n",
    "    def __init__(self, num_categories, seq_len, size, np_rng):\n",
    "        super().__init__()\n",
    "        self.num_categories = num_categories\n",
    "        self.seq_len = seq_len\n",
    "        self.size = size\n",
    "        self.np_rng = np_rng\n",
    "\n",
    "        self.data = self.np_rng.integers(self.num_categories, size=(self.size, self.seq_len))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inp_data = self.data[idx]\n",
    "        labels = np.flip(inp_data, axis=0)\n",
    "        return inp_data, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create an arbitrary number of random sequences of numbers between 0 and `num_categories-1`. The label is simply the tensor flipped over the sequence dimension. We can create the corresponding data loaders using PyTorch below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine batch elements (all numpy) by stacking\n",
    "def numpy_collate(batch):\n",
    "    if isinstance(batch[0], np.ndarray):\n",
    "        return np.stack(batch)\n",
    "    elif isinstance(batch[0], (tuple,list)):\n",
    "        transposed = zip(*batch)\n",
    "        return [numpy_collate(samples) for samples in transposed]\n",
    "    else:\n",
    "        return np.array(batch)\n",
    "\n",
    "dataset = partial(ReverseDataset, 10, 16)\n",
    "rev_train_loader = data.DataLoader(dataset(50000, np_rng=np.random.default_rng(42)), \n",
    "                                   batch_size=128, \n",
    "                                   shuffle=True, \n",
    "                                   drop_last=True,\n",
    "                                   collate_fn=numpy_collate)\n",
    "rev_val_loader   = data.DataLoader(dataset(1000, np_rng=np.random.default_rng(43)), \n",
    "                                   batch_size=128,\n",
    "                                   collate_fn=numpy_collate)\n",
    "rev_test_loader  = data.DataLoader(dataset(10000, np_rng=np.random.default_rng(44)), \n",
    "                                   batch_size=128,\n",
    "                                   collate_fn=numpy_collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that these data loaders return numpy arrays instead of PyTorch tensors, as we define in the `numpy_collate` function which combines individual elements to a batch. As the data set is so simple and the `__getitem__` finishes a neglectable time, we don't need subprocesses, i.e. workers, to provide us the data (in fact, more workers can slow down the training as we have communication overhead among processes/threads). \n",
    "\n",
    "Now, let's look at an arbitrary sample of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data: [0 7 6 4 4 8 0 6 2 0 5 9 7 7 7 7]\n",
      "Labels:     [7 7 7 7 9 5 0 2 6 0 8 4 4 6 7 0]\n"
     ]
    }
   ],
   "source": [
    "inp_data, labels = rev_train_loader.dataset[0]\n",
    "print(\"Input data:\", inp_data)\n",
    "print(\"Labels:    \", labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training, we pass the input sequence through the Transformer encoder and predict the output for each input token. We use the standard Cross-Entropy loss to perform this. Every number is represented as a one-hot vector. Remember that representing the categories as single scalars decreases the expressiveness of the model extremely as $0$ and $1$ are not closer related than $0$ and $9$ in our example. An alternative to a one-hot vector is using a learned embedding vector as it is provided by an `nn.Embed` module ([documentation](https://flax.readthedocs.io/en/latest/_autosummary/flax.linen.Embed.html)). However, using a one-hot vector with an additional linear layer as in our case has the same effect as an embedding layer (`self.input_net` maps one-hot vector to a dense vector, where each row of the weight matrix represents the embedding for a specific category).\n",
    "\n",
    "To implement the training dynamic, we create a new class inheriting from `TrainerModule` and defining the loss function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReverseTrainer(TrainerModule):\n",
    "    \n",
    "    def batch_to_input(self, batch):\n",
    "        inp_data, _ = batch\n",
    "        inp_data = jax.nn.one_hot(inp_data, num_classes=self.model.num_classes)\n",
    "        return inp_data\n",
    "    \n",
    "    def get_loss_function(self):\n",
    "        # Function for calculating loss and accuracy for a batch\n",
    "        def calculate_loss(params, rng, batch, train):\n",
    "            inp_data, labels = batch\n",
    "            inp_data = jax.nn.one_hot(inp_data, num_classes=self.model.num_classes)\n",
    "            rng, dropout_apply_rng = random.split(rng)\n",
    "            logits = self.model.apply({'params': params}, inp_data, train=train, rngs={'dropout': dropout_apply_rng})\n",
    "            loss = optax.softmax_cross_entropy_with_integer_labels(logits, labels).mean()\n",
    "            acc = (logits.argmax(axis=-1) == labels).mean()\n",
    "            return loss, (acc, rng)\n",
    "        return calculate_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can create a training function, similar to ones we have seen before. We create a `ReverseTrainer` object, run the training for $N$ epochs while logging in TensorBoard, and saving our best model based on the validation. Afterward, we test our models on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_reverse(max_epochs=10, **model_args):\n",
    "    num_train_iters = len(rev_train_loader) * max_epochs\n",
    "    # Create a trainer module with specified hyperparameters\n",
    "    trainer = ReverseTrainer(model_name='ReverseTask', \n",
    "                             exmp_batch=next(iter(rev_train_loader)),\n",
    "                             max_iters=num_train_iters, \n",
    "                             **model_args)\n",
    "    if not trainer.checkpoint_exists():  # Skip training if pretrained model exists\n",
    "        trainer.train_model(rev_train_loader, rev_val_loader, num_epochs=max_epochs)\n",
    "        trainer.load_model()\n",
    "    else:\n",
    "        trainer.load_model(pretrained=True)\n",
    "    val_acc = trainer.eval_model(rev_val_loader)\n",
    "    test_acc = trainer.eval_model(rev_test_loader)\n",
    "    # Bind parameters to model for easier inference\n",
    "    trainer.model_bd = trainer.model.bind({'params': trainer.state.params})\n",
    "    return trainer, {'val_acc': val_acc, 'test_acc': test_acc}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can train the model. In this setup, we will use a single encoder block and a single head in the Multi-Head Attention. This is chosen because of the simplicity of the task, and in this case, the attention can actually be interpreted as an \"explanation\" of the predictions (compared to the other papers above dealing with deep Transformers). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_trainer, reverse_result = train_reverse(model_dim=32,\n",
    "                                                num_heads=1,\n",
    "                                                num_classes=rev_train_loader.dataset.num_categories,\n",
    "                                                num_layers=1,\n",
    "                                                dropout_prob=0.0,\n",
    "                                                lr=5e-4,\n",
    "                                                warmup=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's print the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val accuracy:  100.00%\n",
      "Test accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Val accuracy:  {(100.0 * reverse_result['val_acc']):4.2f}%\")\n",
    "print(f\"Test accuracy: {(100.0 * reverse_result['test_acc']):4.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we would have expected, the Transformer can correctly solve the task. However, how does the attention in the Multi-Head Attention block looks like for an arbitrary input? Let's try to visualize it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_input, labels = next(iter(rev_val_loader))\n",
    "inp_data = jax.nn.one_hot(data_input, num_classes=reverse_trainer.model.num_classes)\n",
    "attention_maps = reverse_trainer.model_bd.get_attention_maps(inp_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object `attention_maps` is a list of length $N$ where $N$ is the number of layers. Each element is a tensor of shape [Batch, Heads, SeqLen, SeqLen], which we can verify below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 1, 16, 16)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_maps[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will write a plotting function that takes as input the sequences, attention maps, and an index indicating for which batch element we want to visualize the attention map. We will create a plot where over rows, we have different layers, while over columns, we show the different heads. Remember that the softmax has been applied for each row separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention_maps(input_data, attn_maps, idx=0):\n",
    "    if input_data is not None:\n",
    "        input_data = jax.device_get(input_data[idx])\n",
    "    else:\n",
    "        input_data = np.arange(attn_maps[0][idx].shape[-1])\n",
    "    attn_maps = [jax.device_get(m[idx]) for m in attn_maps]\n",
    "    \n",
    "    num_heads = attn_maps[0].shape[0]\n",
    "    num_layers = len(attn_maps)\n",
    "    seq_len = input_data.shape[0]\n",
    "    fig_size = 4 if num_heads == 1 else 3\n",
    "    fig, ax = plt.subplots(num_layers, num_heads, figsize=(num_heads*fig_size, num_layers*fig_size))\n",
    "    if num_layers == 1:\n",
    "        ax = [ax]\n",
    "    if num_heads == 1:\n",
    "        ax = [[a] for a in ax]\n",
    "    for row in range(num_layers):\n",
    "        for column in range(num_heads):\n",
    "            ax[row][column].imshow(attn_maps[row][column], origin='lower', vmin=0)\n",
    "            ax[row][column].set_xticks(list(range(seq_len)))\n",
    "            ax[row][column].set_xticklabels(input_data.tolist())\n",
    "            ax[row][column].set_yticks(list(range(seq_len)))\n",
    "            ax[row][column].set_yticklabels(input_data.tolist())\n",
    "            ax[row][column].set_title(f\"Layer {row+1}, Head {column+1}\")\n",
    "    fig.subplots_adjust(hspace=0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can plot the attention map of our trained Transformer on the reverse task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/pdf": "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\n",
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"249.5225pt\" height=\"267.95625pt\" viewBox=\"0 0 249.5225 267.95625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2022-10-17T15:54:18.673186</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.6.1, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 267.95625 \n",
       "L 249.5225 267.95625 \n",
       "L 249.5225 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 20.5625 244.078125 \n",
       "L 242.3225 244.078125 \n",
       "L 242.3225 22.318125 \n",
       "L 20.5625 22.318125 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g clip-path=\"url(#p431a95b61d)\">\n",
       "    <image xlink:href=\"data:image/png;base64,\n",
       "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\" id=\"imaged901337e7e\" transform=\"scale(1 -1) translate(0 -221.76)\" x=\"20.5625\" y=\"-22.318125\" width=\"221.76\" height=\"221.76\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"mf1f3a7a50c\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#mf1f3a7a50c\" x=\"27.4925\" y=\"244.078125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 5 -->\n",
       "      <g transform=\"translate(24.31125 258.676562) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-35\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mf1f3a7a50c\" x=\"41.3525\" y=\"244.078125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 6 -->\n",
       "      <g transform=\"translate(38.17125 258.676562) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \n",
       "Q 1688 2584 1439 2293 \n",
       "Q 1191 2003 1191 1497 \n",
       "Q 1191 994 1439 701 \n",
       "Q 1688 409 2113 409 \n",
       "Q 2538 409 2786 701 \n",
       "Q 3034 994 3034 1497 \n",
       "Q 3034 2003 2786 2293 \n",
       "Q 2538 2584 2113 2584 \n",
       "z\n",
       "M 3366 4563 \n",
       "L 3366 3988 \n",
       "Q 3128 4100 2886 4159 \n",
       "Q 2644 4219 2406 4219 \n",
       "Q 1781 4219 1451 3797 \n",
       "Q 1122 3375 1075 2522 \n",
       "Q 1259 2794 1537 2939 \n",
       "Q 1816 3084 2150 3084 \n",
       "Q 2853 3084 3261 2657 \n",
       "Q 3669 2231 3669 1497 \n",
       "Q 3669 778 3244 343 \n",
       "Q 2819 -91 2113 -91 \n",
       "Q 1303 -91 875 529 \n",
       "Q 447 1150 447 2328 \n",
       "Q 447 3434 972 4092 \n",
       "Q 1497 4750 2381 4750 \n",
       "Q 2619 4750 2861 4703 \n",
       "Q 3103 4656 3366 4563 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-36\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mf1f3a7a50c\" x=\"55.2125\" y=\"244.078125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 4 -->\n",
       "      <g transform=\"translate(52.03125 258.676562) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mf1f3a7a50c\" x=\"69.0725\" y=\"244.078125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(65.89125 258.676562) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mf1f3a7a50c\" x=\"82.9325\" y=\"244.078125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 5 -->\n",
       "      <g transform=\"translate(79.75125 258.676562) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-35\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mf1f3a7a50c\" x=\"96.7925\" y=\"244.078125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(93.61125 258.676562) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_7\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mf1f3a7a50c\" x=\"110.6525\" y=\"244.078125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(107.47125 258.676562) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_8\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mf1f3a7a50c\" x=\"124.5125\" y=\"244.078125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 8 -->\n",
       "      <g transform=\"translate(121.33125 258.676562) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \n",
       "Q 1584 2216 1326 1975 \n",
       "Q 1069 1734 1069 1313 \n",
       "Q 1069 891 1326 650 \n",
       "Q 1584 409 2034 409 \n",
       "Q 2484 409 2743 651 \n",
       "Q 3003 894 3003 1313 \n",
       "Q 3003 1734 2745 1975 \n",
       "Q 2488 2216 2034 2216 \n",
       "z\n",
       "M 1403 2484 \n",
       "Q 997 2584 770 2862 \n",
       "Q 544 3141 544 3541 \n",
       "Q 544 4100 942 4425 \n",
       "Q 1341 4750 2034 4750 \n",
       "Q 2731 4750 3128 4425 \n",
       "Q 3525 4100 3525 3541 \n",
       "Q 3525 3141 3298 2862 \n",
       "Q 3072 2584 2669 2484 \n",
       "Q 3125 2378 3379 2068 \n",
       "Q 3634 1759 3634 1313 \n",
       "Q 3634 634 3220 271 \n",
       "Q 2806 -91 2034 -91 \n",
       "Q 1263 -91 848 271 \n",
       "Q 434 634 434 1313 \n",
       "Q 434 1759 690 2068 \n",
       "Q 947 2378 1403 2484 \n",
       "z\n",
       "M 1172 3481 \n",
       "Q 1172 3119 1398 2916 \n",
       "Q 1625 2713 2034 2713 \n",
       "Q 2441 2713 2670 2916 \n",
       "Q 2900 3119 2900 3481 \n",
       "Q 2900 3844 2670 4047 \n",
       "Q 2441 4250 2034 4250 \n",
       "Q 1625 4250 1398 4047 \n",
       "Q 1172 3844 1172 3481 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-38\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_9\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mf1f3a7a50c\" x=\"138.3725\" y=\"244.078125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 4 -->\n",
       "      <g transform=\"translate(135.19125 258.676562) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_10\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mf1f3a7a50c\" x=\"152.2325\" y=\"244.078125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 5 -->\n",
       "      <g transform=\"translate(149.05125 258.676562) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-35\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_11\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mf1f3a7a50c\" x=\"166.0925\" y=\"244.078125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 9 -->\n",
       "      <g transform=\"translate(162.91125 258.676562) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-39\" d=\"M 703 97 \n",
       "L 703 672 \n",
       "Q 941 559 1184 500 \n",
       "Q 1428 441 1663 441 \n",
       "Q 2288 441 2617 861 \n",
       "Q 2947 1281 2994 2138 \n",
       "Q 2813 1869 2534 1725 \n",
       "Q 2256 1581 1919 1581 \n",
       "Q 1219 1581 811 2004 \n",
       "Q 403 2428 403 3163 \n",
       "Q 403 3881 828 4315 \n",
       "Q 1253 4750 1959 4750 \n",
       "Q 2769 4750 3195 4129 \n",
       "Q 3622 3509 3622 2328 \n",
       "Q 3622 1225 3098 567 \n",
       "Q 2575 -91 1691 -91 \n",
       "Q 1453 -91 1209 -44 \n",
       "Q 966 3 703 97 \n",
       "z\n",
       "M 1959 2075 \n",
       "Q 2384 2075 2632 2365 \n",
       "Q 2881 2656 2881 3163 \n",
       "Q 2881 3666 2632 3958 \n",
       "Q 2384 4250 1959 4250 \n",
       "Q 1534 4250 1286 3958 \n",
       "Q 1038 3666 1038 3163 \n",
       "Q 1038 2656 1286 2365 \n",
       "Q 1534 2075 1959 2075 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-39\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_12\">\n",
       "     <g id=\"line2d_12\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mf1f3a7a50c\" x=\"179.9525\" y=\"244.078125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(176.77125 258.676562) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_13\">\n",
       "     <g id=\"line2d_13\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mf1f3a7a50c\" x=\"193.8125\" y=\"244.078125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_13\">\n",
       "      <!-- 8 -->\n",
       "      <g transform=\"translate(190.63125 258.676562) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-38\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_14\">\n",
       "     <g id=\"line2d_14\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mf1f3a7a50c\" x=\"207.6725\" y=\"244.078125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_14\">\n",
       "      <!-- 7 -->\n",
       "      <g transform=\"translate(204.49125 258.676562) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-37\" d=\"M 525 4666 \n",
       "L 3525 4666 \n",
       "L 3525 4397 \n",
       "L 1831 0 \n",
       "L 1172 0 \n",
       "L 2766 4134 \n",
       "L 525 4134 \n",
       "L 525 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-37\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_15\">\n",
       "     <g id=\"line2d_15\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mf1f3a7a50c\" x=\"221.5325\" y=\"244.078125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_15\">\n",
       "      <!-- 3 -->\n",
       "      <g transform=\"translate(218.35125 258.676562) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \n",
       "Q 3050 2419 3304 2112 \n",
       "Q 3559 1806 3559 1356 \n",
       "Q 3559 666 3084 287 \n",
       "Q 2609 -91 1734 -91 \n",
       "Q 1441 -91 1130 -33 \n",
       "Q 819 25 488 141 \n",
       "L 488 750 \n",
       "Q 750 597 1062 519 \n",
       "Q 1375 441 1716 441 \n",
       "Q 2309 441 2620 675 \n",
       "Q 2931 909 2931 1356 \n",
       "Q 2931 1769 2642 2001 \n",
       "Q 2353 2234 1838 2234 \n",
       "L 1294 2234 \n",
       "L 1294 2753 \n",
       "L 1863 2753 \n",
       "Q 2328 2753 2575 2939 \n",
       "Q 2822 3125 2822 3475 \n",
       "Q 2822 3834 2567 4026 \n",
       "Q 2313 4219 1838 4219 \n",
       "Q 1578 4219 1281 4162 \n",
       "Q 984 4106 628 3988 \n",
       "L 628 4550 \n",
       "Q 988 4650 1302 4700 \n",
       "Q 1616 4750 1894 4750 \n",
       "Q 2613 4750 3031 4423 \n",
       "Q 3450 4097 3450 3541 \n",
       "Q 3450 3153 3228 2886 \n",
       "Q 3006 2619 2597 2516 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-33\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_16\">\n",
       "     <g id=\"line2d_16\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mf1f3a7a50c\" x=\"235.3925\" y=\"244.078125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_16\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(232.21125 258.676562) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_17\">\n",
       "      <defs>\n",
       "       <path id=\"m78e10b4914\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m78e10b4914\" x=\"20.5625\" y=\"237.148125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_17\">\n",
       "      <!-- 5 -->\n",
       "      <g transform=\"translate(7.2 240.947344) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-35\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_18\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m78e10b4914\" x=\"20.5625\" y=\"223.288125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_18\">\n",
       "      <!-- 6 -->\n",
       "      <g transform=\"translate(7.2 227.087344) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-36\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_19\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m78e10b4914\" x=\"20.5625\" y=\"209.428125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_19\">\n",
       "      <!-- 4 -->\n",
       "      <g transform=\"translate(7.2 213.227344) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_20\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m78e10b4914\" x=\"20.5625\" y=\"195.568125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_20\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(7.2 199.367344) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_21\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m78e10b4914\" x=\"20.5625\" y=\"181.708125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_21\">\n",
       "      <!-- 5 -->\n",
       "      <g transform=\"translate(7.2 185.507344) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-35\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_22\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m78e10b4914\" x=\"20.5625\" y=\"167.848125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_22\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(7.2 171.647344) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_7\">\n",
       "     <g id=\"line2d_23\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m78e10b4914\" x=\"20.5625\" y=\"153.988125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_23\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(7.2 157.787344) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_8\">\n",
       "     <g id=\"line2d_24\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m78e10b4914\" x=\"20.5625\" y=\"140.128125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_24\">\n",
       "      <!-- 8 -->\n",
       "      <g transform=\"translate(7.2 143.927344) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-38\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_9\">\n",
       "     <g id=\"line2d_25\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m78e10b4914\" x=\"20.5625\" y=\"126.268125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_25\">\n",
       "      <!-- 4 -->\n",
       "      <g transform=\"translate(7.2 130.067344) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_10\">\n",
       "     <g id=\"line2d_26\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m78e10b4914\" x=\"20.5625\" y=\"112.408125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_26\">\n",
       "      <!-- 5 -->\n",
       "      <g transform=\"translate(7.2 116.207344) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-35\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_11\">\n",
       "     <g id=\"line2d_27\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m78e10b4914\" x=\"20.5625\" y=\"98.548125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_27\">\n",
       "      <!-- 9 -->\n",
       "      <g transform=\"translate(7.2 102.347344) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-39\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_12\">\n",
       "     <g id=\"line2d_28\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m78e10b4914\" x=\"20.5625\" y=\"84.688125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_28\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(7.2 88.487344) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_13\">\n",
       "     <g id=\"line2d_29\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m78e10b4914\" x=\"20.5625\" y=\"70.828125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_29\">\n",
       "      <!-- 8 -->\n",
       "      <g transform=\"translate(7.2 74.627344) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-38\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_14\">\n",
       "     <g id=\"line2d_30\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m78e10b4914\" x=\"20.5625\" y=\"56.968125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_30\">\n",
       "      <!-- 7 -->\n",
       "      <g transform=\"translate(7.2 60.767344) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-37\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_15\">\n",
       "     <g id=\"line2d_31\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m78e10b4914\" x=\"20.5625\" y=\"43.108125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_31\">\n",
       "      <!-- 3 -->\n",
       "      <g transform=\"translate(7.2 46.907344) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-33\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_16\">\n",
       "     <g id=\"line2d_32\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m78e10b4914\" x=\"20.5625\" y=\"29.248125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_32\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(7.2 33.047344) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 20.5625 244.078125 \n",
       "L 20.5625 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 242.3225 244.078125 \n",
       "L 242.3225 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 20.5625 244.078125 \n",
       "L 242.3225 244.078125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 20.5625 22.318125 \n",
       "L 242.3225 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_33\">\n",
       "    <!-- Layer 1, Head 1 -->\n",
       "    <g transform=\"translate(83.760312 16.318125) scale(0.12 -0.12)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-4c\" d=\"M 628 4666 \n",
       "L 1259 4666 \n",
       "L 1259 531 \n",
       "L 3531 531 \n",
       "L 3531 0 \n",
       "L 628 0 \n",
       "L 628 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \n",
       "Q 1497 1759 1228 1600 \n",
       "Q 959 1441 959 1056 \n",
       "Q 959 750 1161 570 \n",
       "Q 1363 391 1709 391 \n",
       "Q 2188 391 2477 730 \n",
       "Q 2766 1069 2766 1631 \n",
       "L 2766 1759 \n",
       "L 2194 1759 \n",
       "z\n",
       "M 3341 1997 \n",
       "L 3341 0 \n",
       "L 2766 0 \n",
       "L 2766 531 \n",
       "Q 2569 213 2275 61 \n",
       "Q 1981 -91 1556 -91 \n",
       "Q 1019 -91 701 211 \n",
       "Q 384 513 384 1019 \n",
       "Q 384 1609 779 1909 \n",
       "Q 1175 2209 1959 2209 \n",
       "L 2766 2209 \n",
       "L 2766 2266 \n",
       "Q 2766 2663 2505 2880 \n",
       "Q 2244 3097 1772 3097 \n",
       "Q 1472 3097 1187 3025 \n",
       "Q 903 2953 641 2809 \n",
       "L 641 3341 \n",
       "Q 956 3463 1253 3523 \n",
       "Q 1550 3584 1831 3584 \n",
       "Q 2591 3584 2966 3190 \n",
       "Q 3341 2797 3341 1997 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-79\" d=\"M 2059 -325 \n",
       "Q 1816 -950 1584 -1140 \n",
       "Q 1353 -1331 966 -1331 \n",
       "L 506 -1331 \n",
       "L 506 -850 \n",
       "L 844 -850 \n",
       "Q 1081 -850 1212 -737 \n",
       "Q 1344 -625 1503 -206 \n",
       "L 1606 56 \n",
       "L 191 3500 \n",
       "L 800 3500 \n",
       "L 1894 763 \n",
       "L 2988 3500 \n",
       "L 3597 3500 \n",
       "L 2059 -325 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \n",
       "Q 2534 3019 2420 3045 \n",
       "Q 2306 3072 2169 3072 \n",
       "Q 1681 3072 1420 2755 \n",
       "Q 1159 2438 1159 1844 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1341 3275 1631 3429 \n",
       "Q 1922 3584 2338 3584 \n",
       "Q 2397 3584 2469 3576 \n",
       "Q 2541 3569 2628 3553 \n",
       "L 2631 2963 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-2c\" d=\"M 750 794 \n",
       "L 1409 794 \n",
       "L 1409 256 \n",
       "L 897 -744 \n",
       "L 494 -744 \n",
       "L 750 256 \n",
       "L 750 794 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-48\" d=\"M 628 4666 \n",
       "L 1259 4666 \n",
       "L 1259 2753 \n",
       "L 3553 2753 \n",
       "L 3553 4666 \n",
       "L 4184 4666 \n",
       "L 4184 0 \n",
       "L 3553 0 \n",
       "L 3553 2222 \n",
       "L 1259 2222 \n",
       "L 1259 0 \n",
       "L 628 0 \n",
       "L 628 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-64\" d=\"M 2906 2969 \n",
       "L 2906 4863 \n",
       "L 3481 4863 \n",
       "L 3481 0 \n",
       "L 2906 0 \n",
       "L 2906 525 \n",
       "Q 2725 213 2448 61 \n",
       "Q 2172 -91 1784 -91 \n",
       "Q 1150 -91 751 415 \n",
       "Q 353 922 353 1747 \n",
       "Q 353 2572 751 3078 \n",
       "Q 1150 3584 1784 3584 \n",
       "Q 2172 3584 2448 3432 \n",
       "Q 2725 3281 2906 2969 \n",
       "z\n",
       "M 947 1747 \n",
       "Q 947 1113 1208 752 \n",
       "Q 1469 391 1925 391 \n",
       "Q 2381 391 2643 752 \n",
       "Q 2906 1113 2906 1747 \n",
       "Q 2906 2381 2643 2742 \n",
       "Q 2381 3103 1925 3103 \n",
       "Q 1469 3103 1208 2742 \n",
       "Q 947 2381 947 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-4c\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-61\" x=\"55.712891\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-79\" x=\"116.992188\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" x=\"176.171875\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-72\" x=\"237.695312\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"278.808594\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-31\" x=\"310.595703\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-2c\" x=\"374.21875\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"406.005859\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-48\" x=\"437.792969\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" x=\"512.988281\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-61\" x=\"574.511719\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-64\" x=\"635.791016\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"699.267578\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-31\" x=\"731.054688\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p431a95b61d\">\n",
       "   <rect x=\"20.5625\" y=\"22.318125\" width=\"221.76\" height=\"221.76\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_attention_maps(data_input, attention_maps, idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has learned to attend to the token that is on the flipped index of itself. Hence, it actually does what we intended it to do. We see that it however also pays some attention to values close to the flipped index. This is because the model doesn't need the perfect, hard attention to solve this problem, but is fine with this approximate, noisy attention map. The close-by indices are caused by the similarity of the positional encoding, which we also intended with the positional encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Anomaly Detection\n",
    "\n",
    "Besides sequences, sets are another data structure that is relevant for many applications. In contrast to sequences, elements are unordered in a set. RNNs can only be applied on sets by assuming an order in the data, which however biases the model towards a non-existing order in the data. [Vinyals et al. (2015)](https://arxiv.org/abs/1511.06391) and other papers have shown that the assumed order can have a significant impact on the model's performance, and hence, we should try to not use RNNs on sets. Ideally, our model should be permutation-equivariant/invariant such that the output is the same no matter how we sort the elements in a set. \n",
    "\n",
    "Transformers offer the perfect architecture for this as the Multi-Head Attention is permutation-equivariant, and thus, outputs the same values no matter in what order we enter the inputs (inputs and outputs are permuted equally). The task we are looking at for sets is _Set Anomaly Detection_ which means that we try to find the element(s) in a set that does not fit the others. In the research community, the common application of anomaly detection is performed on a set of images, where $N-1$ images belong to the same category/have the same high-level features while one belongs to another category. Note that category does not necessarily have to relate to a class in a standard classification problem, but could be the combination of multiple features. For instance, on a face dataset, this could be people with glasses, male, beard, etc. An example of distinguishing different animals can be seen below. The first four images show foxes, while the last represents a different animal. We want to recognize that the last image shows a different animal, but it is not relevant which class of animal it is.\n",
    "\n",
    "<center width=\"100%\" style=\"padding:20px\"><img src=\"../../tutorial6/cifar100_example_anomaly.png\" width=\"600px\"></center>\n",
    "\n",
    "In this tutorial, we will use the CIFAR100 dataset. CIFAR100 has 600 images for 100 classes each with a resolution of 32x32, similar to CIFAR10. The larger amount of classes requires the model to attend to specific features in the images instead of coarse features as in CIFAR10, therefore making the task harder. We will show the model a set of 9 images of one class, and 1 image from another class. The task is to find the image that is from a different class than the other images.\n",
    "Using the raw images directly as input to the Transformer is not a good idea, because it is not translation invariant as a CNN, and would need to learn to detect image features from high-dimensional input first of all. Instead, we will use a pre-trained ResNet34 model from the package `flaxmodels` ([link](https://github.com/matthias-wright/flaxmodels)) to obtain high-level, low-dimensional features of the images. The ResNet model has been pre-trained on the [ImageNet](http://image-net.org/) dataset which contains 1 million images of 1k classes and varying resolutions. However, during training and testing, the images are usually scaled to a resolution of 224x224, and hence we rescale our CIFAR images to this resolution as well. Below, we will load the dataset, and prepare the data for being processed by the ResNet model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dce6d54dd90643b9ba016aeda671d7d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/169001437 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-100-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "def image_to_numpy(img):\n",
    "    img = np.array(img, dtype=np.float32)\n",
    "    img = img / 255.   # Normalization is done in the ResNet\n",
    "    return img\n",
    "\n",
    "# Resize to 224x224, and map to JAX\n",
    "transform = transforms.Compose([transforms.Resize((224,224)),\n",
    "                                image_to_numpy\n",
    "                               ])\n",
    "# Loading the training dataset. \n",
    "train_set = CIFAR100(root=DATASET_PATH, train=True, transform=transform, download=True)\n",
    "\n",
    "# Loading the test set\n",
    "test_set = CIFAR100(root=DATASET_PATH, train=False, transform=transform, download=True)\n",
    "\n",
    "# For later, keep a dictionary mapping class indices to class names\n",
    "class_idx_to_name = {val: key for key, val in train_set.class_to_idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to run the pre-trained ResNet model on the images, and extract the features before the classification layer. These are the most high-level features, and should sufficiently describe the images. CIFAR100 has some similarity to ImageNet, and thus we are not retraining the ResNet model in any form. However, if you would want to get the best performance and have a very large dataset, it would be better to add the ResNet to the computation graph during training and finetune its parameters as well. As we don't have a large enough dataset and want to train our model efficiently, we will extract the features beforehand. Let's load and prepare the model below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/matthias-wright/flaxmodels.git\n",
      "  Cloning https://github.com/matthias-wright/flaxmodels.git to /tmp/pip-req-build-shybbq73\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/matthias-wright/flaxmodels.git /tmp/pip-req-build-shybbq73\n",
      "  Resolved https://github.com/matthias-wright/flaxmodels.git to commit edc6a8571a6d7202bd9f3bc9241221405c083fd4\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: h5py>=2.10.0 in ./.venv/lib/python3.8/site-packages (from flaxmodels==0.1.2) (3.7.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in ./.venv/lib/python3.8/site-packages (from flaxmodels==0.1.2) (1.23.3)\n",
      "Requirement already satisfied: requests>=2.23.0 in ./.venv/lib/python3.8/site-packages (from flaxmodels==0.1.2) (2.28.1)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.venv/lib/python3.8/site-packages (from flaxmodels==0.1.2) (21.3)\n",
      "Collecting dataclasses>=0.6\n",
      "  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: filelock>=3.0.12 in ./.venv/lib/python3.8/site-packages (from flaxmodels==0.1.2) (3.8.0)\n",
      "Requirement already satisfied: jax>=0.3 in ./.venv/lib/python3.8/site-packages (from flaxmodels==0.1.2) (0.3.21)\n",
      "Requirement already satisfied: jaxlib in ./.venv/lib/python3.8/site-packages (from flaxmodels==0.1.2) (0.3.20)\n",
      "Requirement already satisfied: flax>=0.4.0 in ./.venv/lib/python3.8/site-packages (from flaxmodels==0.1.2) (0.6.1)\n",
      "Requirement already satisfied: Pillow>=7.1.2 in ./.venv/lib/python3.8/site-packages (from flaxmodels==0.1.2) (9.2.0)\n",
      "Requirement already satisfied: regex>=2021.4.4 in ./.venv/lib/python3.8/site-packages (from flaxmodels==0.1.2) (2022.9.13)\n",
      "Requirement already satisfied: tqdm>=4.60.0 in ./.venv/lib/python3.8/site-packages (from flaxmodels==0.1.2) (4.64.1)\n",
      "Requirement already satisfied: msgpack in ./.venv/lib/python3.8/site-packages (from flax>=0.4.0->flaxmodels==0.1.2) (1.0.4)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in ./.venv/lib/python3.8/site-packages (from flax>=0.4.0->flaxmodels==0.1.2) (6.0)\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.8/site-packages (from flax>=0.4.0->flaxmodels==0.1.2) (3.6.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in ./.venv/lib/python3.8/site-packages (from flax>=0.4.0->flaxmodels==0.1.2) (4.4.0)\n",
      "Requirement already satisfied: rich>=11.1 in ./.venv/lib/python3.8/site-packages (from flax>=0.4.0->flaxmodels==0.1.2) (12.6.0)\n",
      "Requirement already satisfied: optax in ./.venv/lib/python3.8/site-packages (from flax>=0.4.0->flaxmodels==0.1.2) (0.1.3)\n",
      "Requirement already satisfied: etils[epath] in ./.venv/lib/python3.8/site-packages (from jax>=0.3->flaxmodels==0.1.2) (0.8.0)\n",
      "Requirement already satisfied: scipy>=1.5 in ./.venv/lib/python3.8/site-packages (from jax>=0.3->flaxmodels==0.1.2) (1.9.2)\n",
      "Requirement already satisfied: absl-py in ./.venv/lib/python3.8/site-packages (from jax>=0.3->flaxmodels==0.1.2) (1.2.0)\n",
      "Requirement already satisfied: opt-einsum in ./.venv/lib/python3.8/site-packages (from jax>=0.3->flaxmodels==0.1.2) (3.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in ./.venv/lib/python3.8/site-packages (from packaging>=20.9->flaxmodels==0.1.2) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in ./.venv/lib/python3.8/site-packages (from requests>=2.23.0->flaxmodels==0.1.2) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.8/site-packages (from requests>=2.23.0->flaxmodels==0.1.2) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.8/site-packages (from requests>=2.23.0->flaxmodels==0.1.2) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./.venv/lib/python3.8/site-packages (from requests>=2.23.0->flaxmodels==0.1.2) (1.26.12)\n",
      "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in ./.venv/lib/python3.8/site-packages (from rich>=11.1->flax>=0.4.0->flaxmodels==0.1.2) (0.9.1)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in ./.venv/lib/python3.8/site-packages (from rich>=11.1->flax>=0.4.0->flaxmodels==0.1.2) (2.13.0)\n",
      "Requirement already satisfied: zipp in ./.venv/lib/python3.8/site-packages (from etils[epath]->jax>=0.3->flaxmodels==0.1.2) (3.9.0)\n",
      "Requirement already satisfied: importlib_resources in ./.venv/lib/python3.8/site-packages (from etils[epath]->jax>=0.3->flaxmodels==0.1.2) (5.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./.venv/lib/python3.8/site-packages (from matplotlib->flax>=0.4.0->flaxmodels==0.1.2) (1.4.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.8/site-packages (from matplotlib->flax>=0.4.0->flaxmodels==0.1.2) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.8/site-packages (from matplotlib->flax>=0.4.0->flaxmodels==0.1.2) (4.37.4)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.8/site-packages (from matplotlib->flax>=0.4.0->flaxmodels==0.1.2) (0.11.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.8/site-packages (from matplotlib->flax>=0.4.0->flaxmodels==0.1.2) (1.0.5)\n",
      "Requirement already satisfied: chex>=0.0.4 in ./.venv/lib/python3.8/site-packages (from optax->flax>=0.4.0->flaxmodels==0.1.2) (0.1.5)\n",
      "Requirement already satisfied: toolz>=0.9.0 in ./.venv/lib/python3.8/site-packages (from chex>=0.0.4->optax->flax>=0.4.0->flaxmodels==0.1.2) (0.12.0)\n",
      "Requirement already satisfied: dm-tree>=0.1.5 in ./.venv/lib/python3.8/site-packages (from chex>=0.0.4->optax->flax>=0.4.0->flaxmodels==0.1.2) (0.1.7)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib->flax>=0.4.0->flaxmodels==0.1.2) (1.16.0)\n",
      "Building wheels for collected packages: flaxmodels\n",
      "  Building wheel for flaxmodels (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for flaxmodels: filename=flaxmodels-0.1.2-py3-none-any.whl size=115720 sha256=38736882633208cba9ba754be76e0987d276956804dbcdff1f3a58292c7ccbc7\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-f5ibr69o/wheels/b1/16/4e/6e7f5aa6a1c912db017f34a3bdf96761478b3cfdb907009951\n",
      "Successfully built flaxmodels\n",
      "Installing collected packages: dataclasses, flaxmodels\n",
      "Successfully installed dataclasses-0.6 flaxmodels-0.1.2\n",
      "Downloading: \"https://www.dropbox.com/s/rnqn2x6trnztg4c/resnet34_weights.h5\" to /tmp/flaxmodels/resnet34_weights.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 87.4M/87.4M [00:09<00:00, 8.92MiB/s]\n"
     ]
    }
   ],
   "source": [
    "# Import and install flaxmodels if needed\n",
    "import flaxmodels\n",
    "\n",
    "# Pretrained ResNet34 on ImageNet\n",
    "resnet34 = flaxmodels.ResNet34(output='activations', pretrained='imagenet', normalize=True)\n",
    "main_rng, resnet_rng = random.split(main_rng, 2)\n",
    "resnet_params = resnet34.init(resnet_rng, jnp.zeros((1, 224, 224, 3)))\n",
    "# Jit its forward pass for efficiency\n",
    "apply_resnet = jax.jit(lambda imgs: resnet34.apply(resnet_params, imgs, train=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now write a extraction function for the features below. This cell requires access to a GPU, as the model is rather deep and the images relatively large. The GPUs on GoogleColab are sufficient, but running this cell can take 2-3 minutes. Once it is run, the features are exported on disk so they don't have to be recalculated every time you run the notebook. However, this requires >150MB free disk space. So it is recommended to run this only on a local computer if you have enough free disk and a GPU (GoogleColab is fine for this). If you do not have a GPU, you can download the features from the [GoogleDrive folder](https://drive.google.com/drive/folders/1t8D-GTfmxJ42xLNTeMGMKSYuWBua5lXI?usp=sharing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "176a595e53c84e94bd9cebc2701f02b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [32], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m extracted_features\n\u001b[1;32m     18\u001b[0m train_feat_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(CHECKPOINT_PATH, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_set_features.npz\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m train_set_feats \u001b[38;5;241m=\u001b[39m \u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_feat_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m test_feat_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(CHECKPOINT_PATH, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_set_features.npz\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m test_feats \u001b[38;5;241m=\u001b[39m extract_features(test_set, test_feat_file)\n",
      "Cell \u001b[0;32mIn [32], line 7\u001b[0m, in \u001b[0;36mextract_features\u001b[0;34m(dataset, save_file)\u001b[0m\n\u001b[1;32m      5\u001b[0m extracted_features \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m imgs \u001b[38;5;129;01min\u001b[39;00m tqdm(data_loader):\n\u001b[0;32m----> 7\u001b[0m     feats \u001b[38;5;241m=\u001b[39m \u001b[43mapply_resnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# Average pooling on the last conv features to obtain a image-level feature vector\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     feats \u001b[38;5;241m=\u001b[39m feats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblock4_2\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def extract_features(dataset, save_file):\n",
    "    if not os.path.isfile(save_file):\n",
    "        data_loader = data.DataLoader(dataset, batch_size=128, shuffle=False, drop_last=False, \n",
    "                                      collate_fn=lambda batch: np.stack([b[0] for b in batch], axis=0))\n",
    "        extracted_features = []\n",
    "        for imgs in tqdm(data_loader):\n",
    "            feats = apply_resnet(imgs)\n",
    "            # Average pooling on the last conv features to obtain a image-level feature vector\n",
    "            feats = feats['block4_2'].mean(axis=(1,2))\n",
    "            extracted_features.append(feats)\n",
    "        extracted_features = jnp.concatenate(extracted_features, axis=0)\n",
    "        extracted_features = jax.device_get(extracted_features)\n",
    "        np.savez_compressed(save_file, feats=extracted_features)\n",
    "    else:\n",
    "        extracted_features = np.load(save_file)['feats']\n",
    "    return extracted_features\n",
    "\n",
    "train_feat_file = os.path.join(CHECKPOINT_PATH, \"train_set_features.npz\")\n",
    "train_set_feats = extract_features(train_set, train_feat_file)\n",
    "\n",
    "test_feat_file = os.path.join(CHECKPOINT_PATH, \"test_set_features.npz\")\n",
    "test_feats = extract_features(test_set, test_feat_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify the feature shapes below. The training should have 50k elements, and the test 10k images. The feature dimension is 512 for the ResNet34. If you experiment with other models, you likely see a different feature dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train:\", train_set_feats.shape)\n",
    "print(\"Test: \", test_feats.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we want to create a validation set to detect when we should stop training. In this case, we will split the training set into 90% training, 10% validation. However, the difficulty is here that we need to ensure that the validation set has the same number of images for all 100 labels. Otherwise, we have a class imbalance which is not good for creating the image sets. Hence, we take 10% of the images for each class, and move them into the validation set. The code below does exactly this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split train into train+val\n",
    "# Get labels from train set\n",
    "labels = np.array(train_set.targets, dtype=np.int32)\n",
    "\n",
    "# Get indices of images per class\n",
    "num_labels = labels.max()+1\n",
    "sorted_indices = np.argsort(labels).reshape(num_labels, -1) # [classes, num_imgs per class]\n",
    "\n",
    "# Determine number of validation images per class\n",
    "num_val_exmps = sorted_indices.shape[1] // 10\n",
    "\n",
    "# Get image indices for validation and training\n",
    "val_indices   = sorted_indices[:,:num_val_exmps].reshape(-1)\n",
    "train_indices = sorted_indices[:,num_val_exmps:].reshape(-1)\n",
    "\n",
    "# Group corresponding image features and labels\n",
    "train_feats, train_labels = train_set_feats[train_indices], labels[train_indices]\n",
    "val_feats,   val_labels   = train_set_feats[val_indices],   labels[val_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can prepare a dataset class for the set anomaly task. We define an epoch to be the sequence in which each image has been exactly once as an \"anomaly\". Hence, the length of the dataset is the number of images in it. For the training set, each time we access an item with `__getitem__`, we sample a random, different class than the image at the corresponding index `idx` has. In a second step, we sample $N-1$ images of this sampled class. The set of 10 images is finally returned. The randomness in the `__getitem__` allows us to see a slightly different set during each iteration. However, we can't use the same strategy for the test set as we want the test dataset to be the same every time we iterate over it. Hence, we sample the sets in the `__init__` method, and return those in `__getitem__`. The code below implements exactly this dynamic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SetAnomalyDataset(data.Dataset):\n",
    "    \n",
    "    def __init__(self, img_feats, labels, np_rng, set_size=10, train=True):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            img_feats - Tensor of shape [num_imgs, img_dim]. Represents the high-level features.\n",
    "            labels - Tensor of shape [num_imgs], containing the class labels for the images\n",
    "            set_size - Number of elements in a set. N-1 are sampled from one class, and one from another one.\n",
    "            train - If True, a new set will be sampled every time __getitem__ is called.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.img_feats = img_feats\n",
    "        self.labels = labels\n",
    "        self.np_rng = np_rng\n",
    "        self.set_size = set_size-1  # The set size is here the number of images from the same class per set\n",
    "        self.train = train\n",
    "        \n",
    "        # Tensors with indices of the images per class\n",
    "        self.num_labels = labels.max()+1\n",
    "        self.img_idx_by_label = np.argsort(self.labels).reshape(self.num_labels, -1)\n",
    "        \n",
    "        if not train:\n",
    "            self.test_sets = self._create_test_sets()\n",
    "            \n",
    "    def _create_test_sets(self):\n",
    "        # Pre-generates the sets for each image for the test set\n",
    "        test_sets = []\n",
    "        num_imgs = self.img_feats.shape[0]\n",
    "        test_sets = [self.sample_img_set(self.labels[idx]) for idx in range(num_imgs)]\n",
    "        test_sets = np.stack(test_sets, axis=0)\n",
    "        return test_sets\n",
    "               \n",
    "    def sample_img_set(self, anomaly_label):\n",
    "        \"\"\"\n",
    "        Samples a new set of images, given the label of the anomaly. \n",
    "        The sampled images come from a different class than anomaly_label\n",
    "        \"\"\"\n",
    "        # Sample class from 0,...,num_classes-1 while skipping anomaly_label as class\n",
    "        set_label = self.np_rng.integers(self.num_labels-1)\n",
    "        if set_label >= anomaly_label:\n",
    "            set_label += 1\n",
    "            \n",
    "        # Sample images from the class determined above\n",
    "        img_indices = self.np_rng.choice(self.img_idx_by_label.shape[1], size=self.set_size, replace=False)\n",
    "        img_indices = self.img_idx_by_label[set_label, img_indices]\n",
    "        return img_indices\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.img_feats.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        anomaly = self.img_feats[idx]\n",
    "        if self.train: # If train => sample\n",
    "            img_indices = self.sample_img_set(self.labels[idx])\n",
    "        else: # If test => use pre-generated ones\n",
    "            img_indices = self.test_sets[idx]\n",
    "            \n",
    "        # Concatenate images. The anomaly is always the last image for simplicity\n",
    "        img_set = np.concatenate([self.img_feats[img_indices], anomaly[None]], axis=0)\n",
    "        indices = np.concatenate([img_indices, np.array([idx], dtype=np.int32)], axis=0)\n",
    "        label = img_set.shape[0]-1\n",
    "        \n",
    "        # We return the indices of the images for visualization purpose. \"Label\" is the index of the anomaly\n",
    "        return img_set, indices, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can setup our datasets and data loaders below. Here, we will use a set size of 10, i.e. 9 images from one category + 1 anomaly. Feel free to change it if you want to experiment with the sizes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SET_SIZE = 10\n",
    "test_labels = np.array(test_set.targets, dtype=np.int32)\n",
    "\n",
    "anom_train_dataset = SetAnomalyDataset(train_feats, train_labels, np_rng=np.random.default_rng(42), set_size=SET_SIZE, train=True)\n",
    "anom_val_dataset   = SetAnomalyDataset(val_feats,   val_labels,   np_rng=np.random.default_rng(43), set_size=SET_SIZE, train=False)\n",
    "anom_test_dataset  = SetAnomalyDataset(test_feats,  test_labels,  np_rng=np.random.default_rng(123), set_size=SET_SIZE, train=False)\n",
    "\n",
    "anom_train_loader = data.DataLoader(anom_train_dataset, batch_size=64, shuffle=True,  drop_last=True,  collate_fn=numpy_collate)\n",
    "anom_val_loader   = data.DataLoader(anom_val_dataset,   batch_size=64, shuffle=False, drop_last=False, collate_fn=numpy_collate)\n",
    "anom_test_loader  = data.DataLoader(anom_test_dataset,  batch_size=64, shuffle=False, drop_last=False, collate_fn=numpy_collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the dataset a little better, we can plot below a few sets from the test dataset. Each row shows a different input set, where the first 9 are from the same class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_exmp(indices, orig_dataset):\n",
    "    images = [orig_dataset[idx][0] for idx in indices.reshape(-1)]\n",
    "    images = jax.device_get(jnp.stack(images, axis=0)).astype(np.float32)\n",
    "    images = torch.from_numpy(images)\n",
    "    images = images.permute(0, 3, 1, 2)\n",
    "    img_grid = torchvision.utils.make_grid(images, nrow=SET_SIZE, normalize=True, pad_value=0.5, padding=16)\n",
    "    img_grid = img_grid.permute(1, 2, 0)\n",
    "\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.title(\"Anomaly examples on CIFAR100\")\n",
    "    plt.imshow(img_grid)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "_, indices, _ = next(iter(anom_test_loader))\n",
    "visualize_exmp(indices[:4], test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can already see that for some sets the task might be easier than for others. Difficulties can especially arise if the anomaly is in a different, but yet visually similar class (e.g. insect vs mushroom, train vs bus, etc.).\n",
    "\n",
    "After having prepared the data, we can look closer at the model. Here, we have a classification of the whole set. For the prediction to be permutation-equivariant, we will output one logit for each image. Over these logits, we apply a softmax and train the anomaly image to have the highest score/probability. This is a bit different than a standard classification layer as the softmax is applied over images, not over output classes in the classical sense. However, if we swap two images in their position, we effectively swap their position in the output softmax. Hence, the prediction is equivariant with respect to the input. Furthermore, we need to remove the positional encoding since these features would break the permutation equivariance. We implement this setup below in the subclass of the Trainer module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnomalyTrainer(TrainerModule):\n",
    "    \n",
    "    def batch_to_input(self, batch):\n",
    "        inp_data, _, _ = batch\n",
    "        return inp_data\n",
    "    \n",
    "    def get_loss_function(self):\n",
    "        # Function for calculating loss and accuracy for a batch\n",
    "        def calculate_loss(params, rng, batch, train):\n",
    "            inp_data, _, labels = batch\n",
    "            rng, dropout_apply_rng = random.split(rng)\n",
    "            logits = self.model.apply({'params': params}, inp_data, \n",
    "                                      add_positional_encoding=False,  # No positional encoding since this is a permutation equivariant task\n",
    "                                      train=train, \n",
    "                                      rngs={'dropout': dropout_apply_rng})\n",
    "            logits = logits.squeeze(axis=-1)\n",
    "            loss = optax.softmax_cross_entropy_with_integer_labels(logits, labels).mean()\n",
    "            acc = (logits.argmax(axis=-1) == labels).astype(jnp.float32).mean()\n",
    "            return loss, (acc, rng)\n",
    "        return calculate_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we write our train function below. It has the exact same structure as the reverse task one, hence not much of an explanation is needed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_anomaly(max_epochs=100, **model_args):\n",
    "    num_train_iters = len(anom_train_loader) * max_epochs\n",
    "    # Create a trainer module with specified hyperparameters\n",
    "    trainer = AnomalyTrainer(model_name='SetAnomalyTask', \n",
    "                             exmp_batch=next(iter(anom_train_loader)),\n",
    "                             max_iters=num_train_iters, \n",
    "                             **model_args)\n",
    "    if not trainer.checkpoint_exists():  # Skip training if pretrained model exists\n",
    "        trainer.train_model(anom_train_loader, anom_val_loader, num_epochs=max_epochs)\n",
    "        trainer.load_model()\n",
    "    else:\n",
    "        trainer.load_model(pretrained=True)\n",
    "    train_acc = trainer.eval_model(anom_train_loader)\n",
    "    val_acc = trainer.eval_model(anom_val_loader)\n",
    "    test_acc = trainer.eval_model(anom_test_loader)\n",
    "    # Bind parameters to model for easier inference\n",
    "    trainer.model_bd = trainer.model.bind({'params': trainer.state.params})\n",
    "    return trainer, {'train_acc': train_acc, 'val_acc': val_acc, 'test_acc': test_acc}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's finally train our model. We will use 4 layers with 4 attention heads each. The hidden dimensionality of the model is 256, and we use a dropout of 0.1 throughout the model for good regularization. Note that we also apply the dropout on the input features, as this makes the model more robust against image noise and generalizes better. Again, we use warmup to slowly start our model training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_trainer, anomaly_result = train_anomaly(model_dim=256,\n",
    "                                                num_heads=4,\n",
    "                                                num_classes=1,\n",
    "                                                num_layers=4,\n",
    "                                                dropout_prob=0.1,\n",
    "                                                input_dropout_prob=0.1,\n",
    "                                                lr=5e-4,\n",
    "                                                warmup=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print the achieved accuracy below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train accuracy: {(100.0*anomaly_result['train_acc']):4.2f}%\")\n",
    "print(f\"Val accuracy:   {(100.0*anomaly_result['val_acc']):4.2f}%\")\n",
    "print(f\"Test accuracy:  {(100.0*anomaly_result['test_acc']):4.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With ~94% validation and test accuracy, the model generalizes quite well. It should be noted that you might see slightly different scores depending on what computer/device you are running this notebook. This is because despite setting the seed before generating the test dataset, it may not always be the same across platforms and numpy versions. Nevertheless, we can conclude that the model performs quite well and can solve the task for most sets. Before trying to interpret the model, let's verify that our model is permutation-equivariant, and assigns the same predictions for different permutations of the input set. For this, we sample a batch from the test set and run it through the model to obtain the probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_data, indices, labels = next(iter(anom_test_loader))\n",
    "preds = anomaly_trainer.model_bd(inp_data, add_positional_encoding=False, train=False)\n",
    "preds = jax.nn.softmax(preds.squeeze(axis=-1))\n",
    "\n",
    "permut = np.random.permutation(inp_data.shape[1])\n",
    "permut_inp_data = inp_data[:,permut]\n",
    "perm_preds = anomaly_trainer.model_bd(permut_inp_data, add_positional_encoding=False, train=False)\n",
    "perm_preds = jax.nn.softmax(perm_preds.squeeze(axis=-1))\n",
    "\n",
    "preds = jax.device_get(preds)\n",
    "perm_preds = jax.device_get(perm_preds)\n",
    "\n",
    "assert np.abs(preds[:,permut] - perm_preds).max() < 1e-5, \"Predictions are not permutation equivariant\"\n",
    "\n",
    "print(\"Preds\\n\", preds[0,permut])\n",
    "print(\"Permuted preds\\n\", perm_preds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the predictions are almost exactly the same, and only differ because of slight numerical differences inside the network operation.\n",
    "\n",
    "To interpret the model a little more, we can plot the attention maps inside the model. This will give us an idea of what information the model is sharing/communicating between images, and what each head might represent. First, we need to extract the attention maps for the test batch above, and determine the discrete predictions for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_maps = anomaly_trainer.model_bd.get_attention_maps(inp_data, add_positional_encoding=False, train=False)\n",
    "predictions = preds.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we write a plot function which plots the images in the input set, the prediction of the model, and the attention maps of the different heads on layers of the transformer. Feel free to explore the attention maps for different input examples as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_prediction(idx):\n",
    "    visualize_exmp(indices[idx:idx+1], test_set)\n",
    "    print(f'Main class: {class_idx_to_name[test_labels[indices[idx,0]]]}, Anomaly class: {class_idx_to_name[test_labels[indices[idx,-1]]]}')\n",
    "    print(f'Prediction: image {predictions[idx].item()}')\n",
    "    plot_attention_maps(input_data=None, attn_maps=attention_maps, idx=idx)\n",
    "\n",
    "visualize_prediction(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the random seed, you might see a slightly different input set. For the version on the website, we compare 9 aquarium fish images with a volcano/mountain. We see that multiple heads, for instance, Layer 2 Head 1, Layer 2 Head 4, and Layer 3 Head 2-4 focus on the last image. Additionally, the heads in Layer 4 all seem to ignore the last image and assign a very low attention probability to it. This shows that the model has indeed recognized that the image doesn't fit the setting, and hence predicted it to be the anomaly. Layer 2 Head 3 and Layer 3 Head 1 seems to take a slightly weighted average of all images. That might indicate that the model extracts the \"average\" information of all images, to compare it to the image features itself. \n",
    "\n",
    "Let's try to find where the model actually makes a mistake. We can do this by identifying the sets where the model predicts something else than 9, as in the dataset, we ensured that the anomaly is always at the last position in the set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistakes = np.where(predictions != 9)[0]\n",
    "print(\"Indices with mistake:\", mistakes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our model achieves ~94% accuracy, we only have very little number of mistakes in a batch of 64 sets. Still, let's visualize one of them, for example the last one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_prediction(mistakes[0])\n",
    "print(\"Probabilities:\")\n",
    "for i, p in enumerate(preds[mistakes[0]]):\n",
    "    print(f\"Image {i}: {100.0*p:4.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the model confuses a picture of a sea with a forest, giving a probability of ~55% to image 3, and 25% to the actual anomaly. However, the difficulty here is that the picture of the sea actually contains trees in the foreground, which makes the image a bit of an ambiguous class. It is possible a picture of a sea taken out of a forest, which confuses the model. Nevertheless, in general, the model performs quite well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we took a closer look at the Multi-Head Attention layer which uses a scaled dot product between queries and keys to find correlations and similarities between input elements. The Transformer architecture is based on the Multi-Head Attention layer and applies multiple of them in a ResNet-like block. The Transformer is a very important, recent architecture that can be applied to many tasks and datasets. Although it is best known for its success in NLP, there is so much more to it. We have seen its application on sequence-to-sequence tasks and set anomaly detection. Its property of being permutation-equivariant if we do not provide any positional encodings, allows it to generalize to many settings. Hence, it is important to know the architecture, but also its possible issues such as the gradient problem during the first iterations solved by learning rate warm-up. If you are interested in continuing with the study of the Transformer architecture, please have a look at the blog posts listed at the beginning of the tutorial notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "[![Star our repository](https://img.shields.io/static/v1.svg?logo=star&label=&message=Star%20Our%20Repository&color=yellow)](https://github.com/phlippe/uvadlc_notebooks/)  If you found this tutorial helpful, consider -ing our repository.    \n",
    "[![Ask questions](https://img.shields.io/static/v1.svg?logo=star&label=&message=Ask%20Questions&color=9cf)](https://github.com/phlippe/uvadlc_notebooks/issues)  For any questions, typos, or bugs that you found, please raise an issue on GitHub. \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
