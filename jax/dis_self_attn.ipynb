{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d7ee995-c46d-4e63-b981-7b56705d451f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.utils.checkpoint\n",
    "from torch import nn\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1238d16c-1c40-4d07-a517-27e1bf6a8353",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisentangledSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Disentangled self-attention module\n",
    "\n",
    "    Parameters:\n",
    "        config (`str`):\n",
    "            A model config class instance with the configuration to build a new model. The schema is similar to\n",
    "            *BertConfig*, for more details, please refer [`DebertaConfig`]\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        if config.hidden_size % config.num_attention_heads != 0:\n",
    "            raise ValueError(\n",
    "                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n",
    "                f\"heads ({config.num_attention_heads})\"\n",
    "            )\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "        self.in_proj = nn.Linear(config.hidden_size, self.all_head_size * 3, bias=False)\n",
    "        self.q_bias = nn.Parameter(torch.zeros((self.all_head_size), dtype=torch.float))\n",
    "        self.v_bias = nn.Parameter(torch.zeros((self.all_head_size), dtype=torch.float))\n",
    "        self.pos_att_type = config.pos_att_type if config.pos_att_type is not None else []\n",
    "\n",
    "        self.relative_attention = getattr(config, \"relative_attention\", False)\n",
    "        self.talking_head = getattr(config, \"talking_head\", False)\n",
    "\n",
    "        if self.talking_head:\n",
    "            self.head_logits_proj = nn.Linear(config.num_attention_heads, config.num_attention_heads, bias=False)\n",
    "            self.head_weights_proj = nn.Linear(config.num_attention_heads, config.num_attention_heads, bias=False)\n",
    "\n",
    "        if self.relative_attention:\n",
    "            self.max_relative_positions = getattr(config, \"max_relative_positions\", -1)\n",
    "            if self.max_relative_positions < 1:\n",
    "                self.max_relative_positions = config.max_position_embeddings\n",
    "            self.pos_dropout = StableDropout(config.hidden_dropout_prob)\n",
    "\n",
    "            if \"c2p\" in self.pos_att_type:\n",
    "                self.pos_proj = nn.Linear(config.hidden_size, self.all_head_size, bias=False)\n",
    "            if \"p2c\" in self.pos_att_type:\n",
    "                self.pos_q_proj = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "\n",
    "        self.dropout = StableDropout(config.attention_probs_dropout_prob)\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, -1)\n",
    "        x = x.view(new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask,\n",
    "        output_attentions=False,\n",
    "        query_states=None,\n",
    "        relative_pos=None,\n",
    "        rel_embeddings=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Call the module\n",
    "\n",
    "        Args:\n",
    "            hidden_states (`torch.FloatTensor`):\n",
    "                Input states to the module usually the output from previous layer, it will be the Q,K and V in\n",
    "                *Attention(Q,K,V)*\n",
    "\n",
    "            attention_mask (`torch.ByteTensor`):\n",
    "                An attention mask matrix of shape [*B*, *N*, *N*] where *B* is the batch size, *N* is the maximum\n",
    "                sequence length in which element [i,j] = *1* means the *i* th token in the input can attend to the *j*\n",
    "                th token.\n",
    "\n",
    "            output_attentions (`bool`, optional):\n",
    "                Whether return the attention matrix.\n",
    "\n",
    "            query_states (`torch.FloatTensor`, optional):\n",
    "                The *Q* state in *Attention(Q,K,V)*.\n",
    "\n",
    "            relative_pos (`torch.LongTensor`):\n",
    "                The relative position encoding between the tokens in the sequence. It's of shape [*B*, *N*, *N*] with\n",
    "                values ranging in [*-max_relative_positions*, *max_relative_positions*].\n",
    "\n",
    "            rel_embeddings (`torch.FloatTensor`):\n",
    "                The embedding of relative distances. It's a tensor of shape [\\\\(2 \\\\times\n",
    "                \\\\text{max_relative_positions}\\\\), *hidden_size*].\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        if query_states is None:\n",
    "            qp = self.in_proj(hidden_states)  # .split(self.all_head_size, dim=-1)\n",
    "            query_layer, key_layer, value_layer = self.transpose_for_scores(qp).chunk(3, dim=-1)\n",
    "        else:\n",
    "\n",
    "            def linear(w, b, x):\n",
    "                if b is not None:\n",
    "                    return torch.matmul(x, w.t()) + b.t()\n",
    "                else:\n",
    "                    return torch.matmul(x, w.t())  # + b.t()\n",
    "\n",
    "            ws = self.in_proj.weight.chunk(self.num_attention_heads * 3, dim=0)\n",
    "            qkvw = [torch.cat([ws[i * 3 + k] for i in range(self.num_attention_heads)], dim=0) for k in range(3)]\n",
    "            qkvb = [None] * 3\n",
    "\n",
    "            q = linear(qkvw[0], qkvb[0], torch.tensor(query_states, dtype=qkvw[0].dtype))\n",
    "            k, v = [linear(qkvw[i], qkvb[i], torch.tensor(hidden_states, dtype=qkvw[i].dtype)) for i in range(1, 3)]\n",
    "            query_layer, key_layer, value_layer = [self.transpose_for_scores(x) for x in [q, k, v]]\n",
    "\n",
    "        query_layer = query_layer + self.transpose_for_scores(self.q_bias[None, None, :])\n",
    "        value_layer = value_layer + self.transpose_for_scores(self.v_bias[None, None, :])\n",
    "\n",
    "        rel_att = None\n",
    "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
    "        scale_factor = 1 + len(self.pos_att_type)\n",
    "        scale = torch.sqrt(torch.tensor(query_layer.size(-1), dtype=torch.float) * scale_factor)\n",
    "        query_layer = query_layer / torch.tensor(scale, dtype=query_layer.dtype)\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        if self.relative_attention:\n",
    "            rel_embeddings = self.pos_dropout(rel_embeddings)\n",
    "            rel_att = self.disentangled_att_bias(query_layer, key_layer, relative_pos, rel_embeddings, scale_factor)\n",
    "\n",
    "        if rel_att is not None:\n",
    "            attention_scores = attention_scores + rel_att\n",
    "\n",
    "        # bxhxlxd\n",
    "        if self.talking_head:\n",
    "            attention_scores = self.head_logits_proj(attention_scores.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)\n",
    "\n",
    "        attention_probs = XSoftmax.apply(attention_scores, attention_mask, -1)\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "        if self.talking_head:\n",
    "            attention_probs = self.head_weights_proj(attention_probs.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (-1,)\n",
    "        context_layer = context_layer.view(new_context_layer_shape)\n",
    "        if output_attentions:\n",
    "            return (context_layer, attention_probs)\n",
    "        else:\n",
    "            return context_layer\n",
    "\n",
    "    def disentangled_att_bias(self, query_layer, key_layer, relative_pos, rel_embeddings, scale_factor):\n",
    "        if relative_pos is None:\n",
    "            q = query_layer.size(-2)\n",
    "            relative_pos = build_relative_position(q, key_layer.size(-2), query_layer.device)\n",
    "        if relative_pos.dim() == 2:\n",
    "            relative_pos = relative_pos.unsqueeze(0).unsqueeze(0)\n",
    "        elif relative_pos.dim() == 3:\n",
    "            relative_pos = relative_pos.unsqueeze(1)\n",
    "        # bxhxqxk\n",
    "        elif relative_pos.dim() != 4:\n",
    "            raise ValueError(f\"Relative position ids must be of dim 2 or 3 or 4. {relative_pos.dim()}\")\n",
    "\n",
    "        att_span = min(max(query_layer.size(-2), key_layer.size(-2)), self.max_relative_positions)\n",
    "        relative_pos = relative_pos.long().to(query_layer.device)\n",
    "        rel_embeddings = rel_embeddings[\n",
    "            self.max_relative_positions - att_span : self.max_relative_positions + att_span, :\n",
    "        ].unsqueeze(0)\n",
    "\n",
    "        score = 0\n",
    "\n",
    "        # content->position\n",
    "        if \"c2p\" in self.pos_att_type:\n",
    "            pos_key_layer = self.pos_proj(rel_embeddings)\n",
    "            pos_key_layer = self.transpose_for_scores(pos_key_layer)\n",
    "            c2p_att = torch.matmul(query_layer, pos_key_layer.transpose(-1, -2))\n",
    "            c2p_pos = torch.clamp(relative_pos + att_span, 0, att_span * 2 - 1)\n",
    "            c2p_att = torch.gather(c2p_att, dim=-1, index=c2p_dynamic_expand(c2p_pos, query_layer, relative_pos))\n",
    "            score += c2p_att\n",
    "\n",
    "        # position->content\n",
    "        if \"p2c\" in self.pos_att_type:\n",
    "            pos_query_layer = self.pos_q_proj(rel_embeddings)\n",
    "            pos_query_layer = self.transpose_for_scores(pos_query_layer)\n",
    "            pos_query_layer /= torch.sqrt(torch.tensor(pos_query_layer.size(-1), dtype=torch.float) * scale_factor)\n",
    "            if query_layer.size(-2) != key_layer.size(-2):\n",
    "                r_pos = build_relative_position(key_layer.size(-2), key_layer.size(-2), query_layer.device)\n",
    "            else:\n",
    "                r_pos = relative_pos\n",
    "            p2c_pos = torch.clamp(-r_pos + att_span, 0, att_span * 2 - 1)\n",
    "            p2c_att = torch.matmul(key_layer, torch.tensor(pos_query_layer.transpose(-1, -2), dtype=key_layer.dtype))\n",
    "            p2c_att = torch.gather(\n",
    "                p2c_att, dim=-1, index=p2c_dynamic_expand(p2c_pos, query_layer, key_layer)\n",
    "            ).transpose(-1, -2)\n",
    "\n",
    "            if query_layer.size(-2) != key_layer.size(-2):\n",
    "                pos_index = relative_pos[:, :, :, 0].unsqueeze(-1)\n",
    "                p2c_att = torch.gather(p2c_att, dim=-2, index=pos_dynamic_expand(pos_index, p2c_att, key_layer))\n",
    "            score += p2c_att\n",
    "\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5f7f47b6-ca67-4ce7-8a43-fc6d4e543d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b49b3bf9-2561-419b-9178-c264dfd0d919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DebertaConfig {\n",
       "  \"_name_or_path\": \"HannahRoseKirk/Hatemoji\",\n",
       "  \"architectures\": [\n",
       "    \"DebertaForSequenceClassification\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-07,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"max_relative_positions\": -1,\n",
       "  \"model_type\": \"deberta\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"pooler_dropout\": 0,\n",
       "  \"pooler_hidden_act\": \"gelu\",\n",
       "  \"pooler_hidden_size\": 768,\n",
       "  \"pos_att_type\": [\n",
       "    \"c2p\",\n",
       "    \"p2c\"\n",
       "  ],\n",
       "  \"position_biased_input\": false,\n",
       "  \"relative_attention\": true,\n",
       "  \"transformers_version\": \"4.23.1\",\n",
       "  \"type_vocab_size\": 0,\n",
       "  \"vocab_size\": 50265\n",
       "}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(\"HannahRoseKirk/Hatemoji\")\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c03fa0f-6038-4405-9924-7f9fd3c65f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax\n",
    "import flax.linen as nn\n",
    "import numpy as np\n",
    "from flax.core.frozen_dict import FrozenDict, freeze, unfreeze\n",
    "from flax.linen import combine_masks, make_causal_mask\n",
    "from flax.linen import partitioning as nn_partitioning\n",
    "from flax.linen.attention import dot_product_attention_weights\n",
    "from flax.traverse_util import flatten_dict, unflatten_dict\n",
    "from transformers.models.deberta.configuration_deberta import DebertaConfig\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import lax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0751e10-294a-4972-832e-42879ce35702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no talking_head\n",
    "# no query states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "033a0f8c-1cc5-4dbd-a43f-1f2c5276540c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlaxDisentangledSelfAttention(nn.Module):\n",
    "    config: DebertaConfig\n",
    "    dtype: jnp.dtype = jnp.float32  # the dtype of the computation\n",
    "\n",
    "    def setup(self):\n",
    "        self.num_attention_heads = self.config.num_attention_heads\n",
    "        self.attention_head_size = int(self.config.hidden_size / self.config.num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "        self.in_proj = nn.Dense(\n",
    "            self.all_head_size * 3,\n",
    "            use_bias=False,\n",
    "            kernel_init=jax.nn.initializers.normal(self.config.initializer_range),\n",
    "            dtype=self.dtype,\n",
    "        )\n",
    "        self.q_bias = self.param(\"q_bias\", lambda rng, shape: jnp.zeros(shape), [self.all_head_size])\n",
    "        self.v_bias = self.param(\"v_bias\", lambda rng, shape: jnp.zeros(shape), [self.all_head_size])\n",
    "        self.pos_att_type = self.config.pos_att_type or []\n",
    "\n",
    "        self.relative_attention = getattr(self.config, \"relative_attention\", False)\n",
    "\n",
    "        if self.relative_attention:\n",
    "            self.max_relative_positions = getattr(self.config, \"max_relative_positions\", -1)\n",
    "            if self.max_relative_positions < 1:\n",
    "                self.max_relative_positions = self.config.max_position_embeddings\n",
    "            self.pos_dropout = nn.Dropout(self.config.hidden_dropout_prob)\n",
    "\n",
    "            if \"c2p\" in self.pos_att_type:\n",
    "                self.pos_proj = nn.Dense(\n",
    "                    self.all_head_size,\n",
    "                    use_bias=False,\n",
    "                    kernel_init=jax.nn.initializers.normal(self.config.initializer_range),\n",
    "                    dtype=self.dtype,\n",
    "                )\n",
    "            if \"p2c\" in self.pos_att_type:\n",
    "                self.pos_q_proj = nn.Dense(\n",
    "                    self.all_head_size,\n",
    "                    kernel_init=jax.nn.initializers.normal(self.config.initializer_range),\n",
    "                    dtype=self.dtype,\n",
    "                )\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.shape[:-1] + (self.num_attention_heads, -1)\n",
    "        x = x.reshape(new_x_shape)\n",
    "        return x.transpose(0, 2, 1, 3)\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask,\n",
    "        deterministic=True,\n",
    "        output_attentions=False,\n",
    "        relative_pos=None,\n",
    "        rel_embeddings=None,\n",
    "    ):\n",
    "        qp = self.in_proj(hidden_states)\n",
    "        # 3 arrays of b x h x l x d\n",
    "        query_layer, key_layer, value_layer = self.transpose_for_scores(qp).split(3, axis=-1)\n",
    "\n",
    "        query_layer = query_layer + self.transpose_for_scores(self.q_bias[None, None, :])\n",
    "        value_layer = value_layer + self.transpose_for_scores(self.v_bias[None, None, :])\n",
    "\n",
    "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
    "        scale_factor = 1 + len(self.pos_att_type)\n",
    "        scale = jnp.sqrt(self.attention_head_size * scale_factor)\n",
    "        query_layer /= scale\n",
    "\n",
    "        # Convert the attention mask to an attention bias.\n",
    "        if attention_mask is not None:\n",
    "            attention_bias = lax.select(\n",
    "                attention_mask > 0,\n",
    "                jnp.full(attention_mask.shape, 0.0).astype(self.dtype),\n",
    "                jnp.full(attention_mask.shape, -1e10).astype(self.dtype),\n",
    "            )\n",
    "        else:\n",
    "            attention_bias = None\n",
    "\n",
    "        dropout_rng = None\n",
    "        if not deterministic and self.config.attention_probs_dropout_prob > 0.0:\n",
    "            dropout_rng = self.make_rng(\"dropout\")\n",
    "\n",
    "        attention_scores = dot_product_attention_weights(\n",
    "            query_layer,\n",
    "            key_layer,\n",
    "            bias=attention_bias,\n",
    "            dropout_rng=dropout_rng,\n",
    "            dropout_rate=self.config.attention_probs_dropout_prob,\n",
    "            broadcast_dropout=True,\n",
    "            deterministic=deterministic,\n",
    "            dtype=self.dtype,\n",
    "            precision=None,\n",
    "        )\n",
    "        if self.relative_attention:\n",
    "            rel_embeddings = self.pos_dropout(rel_embeddings)\n",
    "            rel_att = self.disentangled_att_bias(query_layer, key_layer, relative_pos, rel_embeddings, scale_factor)\n",
    "            attention_scores = attention_scores + rel_att\n",
    "\n",
    "        context_layer = jnp.einsum(\"...hqk,...khd->...qhd\", attention_scores, value_layer)\n",
    "        context_layer = attn_output.reshape(attn_output.shape[:2] + (-1,))  # b x l x hidden_size\n",
    "\n",
    "        outputs = (context_layer, attention_scores) if output_attentions else (context_layer,)\n",
    "        return context_layer\n",
    "\n",
    "    def disentangled_att_bias(self, query_layer, key_layer, relative_pos, rel_embeddings, scale_factor):\n",
    "        if relative_pos is None:\n",
    "            q = query_layer.size(-2)\n",
    "            relative_pos = build_relative_position(q, key_layer.size(-2), query_layer.device)\n",
    "        if relative_pos.dim() == 2:\n",
    "            relative_pos = relative_pos.unsqueeze(0).unsqueeze(0)\n",
    "        elif relative_pos.dim() == 3:\n",
    "            relative_pos = relative_pos.unsqueeze(1)\n",
    "        # bxhxqxk\n",
    "        elif relative_pos.dim() != 4:\n",
    "            raise ValueError(f\"Relative position ids must be of dim 2 or 3 or 4. {relative_pos.dim()}\")\n",
    "\n",
    "        att_span = min(max(query_layer.shape[-2], key_layer.shape[-2]), self.max_relative_positions)\n",
    "        relative_pos = relative_pos.long().to(query_layer.device)\n",
    "        rel_embeddings = rel_embeddings[\n",
    "            self.max_relative_positions - att_span : self.max_relative_positions + att_span, :\n",
    "        ].unsqueeze(0)\n",
    "\n",
    "        score = 0\n",
    "\n",
    "        # content->position\n",
    "        if \"c2p\" in self.pos_att_type:\n",
    "            pos_key_layer = self.pos_proj(rel_embeddings)\n",
    "            pos_key_layer = self.transpose_for_scores(pos_key_layer)\n",
    "            c2p_att = torch.matmul(query_layer, pos_key_layer.transpose(-1, -2))\n",
    "            c2p_pos = torch.clamp(relative_pos + att_span, 0, att_span * 2 - 1)\n",
    "            c2p_att = torch.gather(c2p_att, dim=-1, index=c2p_dynamic_expand(c2p_pos, query_layer, relative_pos))\n",
    "            score += c2p_att\n",
    "\n",
    "        # position->content\n",
    "        if \"p2c\" in self.pos_att_type:\n",
    "            pos_query_layer = self.pos_q_proj(rel_embeddings)\n",
    "            pos_query_layer = self.transpose_for_scores(pos_query_layer)\n",
    "            pos_query_layer /= torch.sqrt(torch.tensor(pos_query_layer.size(-1), dtype=torch.float) * scale_factor)\n",
    "            if query_layer.size(-2) != key_layer.size(-2):\n",
    "                r_pos = build_relative_position(key_layer.size(-2), key_layer.size(-2), query_layer.device)\n",
    "            else:\n",
    "                r_pos = relative_pos\n",
    "            p2c_pos = torch.clamp(-r_pos + att_span, 0, att_span * 2 - 1)\n",
    "            p2c_att = torch.matmul(key_layer, torch.tensor(pos_query_layer.transpose(-1, -2), dtype=key_layer.dtype))\n",
    "            p2c_att = torch.gather(\n",
    "                p2c_att, dim=-1, index=p2c_dynamic_expand(p2c_pos, query_layer, key_layer)\n",
    "            ).transpose(-1, -2)\n",
    "\n",
    "            if query_layer.size(-2) != key_layer.size(-2):\n",
    "                pos_index = relative_pos[:, :, :, 0].unsqueeze(-1)\n",
    "                p2c_att = torch.gather(p2c_att, dim=-2, index=pos_dynamic_expand(pos_index, p2c_att, key_layer))\n",
    "            score += p2c_att\n",
    "\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "22900c69-37f0-41ad-8650-70753748dce4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Incompatible shapes for broadcasting: shapes=[(1, 3, 12, 12), (1, 3, 3)]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/jax/.venv/lib/python3.8/site-packages/jax/_src/util.py:222\u001b[0m, in \u001b[0;36mcache.<locals>.wrap.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 222\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcached\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_trace_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/jax/.venv/lib/python3.8/site-packages/jax/_src/util.py:215\u001b[0m, in \u001b[0;36mcache.<locals>.wrap.<locals>.cached\u001b[0;34m(_, *args, **kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache(max_size)\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcached\u001b[39m(_, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 215\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/jax/.venv/lib/python3.8/site-packages/jax/_src/lax/lax.py:141\u001b[0m, in \u001b[0;36m_broadcast_shapes_cached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;129m@cache\u001b[39m()\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_broadcast_shapes_cached\u001b[39m(\u001b[38;5;241m*\u001b[39mshapes: Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]:\n\u001b[0;32m--> 141\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_broadcast_shapes_uncached\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mshapes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/jax/.venv/lib/python3.8/site-packages/jax/_src/lax/lax.py:157\u001b[0m, in \u001b[0;36m_broadcast_shapes_uncached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result_shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 157\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncompatible shapes for broadcasting: shapes=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(shapes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result_shape\n",
      "\u001b[0;31mValueError\u001b[0m: Incompatible shapes for broadcasting: shapes=[(1, 3, 12, 12), (1, 3, 3)]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [98], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m rng, inp_rng, init_rng \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39msplit(rng, \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m      5\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(hidden_states\u001b[38;5;241m=\u001b[39mjnp\u001b[38;5;241m.\u001b[39mzeros([\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, config\u001b[38;5;241m.\u001b[39mhidden_size]), attention_mask\u001b[38;5;241m=\u001b[39mjnp\u001b[38;5;241m.\u001b[39mones([\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m]))\n\u001b[0;32m----> 6\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43minit_rng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39mapply()\n",
      "    \u001b[0;31m[... skipping hidden 9 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn [97], line 81\u001b[0m, in \u001b[0;36mFlaxDisentangledSelfAttention.__call__\u001b[0;34m(self, hidden_states, attention_mask, deterministic, output_attentions, relative_pos, rel_embeddings)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m deterministic \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mattention_probs_dropout_prob \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m     79\u001b[0m     dropout_rng \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_rng(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 81\u001b[0m attention_scores \u001b[38;5;241m=\u001b[39m \u001b[43mdot_product_attention_weights\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_probs_dropout_prob\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbroadcast_dropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprecision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelative_attention:\n\u001b[1;32m     93\u001b[0m     rel_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_dropout(rel_embeddings)\n",
      "File \u001b[0;32m~/Desktop/jax/.venv/lib/python3.8/site-packages/flax/linen/attention.py:100\u001b[0m, in \u001b[0;36mdot_product_attention_weights\u001b[0;34m(query, key, bias, mask, broadcast_dropout, dropout_rng, dropout_rate, deterministic, dtype, precision)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# apply attention bias: masking, dropout, proximity bias, etc.\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 100\u001b[0m   attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mattn_weights\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# apply attention mask\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/jax/.venv/lib/python3.8/site-packages/jax/_src/numpy/lax_numpy.py:4705\u001b[0m, in \u001b[0;36m_defer_to_unrecognized_arg.<locals>.deferring_binary_op\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   4703\u001b[0m args \u001b[38;5;241m=\u001b[39m (other, \u001b[38;5;28mself\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m swap \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28mself\u001b[39m, other)\n\u001b[1;32m   4704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, _accepted_binop_types):\n\u001b[0;32m-> 4705\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbinary_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, _rejected_binop_types):\n\u001b[1;32m   4707\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munsupported operand type(s) for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mopchar\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4708\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(args[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(args[\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "    \u001b[0;31m[... skipping hidden 14 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/jax/.venv/lib/python3.8/site-packages/jax/_src/numpy/ufuncs.py:83\u001b[0m, in \u001b[0;36m_maybe_bool_binop.<locals>.fn\u001b[0;34m(x1, x2)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn\u001b[39m(x1, x2):\n\u001b[0;32m---> 83\u001b[0m   x1, x2 \u001b[38;5;241m=\u001b[39m \u001b[43m_promote_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumpy_fn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m lax_fn(x1, x2) \u001b[38;5;28;01mif\u001b[39;00m x1\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mbool_ \u001b[38;5;28;01melse\u001b[39;00m bool_lax_fn(x1, x2)\n",
      "File \u001b[0;32m~/Desktop/jax/.venv/lib/python3.8/site-packages/jax/_src/numpy/util.py:355\u001b[0m, in \u001b[0;36m_promote_args\u001b[0;34m(fun_name, *args)\u001b[0m\n\u001b[1;32m    353\u001b[0m _check_arraylike(fun_name, \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    354\u001b[0m _check_no_float0s(fun_name, \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m--> 355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_promote_shapes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_promote_dtypes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/jax/.venv/lib/python3.8/site-packages/jax/_src/numpy/util.py:248\u001b[0m, in \u001b[0;36m_promote_shapes\u001b[0;34m(fun_name, *args)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mjax_numpy_rank_promotion \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    247\u001b[0m   _rank_promotion_warning_or_error(fun_name, shapes)\n\u001b[0;32m--> 248\u001b[0m result_rank \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_shapes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mshapes\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [_broadcast_to(arg, (\u001b[38;5;241m1\u001b[39m,) \u001b[38;5;241m*\u001b[39m (result_rank \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(shp)) \u001b[38;5;241m+\u001b[39m shp)\n\u001b[1;32m    250\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m arg, shp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(args, shapes)]\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/jax/.venv/lib/python3.8/site-packages/jax/_src/lax/lax.py:157\u001b[0m, in \u001b[0;36m_broadcast_shapes_uncached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    155\u001b[0m result_shape \u001b[38;5;241m=\u001b[39m _try_broadcast_shapes(shape_list)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result_shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 157\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncompatible shapes for broadcasting: shapes=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(shapes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result_shape\n",
      "\u001b[0;31mValueError\u001b[0m: Incompatible shapes for broadcasting: shapes=[(1, 3, 12, 12), (1, 3, 3)]"
     ]
    }
   ],
   "source": [
    "model = FlaxDisentangledSelfAttention(config)\n",
    "rng = jax.random.PRNGKey(42)\n",
    "\n",
    "rng, inp_rng, init_rng = jax.random.split(rng, 3)\n",
    "inputs = dict(hidden_states=jnp.zeros([1, 3, config.hidden_size]), attention_mask=jnp.ones([1, 3, 3]))\n",
    "params = model.init(init_rng, **inputs)\n",
    "model.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1dd2e5a0-39ed-4999-8de4-81a3ab18b1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transpose_for_scores(num_attention_heads, x):\n",
    "    new_x_shape = x.shape[:-1] + (num_attention_heads, -1)\n",
    "    x = x.reshape(new_x_shape)\n",
    "    return x.transpose(0, 2, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "77e799d4-f919-4c53-9f1c-09ac997f5d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = jnp.zeros([1, 3, 768])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "99290793-f37c-4659-9089-d15a13dfe5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_layer = transpose_for_scores(12, x)\n",
    "key_layer = transpose_for_scores(12, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "879e01b4-1844-4925-8072-bfdf70270556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 12, 3, 64)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_layer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "40aae218-6e35-4c41-8a50-627258cdd671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 12, 3, 3)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.matmul(query_layer, key_layer.swapaxes(-1, -2)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d0f2483a-f670-4034-8597-826e2d00b314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 12, 64, 3)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_layer.swapaxes(-1, -2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f280ace-cd13-430f-8e41-3042b5744f61",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Tuple must have size 2, but has size 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/jax/.venv/lib/python3.8/site-packages/jax/_src/numpy/lax_numpy.py:4629\u001b[0m, in \u001b[0;36m_view\u001b[0;34m(arr, dtype, type)\u001b[0m\n\u001b[1;32m   4628\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_view\u001b[39m(arr, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m-> 4629\u001b[0m   \u001b[43mlax_internal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_user_dtype_supported\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mview\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4630\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4631\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`type` argument of array.view()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/jax/.venv/lib/python3.8/site-packages/jax/_src/lax/lax.py:4745\u001b[0m, in \u001b[0;36m_check_user_dtype_supported\u001b[0;34m(dtype, fun_name)\u001b[0m\n\u001b[1;32m   4743\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, \u001b[38;5;28mtype\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m dtype \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, builtins\u001b[38;5;241m.\u001b[39mcomplex}:\n\u001b[1;32m   4744\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m-> 4745\u001b[0m np_dtype \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4746\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np_dtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbiufc\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m np_dtype\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mbfloat16:\n\u001b[1;32m   4747\u001b[0m   msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJAX only supports number and bool dtypes, got dtype \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: Tuple must have size 2, but has size 4"
     ]
    }
   ],
   "source": [
    "x.view((1, 1, 12, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff180ba-1684-4866-ae00-6083064380aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Poetry",
   "language": "python",
   "name": "poetry-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
